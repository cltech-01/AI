import os
from pathlib import Path
from dotenv import load_dotenv
from common import measure_time
from langchain_core.documents import Document
from langchain_community.vectorstores import FAISS
from langchain_openai import OpenAIEmbeddings
from langchain_text_splitters import RecursiveCharacterTextSplitter


load_dotenv()

### Text íŒŒì¼ì„ Chunking í•œ í›„ Document List ë¡œ ë¶„ë¦¬í•©ë‹ˆë‹¤. 
def chunk_text_to_documents(text_path: str, chunk_size: int = 1000, chunk_overlap: int = 50) -> list:
    ext = Path(text_path).suffix
    
    try:
        with open(text_path, "r", encoding="utf-8") as f:
            full_text = f.read()
    except Exception as e:
        print(f"âŒ í…ìŠ¤íŠ¸ íŒŒì¼ ì½ê¸° ì‹¤íŒ¨: {e}")
        raise

    # í…ìŠ¤íŠ¸ ì²­í‚¹
    text_splitter = RecursiveCharacterTextSplitter(
        chunk_size=chunk_size,
        chunk_overlap=chunk_overlap
    )
    
    chunks = text_splitter.split_text(full_text)
    print(f"âœ… í…ìŠ¤íŠ¸ ì²­í‚¹ ì™„ë£Œ: {len(chunks)}ê°œ ì²­í¬ ìƒì„±")
    
    # Document ë¦¬ìŠ¤íŠ¸ë¡œ ë³€í™˜
    source_name = Path(text_path).stem
    docs = [
        Document(
            page_content=chunk, 
            metadata={
                "source": source_name,
                "chunk_index": i
            }
        ) 
        for i, chunk in enumerate(chunks)
    ]
    
    return docs


# í˜„ì¬ëŠ” FAISS ë¡œ ì„ë² ë”© ì¤‘ì¸ë° ì´ê²ƒì„ qdrantë¡œ ë°”ê¿”ì£¼ë©´ ë©ë‹ˆë‹¤. 
@measure_time
def vector_embedding(text_path: str, username: str, openai_api_key: str = None):
    """
    í…ìŠ¤íŠ¸ íŒŒì¼ì„ ë²¡í„°ë¡œ ì„ë² ë”©í•˜ê³  FAISS ì¸ë±ìŠ¤ë¥¼ ìƒì„±í•©ë‹ˆë‹¤.
    
    Args:
        text_path: í…ìŠ¤íŠ¸ íŒŒì¼ ê²½ë¡œ
        username: ì‚¬ìš©ì ì´ë¦„
        openai_api_key: OpenAI API í‚¤ (í™˜ê²½ë³€ìˆ˜ì—ì„œ ê°€ì ¸ì˜¤ì§€ ì•Šì„ ê²½ìš°)
        
    Returns:
        ìƒì„±ëœ ë²¡í„°ìŠ¤í† ì–´ ê²½ë¡œ
    """
    # API í‚¤ ì„¤ì •
    if openai_api_key:
        os.environ["OPENAI_API_KEY"] = openai_api_key
    elif "OPENAI_API_KEY" not in os.environ:
        raise ValueError("OpenAI API í‚¤ê°€ í•„ìš”í•©ë‹ˆë‹¤. í™˜ê²½ë³€ìˆ˜ ë˜ëŠ” ë§¤ê°œë³€ìˆ˜ë¡œ ì œê³µí•´ì£¼ì„¸ìš”.")
    
    # í…ìŠ¤íŠ¸ íŒŒì¼ ì²­í‚¹ ë° Document ìƒì„±
    print(f"ğŸ”„ í…ìŠ¤íŠ¸ íŒŒì¼ ì²­í‚¹ ì‹œì‘: {text_path}")
    docs = chunk_text_to_documents(text_path)
    
    # ì„ë² ë”© ë° ë²¡í„°ìŠ¤í† ì–´ ìƒì„±
    print(f"ğŸ”„ ë²¡í„° ì„ë² ë”© ì‹œì‘: {len(docs)}ê°œ ë¬¸ì„œ")
    embedding = OpenAIEmbeddings()
    vectorstore = FAISS.from_documents(docs, embedding)
    
    # ë²¡í„°ìŠ¤í† ì–´ ì €ì¥ # ì‹¤ì œë¡œëŠ” ì—†ì–´ë„ ëŒ
    # file_stem = Path(text_path).stem
    # embedding_dir = Path(f"./Data/Embeddings/{username}")
    # embedding_dir.mkdir(exist_ok=True, parents=True)
    
    # embedding_path = f"{embedding_dir}/{file_stem}"
    # vectorstore.save_local(embedding_path)
    
    print(f"âœ… ë²¡í„° ì„ë² ë”© ì €ì¥ ì™„ë£Œ")
    return vectorstore





if __name__ == "__main__":
    print(vector_embedding("./Data/Text/jhkim/01_transcript.txt", "jhkim"))
    # docs = chunk_text_to_documents("./Data/Sound/jhkim/01_transcript.txt")
    # print(docs)