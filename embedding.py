import os
from pathlib import Path
from dotenv import load_dotenv
from common import measure_time
from langchain_core.documents import Document
from langchain_community.vectorstores import FAISS
from langchain_openai import OpenAIEmbeddings
from langchain_text_splitters import RecursiveCharacterTextSplitter
from azure.core.credentials import AzureKeyCredential
from fastapi import FastAPI, HTTPException
from pydantic import BaseModel
from typing import List, Optional
from qdrant_client import QdrantClient
from qdrant_client.models import PointStruct, Distance, VectorParams
import uuid
from openai import AzureOpenAI
import os
from dotenv import load_dotenv


load_dotenv()

app = FastAPI(title="Vector Search API")

endpoint = os.getenv("AZURE_OPENAI_ENDPOINT")
model_name = "text-embedding-3-large"
deployment = "text-embedding-3-large"

api_version = os.getenv("AZURE_OPENAI_API_VERSION")

# Azure OpenAI í´ë¼ì´ì–¸íŠ¸ ì„¤ì •
client = AzureOpenAI(
    api_version="2024-12-01-preview",
    base_url= endpoint,
    api_key=os.getenv("AZURE_OPENAI_API_KEY")
)

# Qdrant í´ë¼ì´ì–¸íŠ¸ ì„¤ì •
qdrant_client = QdrantClient("localhost", port=6333)

### Text íŒŒì¼ì„ Chunking í•œ í›„ Document List ë¡œ ë¶„ë¦¬í•©ë‹ˆë‹¤. 
def chunk_text_to_documents(text_path: str, chunk_size: int = 1000, chunk_overlap: int = 50) -> list:
    ext = Path(text_path).suffix
    
    try:
        with open(text_path, "r", encoding="utf-8") as f:
            full_text = f.read()
    except Exception as e:
        print(f"âŒ í…ìŠ¤íŠ¸ íŒŒì¼ ì½ê¸° ì‹¤íŒ¨: {e}")
        raise

    # í…ìŠ¤íŠ¸ ì²­í‚¹
    text_splitter = RecursiveCharacterTextSplitter(
        chunk_size=chunk_size,
        chunk_overlap=chunk_overlap
    )
    
    chunks = text_splitter.split_text(full_text)
    print(f"âœ… í…ìŠ¤íŠ¸ ì²­í‚¹ ì™„ë£Œ: {len(chunks)}ê°œ ì²­í¬ ìƒì„±")
    
    # Document ë¦¬ìŠ¤íŠ¸ë¡œ ë³€í™˜
    source_name = Path(text_path).stem
    docs = [
        Document(
            page_content=chunk, 
            metadata={
                "source": source_name,
                "chunk_index": i
            }
        ) 
        for i, chunk in enumerate(chunks)
    ]
    
    return docs


# í˜„ì¬ëŠ” FAISS ë¡œ ì„ë² ë”© ì¤‘ì¸ë° ì´ê²ƒì„ qdrantë¡œ ë°”ê¿”ì£¼ë©´ ë©ë‹ˆë‹¤. 
@measure_time
def vector_embedding(text_path: str, username: str, openai_api_key: str = None):
    """
    í…ìŠ¤íŠ¸ íŒŒì¼ì„ ë²¡í„°ë¡œ ì„ë² ë”©í•˜ê³  FAISS ì¸ë±ìŠ¤ë¥¼ ìƒì„±í•©ë‹ˆë‹¤.
    
    Args:
        text_path: í…ìŠ¤íŠ¸ íŒŒì¼ ê²½ë¡œ
        username: ì‚¬ìš©ì ì´ë¦„
        openai_api_key: OpenAI API í‚¤ (í™˜ê²½ë³€ìˆ˜ì—ì„œ ê°€ì ¸ì˜¤ì§€ ì•Šì„ ê²½ìš°)
        
    Returns:
        ìƒì„±ëœ ë²¡í„°ìŠ¤í† ì–´ ê²½ë¡œ
    """
    # API í‚¤ ì„¤ì •
    if openai_api_key:
        os.environ["OPENAI_API_KEY"] = openai_api_key
    elif "OPENAI_API_KEY" not in os.environ:
        raise ValueError("OpenAI API í‚¤ê°€ í•„ìš”í•©ë‹ˆë‹¤. í™˜ê²½ë³€ìˆ˜ ë˜ëŠ” ë§¤ê°œë³€ìˆ˜ë¡œ ì œê³µí•´ì£¼ì„¸ìš”.")
    
    # í…ìŠ¤íŠ¸ íŒŒì¼ ì²­í‚¹ ë° Document ìƒì„±
    print(f"ğŸ”„ í…ìŠ¤íŠ¸ íŒŒì¼ ì²­í‚¹ ì‹œì‘: {text_path}")
    docs = chunk_text_to_documents(text_path)
    
    # ì„ë² ë”© ë° ë²¡í„°ìŠ¤í† ì–´ ìƒì„±
    print(f"ğŸ”„ ë²¡í„° ì„ë² ë”© ì‹œì‘: {len(docs)}ê°œ ë¬¸ì„œ")
    embedding = OpenAIEmbeddings()
    vectorstore = FAISS.from_documents(docs, embedding)
    
    # ë²¡í„°ìŠ¤í† ì–´ ì €ì¥ # ì‹¤ì œë¡œëŠ” ì—†ì–´ë„ ëŒ
    # file_stem = Path(text_path).stem
    # embedding_dir = Path(f"./Data/Embeddings/{username}")
    # embedding_dir.mkdir(exist_ok=True, parents=True)
    
    # embedding_path = f"{embedding_dir}/{file_stem}"
    # vectorstore.save_local(embedding_path)
    
    print(f"âœ… ë²¡í„° ì„ë² ë”© ì €ì¥ ì™„ë£Œ")
    return vectorstore


class Document(BaseModel):
    title: str
    department: str
    keywords: List[str]
    content: str

class SearchQuery(BaseModel):
    query: str
    department: Optional[str] = None
    limit: int = 5

def get_embedding(text: str) -> List[float]:
    """Azure OpenAI APIë¥¼ ì‚¬ìš©í•˜ì—¬ í…ìŠ¤íŠ¸ì˜ ì„ë² ë”©ì„ ìƒì„±í•©ë‹ˆë‹¤."""
    response = client.embeddings.create(
        model=os.getenv("AZURE_OPENAI_DEPLOYMENT_NAME"),
        input=text
    )
    return response.data[0].embedding

@app.post("/documents")
async def create_document(document: Document):
    """ë¬¸ì„œë¥¼ ë²¡í„° DBì— ì €ì¥í•©ë‹ˆë‹¤."""
    try:
        # ë¬¸ì„œ ë‚´ìš©ì˜ ì„ë² ë”© ìƒì„±
        embedding = get_embedding(document.content)
        
        # Qdrantì— ë¬¸ì„œ ì €ì¥
        qdrant_client.upsert(
            collection_name="meeting_summaries",
            points=[
                PointStruct(
                    id=str(uuid.uuid4()),
                    vector=embedding,
                    payload={
                        "title": document.title,
                        "department": document.department,
                        "keywords": document.keywords,
                        "content": document.content
                    }
                )
            ]
        )
        return {"status": "success", "message": "ë¬¸ì„œê°€ ì„±ê³µì ìœ¼ë¡œ ì €ì¥ë˜ì—ˆìŠµë‹ˆë‹¤."}
    except Exception as e:
        raise HTTPException(status_code=500, detail=str(e))

@app.post("/search")
async def search_documents(query: SearchQuery):
    """ë¬¸ì„œë¥¼ ê²€ìƒ‰í•©ë‹ˆë‹¤."""
    try:
        # ì¿¼ë¦¬ í…ìŠ¤íŠ¸ì˜ ì„ë² ë”© ìƒì„±
        query_vector = get_embedding(query.query)
        
        # Qdrantì—ì„œ ê²€ìƒ‰ ì‹¤í–‰
        results = qdrant_client.search(
            collection_name="meeting_summaries",
            query_vector=query_vector,
            limit=query.limit,
            with_payload=True
        )
        
        return [{
            "score": hit.score,
            "title": hit.payload["title"],
            "department": hit.payload["department"],
            "keywords": hit.payload["keywords"],
            "content": hit.payload["content"]
        } for hit in results]
    except Exception as e:
        raise HTTPException(status_code=500, detail=str(e))

@app.on_event("startup")
async def startup_event():
    """ì„œë²„ ì‹œì‘ ì‹œ Qdrant ì»¬ë ‰ì…˜ì„ ì´ˆê¸°í™”í•©ë‹ˆë‹¤."""
    try:
        qdrant_client.recreate_collection(
            collection_name="meeting_summaries",
            vectors_config=VectorParams(
                size=3072,  # text-embedding-3-large ë²¡í„° í¬ê¸°
                distance=Distance.COSINE
            )
        )
    except Exception as e:
        print(f"ì»¬ë ‰ì…˜ ì´ˆê¸°í™” ì¤‘ ì˜¤ë¥˜ ë°œìƒ: {e}") 


if __name__ == "__main__":
    print(vector_embedding("./reference/cleaned_example.txt", "jhkim"))
    # docs = chunk_text_to_documents("./Data/Sound/jhkim/01_transcript.txt")
    # print(docs)
