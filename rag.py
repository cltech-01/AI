import os
from dotenv import load_dotenv
from embedding import vector_embedding
from langchain.chains import RetrievalQA
from langchain_openai import AzureChatOpenAI
from langchain_openai import ChatOpenAI

# ğŸ”§ í™˜ê²½ë³€ìˆ˜ ë¡œë”©
load_dotenv()

# ğŸ”§ QA ì‹œìŠ¤í…œ êµ¬ì„±
def setup_qa_system(vector_store):
    llm = AzureChatOpenAI(
        deployment_name=os.environ["AZURE_OPENAI_DEPLOYMENT_NAME"],
        openai_api_key=os.environ["AZURE_OPENAI_API_KEY"],
        azure_endpoint=os.environ["AZURE_OPENAI_ENDPOINT"],
        openai_api_version=os.environ["AZURE_OPENAI_API_VERSION"],
    )

    # llm = AzureChatOpenAI(
    #         azure_deployment=os.environ["AZURE_OPENAI_DEPLOYMENT_NAME"],
    #         api_version=os.environ.get("AZURE_OPENAI_API_VERSION", "2024-12-01-preview"),
    #         temperature=0.0,
    #     )
    # llm= ChatOpenAI(model_name="gpt-3.5-turbo", temperature=0)

    retriever = vector_store.as_retriever(search_kwargs={"k": 3})

    qa_chain = RetrievalQA.from_chain_type(
        llm=llm,
        chain_type="stuff",
        retriever=retriever,
        return_source_documents=True
    )

    return qa_chain

# ğŸ”§ ì§ˆë¬¸ ìˆ˜í–‰ í•¨ìˆ˜
def answer_question(qa_chain, question):
    result = qa_chain({"query": question})
    print(f"ì§ˆë¬¸: {question}\n")
    print(f"ë‹µë³€: {result['result']}\n")
    print("ì°¸ì¡° ë¬¸ì„œ:")
    for i, doc in enumerate(result["source_documents"]):
        print(f"{i+1}. {doc.page_content} [{doc.metadata}]")
    print("\n" + "-"*50 + "\n")

# ğŸ”§ ì‹¤í–‰
if __name__ == "__main__":
    print("ğŸ“¦ ë²¡í„° ì„ë² ë”© + VectorStore ë¡œë”©...")
    vectorstore = vector_embedding("reference/cleaned_example.txt", "jhkim")

    print("ğŸ”§ QA ì‹œìŠ¤í…œ êµ¬ì„± ì¤‘...")
    qa_chain = setup_qa_system(vectorstore)

    print("ğŸ“¤ ì§ˆë¬¸ ë³´ë‚´ê¸°...")
    answer_question(qa_chain, "KTì—ì„œ CI/CDëŠ” ì–´ë–»ê²Œ ì²˜ë¦¬í•´?")