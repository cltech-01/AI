import os
from dotenv import load_dotenv
from embedding import vector_embedding, vector_store
from langchain.chains import RetrievalQA
from langchain_openai import AzureChatOpenAI
from typing_extensions import TypedDict
from langchain_text_splitters import RecursiveCharacterTextSplitter
from langgraph.graph import StateGraph, START, END

# í™˜ê²½ë³€ìˆ˜ ë¡œë”©
load_dotenv()

llm = AzureChatOpenAI(
    deployment_name=os.environ["AZURE_OPENAI_DEPLOYMENT_NAME"],
    openai_api_key=os.environ["AZURE_OPENAI_API_KEY"],
    azure_endpoint=os.environ["AZURE_OPENAI_ENDPOINT"],
    openai_api_version=os.environ["AZURE_OPENAI_API_VERSION"],
)

class State(TypedDict):
    user_id: int
    text: str
    summary: str
    chunks: list[str]
    documents: list
    status: str


def split_text_for_filtering(state: State):
    text_splitter_for_filtering = RecursiveCharacterTextSplitter(
        chunk_size=2000, # gptê°€ íš¨ìœ¨ì ìœ¼ë¡œ ì²˜ë¦¬ ê°€ëŠ¥í•œ ê¸€ììˆ˜
        chunk_overlap=50,
    )

    chunks = text_splitter_for_filtering.split_text(state['text'])

    return {"chunks": chunks}


def filter_small_talk(state: State):
    prompt = """
    ë‹¤ìŒì€ IT ê¸°ìˆ  ê°•ì˜ì…ë‹ˆë‹¤. ë‹¤ìŒ ë‚´ìš©ì—ì„œ IT ê¸°ìˆ ê³¼ ê´€ë ¨ì—†ëŠ” ì¼ìƒì ì¸ ëŒ€í™”ë‚˜ ë¶ˆí•„ìš”í•œ ê°íƒ„ì‚¬/ì›ƒìŒ ë“±ì€ ì œê±°í•´ì£¼ì„¸ìš”. ì œê±° í›„ ìš”ì•½í•˜ì§€ ë§ê³  ì›ë¬¸ ê·¸ëŒ€ë¡œ ë‚¨ê²¨ì£¼ì„¸ìš”.:

    """
    result = ""
    chunks = state['chunks']

    for chunk in chunks:
        response = llm.invoke(prompt + chunk)

        if response.content:
            result += response.content
        else:
            result += chunk

    return {"text": result}


def summarize_text(state: State):
    prompt = f"""
    ë‹¤ìŒì€ IT ê¸°ìˆ  ê°•ì˜ì…ë‹ˆë‹¤. ê°•ì˜ë¥¼ 1000ì ë‚´ì™¸ë¡œ ìš”ì•½í•´ì£¼ì„¸ìš”.:

    {state['text']}
    """
    response = llm.invoke(prompt)
    
    # TODO: ë°±ì—”ë“œ API í˜¸ì¶œ
    return {"summary": response.content}


def split_text_for_embedding(state: State):
    text_splitter_for_embedding = RecursiveCharacterTextSplitter(
        chunk_size=1000,
        chunk_overlap=50,
    )

    chunks = text_splitter_for_embedding.split_text(state['text'])

    return {"chunks": chunks}


from langchain.schema import Document

def extract_keywords(state: State):
    prompt = """
    ë‹¤ìŒ ê°•ì˜ ë‚´ìš©ì„ ë³´ê³  ì¤‘ìš”í•œ ê¸°ìˆ  í‚¤ì›Œë“œë‚˜ ì£¼ì œë¥¼ 3~5ê°œ ì¶”ì¶œí•´ì£¼ì„¸ìš”.
    ë‹¨ì–´ë¡œë§Œ ì¶”ì¶œí•˜ê³ , ì‰¼í‘œë¡œ êµ¬ë¶„í•´ì£¼ì„¸ìš”. ì„¤ëª…ì€ í•˜ì§€ ë§ˆì„¸ìš”.

    """

    chunks = state['chunks']
    documents = []

    for chunk in chunks:
        response = llm.invoke(prompt + chunk)
        keywords = [kw.strip() for kw in response.content.split(",")]

        doc = Document(
            page_content=chunk,
            metadata={
                "keywords": keywords,
                "user_id": state["user_id"]
            }
        )

        documents.append(doc)
    
    return {"documents": documents}


def embedding(state: State):
    vector_store.add_documents(state["documents"])

    return state


builder = StateGraph(State)

builder.add_node("split_text_for_filtering", split_text_for_filtering)
builder.add_node("filter_small_talk", filter_small_talk)
builder.add_node("summarize_text", summarize_text)
builder.add_node("split_text_for_embedding", split_text_for_embedding)
builder.add_node("extract_keywords", extract_keywords)
builder.add_node("embedding", embedding)

builder.add_edge(START, "split_text_for_filtering")
builder.add_edge("split_text_for_filtering", "filter_small_talk")
builder.add_edge("filter_small_talk", "summarize_text")
builder.add_edge("summarize_text", "split_text_for_embedding")
builder.add_edge("split_text_for_embedding", "extract_keywords")
builder.add_edge("extract_keywords", "embedding")
builder.add_edge("embedding", END)

graph = builder.compile()


def store_RAG_data(text: str, user_id: int):
    state: State = {
        "text": text,
        "user_id": user_id
    }

    graph.invoke(state)


# QA ì‹œìŠ¤í…œ êµ¬ì„±
def setup_qa_system(vector_store):
    retriever = vector_store.as_retriever(search_kwargs={"k": 3})

    qa_chain = RetrievalQA.from_chain_type(
        llm=llm,
        chain_type="stuff",
        retriever=retriever,
        return_source_documents=True
    )

    return qa_chain

# ì§ˆë¬¸ ìˆ˜í–‰ í•¨ìˆ˜
def answer_question(qa_chain, question):
    result = qa_chain({"query": question})
    print(f"ì§ˆë¬¸: {question}\n")
    print(f"ë‹µë³€: {result['result']}\n")
    print("ì°¸ì¡° ë¬¸ì„œ:")
    for i, doc in enumerate(result["source_documents"]):
        print(f"{i+1}. {doc.page_content} [{doc.metadata}]")
    print("\n" + "-"*50 + "\n")

# ì‹¤í–‰
if __name__ == "__main__":
    # print("ğŸ“¦ ë²¡í„° ì„ë² ë”© + VectorStore ë¡œë”©...")
    # vectorstore = vector_embedding("reference/cleaned_example.txt", "jhkim")

    print("ë°ì´í„° ì „ì²˜ë¦¬ & ì„ë² ë”© ì¤‘...")
    # TODO: text data, user_id parameterë¡œ ë°›ê¸°
    test_text = ""
    with open("./reference/cleaned_example.txt", "r", encoding="utf-8") as f:
        test_text = f.read()

    store_RAG_data(test_text, 1)
    print("ì™„ë£Œ!")

    print("ğŸ”§ QA ì‹œìŠ¤í…œ êµ¬ì„± ì¤‘...")
    qa_chain = setup_qa_system(vector_store)

    print("ğŸ“¤ ì§ˆë¬¸ ë³´ë‚´ê¸°...")
    answer_question(qa_chain, "KTì—ì„œ CI/CDëŠ” ì–´ë–»ê²Œ ì²˜ë¦¬í•´?")