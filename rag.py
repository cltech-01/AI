from embedding import vector_store, llm
from langchain.chains import RetrievalQA


# QA ì‹œìŠ¤í…œ êµ¬ì„±
def setup_qa_system(vector_store):
    retriever = vector_store.as_retriever(search_kwargs={"k": 3})

    qa_chain = RetrievalQA.from_chain_type(
        llm=llm,
        chain_type="stuff",
        retriever=retriever,
        return_source_documents=True
    )

    return qa_chain

# ì§ˆë¬¸ ìˆ˜í–‰ í•¨ìˆ˜
def answer_question(qa_chain, question):
    result = qa_chain({"query": question})
    print(f"ì§ˆë¬¸: {question}\n")
    print(f"ë‹µë³€: {result['result']}\n")
    print("ì°¸ì¡° ë¬¸ì„œ:")
    for i, doc in enumerate(result["source_documents"]):
        print(f"{i+1}. {doc.page_content} [{doc.metadata}]")
    print("\n" + "-"*50 + "\n")

# ì‹¤í–‰
if __name__ == "__main__":
    print("ğŸ”§ QA ì‹œìŠ¤í…œ êµ¬ì„± ì¤‘...")
    qa_chain = setup_qa_system(vector_store)

    print("ğŸ“¤ ì§ˆë¬¸ ë³´ë‚´ê¸°...")
    answer_question(qa_chain, "KTì—ì„œ CI/CDëŠ” ì–´ë–»ê²Œ ì²˜ë¦¬í•´?")