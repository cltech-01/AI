from dotenv import load_dotenv
from embedding import vector_embedding
from langchain.chains import RetrievalQA
from langchain_community.chat_models import ChatOpenAI

def setup_qa_system(vectorstore):
    """
    ë²¡í„° ìŠ¤í† ì–´ë¥¼ ì‚¬ìš©í•˜ì—¬ QA ì‹œìŠ¤í…œì„ ì„¤ì •í•©ë‹ˆë‹¤.
    
    Args:
        vectorstore: ì„ë² ë”©ëœ ë²¡í„° ìŠ¤í† ì–´
    
    Returns:
        RetrievalQA ì²´ì¸
    """
    # MMR ê²€ìƒ‰ ë°©ì‹ì˜ Retriever ì„¤ì •
    retriever = vectorstore.as_retriever(search_type="mmr")
    retriever.search_kwargs.update({
        "k": 10,
        "fetch_k": 100,
        "maximal_marginal_relevance": True
    })
    
    # QA ì²´ì¸ êµ¬ì„±
    qa = RetrievalQA.from_chain_type(
        llm=ChatOpenAI(temperature=0),
        chain_type="stuff",
        retriever=retriever,
        return_source_documents=True
    )
    
    return qa

def ask_question(qa, query):
    """
    ì§ˆë¬¸ì— ëŒ€í•œ ë‹µë³€ì„ ìƒì„±í•©ë‹ˆë‹¤.
    
    Args:
        qa: QA ì²´ì¸
        query: ì§ˆë¬¸ ë¬¸ìì—´
    
    Returns:
        ë‹µë³€ ê²°ê³¼
    """
    print(f"\nğŸ’¬ ì§ˆë¬¸: {query}")
    result = qa(query)
    
    print(f"ğŸ§  ë‹µë³€: {result['result']}")
    print("\nğŸ“„ ì°¸ê³  ë¬¸ì„œ:")
    for i, doc in enumerate(result['source_documents']):
        print(f"\në¬¸ì„œ {i+1}:")
        print(f"ë‚´ìš©: {doc.page_content[:150]}..." if len(doc.page_content) > 150 else f"ë‚´ìš©: {doc.page_content}")
        print(f"ë©”íƒ€ë°ì´í„°: {doc.metadata}")
    
    return result

if __name__ == "__main__":
    load_dotenv()
    vectorstore = vector_embedding("reference/cleaned_example.txt", "jhkim")
    qa = setup_qa_system(vectorstore)
    ask_question(qa, "KTì—ì„œ CI/CDëŠ” ì–´ë–»ê²Œ ì²˜ë¦¬í•´?")

