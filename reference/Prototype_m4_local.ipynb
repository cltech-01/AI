{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "048f3051",
   "metadata": {},
   "source": [
    "1. python3 -m venv prototype-virtual-env 으로 가상환경 생성   \n",
    "2. source prototype-virtual-env/bin/activate\n",
    "3. pip install --upgrade pip + pip install ipykernel jupyter\n",
    "4. 동일경로에 jupyter notebook 입력후 local vscode or cursor 에서 작업시작!  \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "b53f7714",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[34m==>\u001b[0m \u001b[1mDownloading https://formulae.brew.sh/api/formula.jws.json\u001b[0m\n",
      "\u001b[34m==>\u001b[0m \u001b[1mDownloading https://formulae.brew.sh/api/cask.jws.json\u001b[0m\n",
      "\u001b[33mWarning:\u001b[0m Treating cmake as a formula. For the cask, use homebrew/cask/cmake or specify the `--cask` flag. To silence this message, use the `--formula` flag.\n",
      "\u001b[33mWarning:\u001b[0m cmake 4.0.1 is already installed and up-to-date.\n",
      "To reinstall 4.0.1, run:\n",
      "  brew reinstall cmake\n",
      "\u001b[33mWarning:\u001b[0m ffmpeg 7.1.1_2 is already installed and up-to-date.\n",
      "To reinstall 7.1.1_2, run:\n",
      "  brew reinstall ffmpeg\n"
     ]
    }
   ],
   "source": [
    "!brew install cmake ffmpeg"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "756534e0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "fatal: destination path 'whisper.cpp' already exists and is not an empty directory.\n",
      "/Users/aza1200/Desktop/RAG/whisper.cpp\n",
      "\u001b[0mCMake Deprecation Warning at CMakeLists.txt:1 (cmake_minimum_required):\n",
      "  Compatibility with CMake < 3.10 will be removed from a future version of\n",
      "  CMake.\n",
      "\n",
      "  Update the VERSION argument <min> value.  Or, use the <min>...<max> syntax\n",
      "  to tell CMake that the project requires at least <min> but has been updated\n",
      "  to work with policies introduced by <max> or earlier.\n",
      "\n",
      "\u001b[0m\n",
      "-- Warning: ccache not found - consider installing it for faster compilation or disable this warning with GGML_CCACHE=OFF\n",
      "-- CMAKE_SYSTEM_PROCESSOR: arm64\n",
      "-- Including CPU backend\n",
      "-- Accelerate framework found\n",
      "-- Could NOT find OpenMP_C (missing: OpenMP_C_FLAGS OpenMP_C_LIB_NAMES) \n",
      "-- Could NOT find OpenMP_CXX (missing: OpenMP_CXX_FLAGS OpenMP_CXX_LIB_NAMES) \n",
      "-- Could NOT find OpenMP (missing: OpenMP_C_FOUND OpenMP_CXX_FOUND) \n",
      "\u001b[33mCMake Warning at ggml/src/ggml-cpu/CMakeLists.txt:63 (message):\n",
      "  OpenMP not found\n",
      "Call Stack (most recent call first):\n",
      "  ggml/src/CMakeLists.txt:299 (ggml_add_cpu_backend_variant_impl)\n",
      "\n",
      "\u001b[0m\n",
      "-- ARM detected\n",
      "-- ARM -mcpu not found, -mcpu=native will be used\n",
      "-- ARM feature DOTPROD enabled\n",
      "-- ARM feature MATMUL_INT8 enabled\n",
      "-- ARM feature FMA enabled\n",
      "-- ARM feature FP16_VECTOR_ARITHMETIC enabled\n",
      "-- ARM feature SME enabled\n",
      "-- Adding CPU backend variant ggml-cpu: -mcpu=native+dotprod+i8mm+nosve+sme \n",
      "-- BLAS found, Libraries: /Applications/Xcode.app/Contents/Developer/Platforms/MacOSX.platform/Developer/SDKs/MacOSX.sdk/System/Library/Frameworks/Accelerate.framework\n",
      "-- BLAS found, Includes: \n",
      "-- Including BLAS backend\n",
      "-- Metal framework found\n",
      "-- Including METAL backend\n",
      "-- Configuring done (0.6s)\n",
      "-- Generating done (0.1s)\n",
      "-- Build files have been written to: /Users/aza1200/Desktop/RAG/whisper.cpp/build\n",
      "[ 16%] Built target ggml-base\n",
      "[ 25%] Built target ggml-metal\n",
      "[ 52%] Built target ggml-cpu\n",
      "[ 56%] Built target ggml-blas\n",
      "[ 60%] Built target ggml\n",
      "[ 64%] Built target whisper\n",
      "[ 75%] Built target common\n",
      "[ 79%] Built target whisper-cli\n",
      "[ 83%] Built target whisper-bench\n",
      "[ 87%] Built target whisper-server\n",
      "[ 91%] Built target quantize\n",
      "[ 95%] Built target main\n",
      "[100%] Built target bench\n",
      "/Users/aza1200/Desktop/RAG\n"
     ]
    }
   ],
   "source": [
    "# 📥 whisper.cpp 클론\n",
    "!git clone https://github.com/ggerganov/whisper.cpp.git\n",
    "\n",
    "# 📁 디렉토리 이동\n",
    "%cd whisper.cpp\n",
    "\n",
    "# 🔨 CMake로 빌드 M4 맥북에서 CPU로 돌려볼려면 이거 써주세용\n",
    "!cmake -B build\n",
    "!cmake --build build --config Release\n",
    "\n",
    "# 다시 디렉토리 밖으로 이동\n",
    "%cd .."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "a2749138",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: huggingface_hub in ./prototype-virtual-env/lib/python3.13/site-packages (0.30.2)\n",
      "Requirement already satisfied: filelock in ./prototype-virtual-env/lib/python3.13/site-packages (from huggingface_hub) (3.18.0)\n",
      "Requirement already satisfied: fsspec>=2023.5.0 in ./prototype-virtual-env/lib/python3.13/site-packages (from huggingface_hub) (2025.3.2)\n",
      "Requirement already satisfied: packaging>=20.9 in ./prototype-virtual-env/lib/python3.13/site-packages (from huggingface_hub) (24.2)\n",
      "Requirement already satisfied: pyyaml>=5.1 in ./prototype-virtual-env/lib/python3.13/site-packages (from huggingface_hub) (6.0.2)\n",
      "Requirement already satisfied: requests in ./prototype-virtual-env/lib/python3.13/site-packages (from huggingface_hub) (2.32.3)\n",
      "Requirement already satisfied: tqdm>=4.42.1 in ./prototype-virtual-env/lib/python3.13/site-packages (from huggingface_hub) (4.67.1)\n",
      "Requirement already satisfied: typing-extensions>=3.7.4.3 in ./prototype-virtual-env/lib/python3.13/site-packages (from huggingface_hub) (4.13.2)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in ./prototype-virtual-env/lib/python3.13/site-packages (from requests->huggingface_hub) (3.4.1)\n",
      "Requirement already satisfied: idna<4,>=2.5 in ./prototype-virtual-env/lib/python3.13/site-packages (from requests->huggingface_hub) (3.10)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in ./prototype-virtual-env/lib/python3.13/site-packages (from requests->huggingface_hub) (2.4.0)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in ./prototype-virtual-env/lib/python3.13/site-packages (from requests->huggingface_hub) (2025.1.31)\n",
      "Requirement already satisfied: langchain in ./prototype-virtual-env/lib/python3.13/site-packages (0.3.23)\n",
      "Requirement already satisfied: langchain-core<1.0.0,>=0.3.51 in ./prototype-virtual-env/lib/python3.13/site-packages (from langchain) (0.3.54)\n",
      "Requirement already satisfied: langchain-text-splitters<1.0.0,>=0.3.8 in ./prototype-virtual-env/lib/python3.13/site-packages (from langchain) (0.3.8)\n",
      "Requirement already satisfied: langsmith<0.4,>=0.1.17 in ./prototype-virtual-env/lib/python3.13/site-packages (from langchain) (0.3.32)\n",
      "Requirement already satisfied: pydantic<3.0.0,>=2.7.4 in ./prototype-virtual-env/lib/python3.13/site-packages (from langchain) (2.11.3)\n",
      "Requirement already satisfied: SQLAlchemy<3,>=1.4 in ./prototype-virtual-env/lib/python3.13/site-packages (from langchain) (2.0.40)\n",
      "Requirement already satisfied: requests<3,>=2 in ./prototype-virtual-env/lib/python3.13/site-packages (from langchain) (2.32.3)\n",
      "Requirement already satisfied: PyYAML>=5.3 in ./prototype-virtual-env/lib/python3.13/site-packages (from langchain) (6.0.2)\n",
      "Requirement already satisfied: tenacity!=8.4.0,<10.0.0,>=8.1.0 in ./prototype-virtual-env/lib/python3.13/site-packages (from langchain-core<1.0.0,>=0.3.51->langchain) (9.1.2)\n",
      "Requirement already satisfied: jsonpatch<2.0,>=1.33 in ./prototype-virtual-env/lib/python3.13/site-packages (from langchain-core<1.0.0,>=0.3.51->langchain) (1.33)\n",
      "Requirement already satisfied: packaging<25,>=23.2 in ./prototype-virtual-env/lib/python3.13/site-packages (from langchain-core<1.0.0,>=0.3.51->langchain) (24.2)\n",
      "Requirement already satisfied: typing-extensions>=4.7 in ./prototype-virtual-env/lib/python3.13/site-packages (from langchain-core<1.0.0,>=0.3.51->langchain) (4.13.2)\n",
      "Requirement already satisfied: httpx<1,>=0.23.0 in ./prototype-virtual-env/lib/python3.13/site-packages (from langsmith<0.4,>=0.1.17->langchain) (0.28.1)\n",
      "Requirement already satisfied: orjson<4.0.0,>=3.9.14 in ./prototype-virtual-env/lib/python3.13/site-packages (from langsmith<0.4,>=0.1.17->langchain) (3.10.16)\n",
      "Requirement already satisfied: requests-toolbelt<2.0.0,>=1.0.0 in ./prototype-virtual-env/lib/python3.13/site-packages (from langsmith<0.4,>=0.1.17->langchain) (1.0.0)\n",
      "Requirement already satisfied: zstandard<0.24.0,>=0.23.0 in ./prototype-virtual-env/lib/python3.13/site-packages (from langsmith<0.4,>=0.1.17->langchain) (0.23.0)\n",
      "Requirement already satisfied: annotated-types>=0.6.0 in ./prototype-virtual-env/lib/python3.13/site-packages (from pydantic<3.0.0,>=2.7.4->langchain) (0.7.0)\n",
      "Requirement already satisfied: pydantic-core==2.33.1 in ./prototype-virtual-env/lib/python3.13/site-packages (from pydantic<3.0.0,>=2.7.4->langchain) (2.33.1)\n",
      "Requirement already satisfied: typing-inspection>=0.4.0 in ./prototype-virtual-env/lib/python3.13/site-packages (from pydantic<3.0.0,>=2.7.4->langchain) (0.4.0)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in ./prototype-virtual-env/lib/python3.13/site-packages (from requests<3,>=2->langchain) (3.4.1)\n",
      "Requirement already satisfied: idna<4,>=2.5 in ./prototype-virtual-env/lib/python3.13/site-packages (from requests<3,>=2->langchain) (3.10)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in ./prototype-virtual-env/lib/python3.13/site-packages (from requests<3,>=2->langchain) (2.4.0)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in ./prototype-virtual-env/lib/python3.13/site-packages (from requests<3,>=2->langchain) (2025.1.31)\n",
      "Requirement already satisfied: anyio in ./prototype-virtual-env/lib/python3.13/site-packages (from httpx<1,>=0.23.0->langsmith<0.4,>=0.1.17->langchain) (4.9.0)\n",
      "Requirement already satisfied: httpcore==1.* in ./prototype-virtual-env/lib/python3.13/site-packages (from httpx<1,>=0.23.0->langsmith<0.4,>=0.1.17->langchain) (1.0.8)\n",
      "Requirement already satisfied: h11<0.15,>=0.13 in ./prototype-virtual-env/lib/python3.13/site-packages (from httpcore==1.*->httpx<1,>=0.23.0->langsmith<0.4,>=0.1.17->langchain) (0.14.0)\n",
      "Requirement already satisfied: jsonpointer>=1.9 in ./prototype-virtual-env/lib/python3.13/site-packages (from jsonpatch<2.0,>=1.33->langchain-core<1.0.0,>=0.3.51->langchain) (3.0.0)\n",
      "Requirement already satisfied: sniffio>=1.1 in ./prototype-virtual-env/lib/python3.13/site-packages (from anyio->httpx<1,>=0.23.0->langsmith<0.4,>=0.1.17->langchain) (1.3.1)\n",
      "Requirement already satisfied: langchain-community in ./prototype-virtual-env/lib/python3.13/site-packages (0.3.21)\n",
      "Requirement already satisfied: langchain-core<1.0.0,>=0.3.51 in ./prototype-virtual-env/lib/python3.13/site-packages (from langchain-community) (0.3.54)\n",
      "Requirement already satisfied: langchain<1.0.0,>=0.3.23 in ./prototype-virtual-env/lib/python3.13/site-packages (from langchain-community) (0.3.23)\n",
      "Requirement already satisfied: SQLAlchemy<3,>=1.4 in ./prototype-virtual-env/lib/python3.13/site-packages (from langchain-community) (2.0.40)\n",
      "Requirement already satisfied: requests<3,>=2 in ./prototype-virtual-env/lib/python3.13/site-packages (from langchain-community) (2.32.3)\n",
      "Requirement already satisfied: PyYAML>=5.3 in ./prototype-virtual-env/lib/python3.13/site-packages (from langchain-community) (6.0.2)\n",
      "Requirement already satisfied: aiohttp<4.0.0,>=3.8.3 in ./prototype-virtual-env/lib/python3.13/site-packages (from langchain-community) (3.11.16)\n",
      "Requirement already satisfied: tenacity!=8.4.0,<10,>=8.1.0 in ./prototype-virtual-env/lib/python3.13/site-packages (from langchain-community) (9.1.2)\n",
      "Requirement already satisfied: dataclasses-json<0.7,>=0.5.7 in ./prototype-virtual-env/lib/python3.13/site-packages (from langchain-community) (0.6.7)\n",
      "Requirement already satisfied: pydantic-settings<3.0.0,>=2.4.0 in ./prototype-virtual-env/lib/python3.13/site-packages (from langchain-community) (2.9.1)\n",
      "Requirement already satisfied: langsmith<0.4,>=0.1.125 in ./prototype-virtual-env/lib/python3.13/site-packages (from langchain-community) (0.3.32)\n",
      "Requirement already satisfied: httpx-sse<1.0.0,>=0.4.0 in ./prototype-virtual-env/lib/python3.13/site-packages (from langchain-community) (0.4.0)\n",
      "Requirement already satisfied: numpy<3,>=1.26.2 in ./prototype-virtual-env/lib/python3.13/site-packages (from langchain-community) (2.2.4)\n",
      "Requirement already satisfied: aiohappyeyeballs>=2.3.0 in ./prototype-virtual-env/lib/python3.13/site-packages (from aiohttp<4.0.0,>=3.8.3->langchain-community) (2.6.1)\n",
      "Requirement already satisfied: aiosignal>=1.1.2 in ./prototype-virtual-env/lib/python3.13/site-packages (from aiohttp<4.0.0,>=3.8.3->langchain-community) (1.3.2)\n",
      "Requirement already satisfied: attrs>=17.3.0 in ./prototype-virtual-env/lib/python3.13/site-packages (from aiohttp<4.0.0,>=3.8.3->langchain-community) (25.3.0)\n",
      "Requirement already satisfied: frozenlist>=1.1.1 in ./prototype-virtual-env/lib/python3.13/site-packages (from aiohttp<4.0.0,>=3.8.3->langchain-community) (1.6.0)\n",
      "Requirement already satisfied: multidict<7.0,>=4.5 in ./prototype-virtual-env/lib/python3.13/site-packages (from aiohttp<4.0.0,>=3.8.3->langchain-community) (6.4.3)\n",
      "Requirement already satisfied: propcache>=0.2.0 in ./prototype-virtual-env/lib/python3.13/site-packages (from aiohttp<4.0.0,>=3.8.3->langchain-community) (0.3.1)\n",
      "Requirement already satisfied: yarl<2.0,>=1.17.0 in ./prototype-virtual-env/lib/python3.13/site-packages (from aiohttp<4.0.0,>=3.8.3->langchain-community) (1.20.0)\n",
      "Requirement already satisfied: marshmallow<4.0.0,>=3.18.0 in ./prototype-virtual-env/lib/python3.13/site-packages (from dataclasses-json<0.7,>=0.5.7->langchain-community) (3.26.1)\n",
      "Requirement already satisfied: typing-inspect<1,>=0.4.0 in ./prototype-virtual-env/lib/python3.13/site-packages (from dataclasses-json<0.7,>=0.5.7->langchain-community) (0.9.0)\n",
      "Requirement already satisfied: langchain-text-splitters<1.0.0,>=0.3.8 in ./prototype-virtual-env/lib/python3.13/site-packages (from langchain<1.0.0,>=0.3.23->langchain-community) (0.3.8)\n",
      "Requirement already satisfied: pydantic<3.0.0,>=2.7.4 in ./prototype-virtual-env/lib/python3.13/site-packages (from langchain<1.0.0,>=0.3.23->langchain-community) (2.11.3)\n",
      "Requirement already satisfied: jsonpatch<2.0,>=1.33 in ./prototype-virtual-env/lib/python3.13/site-packages (from langchain-core<1.0.0,>=0.3.51->langchain-community) (1.33)\n",
      "Requirement already satisfied: packaging<25,>=23.2 in ./prototype-virtual-env/lib/python3.13/site-packages (from langchain-core<1.0.0,>=0.3.51->langchain-community) (24.2)\n",
      "Requirement already satisfied: typing-extensions>=4.7 in ./prototype-virtual-env/lib/python3.13/site-packages (from langchain-core<1.0.0,>=0.3.51->langchain-community) (4.13.2)\n",
      "Requirement already satisfied: httpx<1,>=0.23.0 in ./prototype-virtual-env/lib/python3.13/site-packages (from langsmith<0.4,>=0.1.125->langchain-community) (0.28.1)\n",
      "Requirement already satisfied: orjson<4.0.0,>=3.9.14 in ./prototype-virtual-env/lib/python3.13/site-packages (from langsmith<0.4,>=0.1.125->langchain-community) (3.10.16)\n",
      "Requirement already satisfied: requests-toolbelt<2.0.0,>=1.0.0 in ./prototype-virtual-env/lib/python3.13/site-packages (from langsmith<0.4,>=0.1.125->langchain-community) (1.0.0)\n",
      "Requirement already satisfied: zstandard<0.24.0,>=0.23.0 in ./prototype-virtual-env/lib/python3.13/site-packages (from langsmith<0.4,>=0.1.125->langchain-community) (0.23.0)\n",
      "Requirement already satisfied: python-dotenv>=0.21.0 in ./prototype-virtual-env/lib/python3.13/site-packages (from pydantic-settings<3.0.0,>=2.4.0->langchain-community) (1.1.0)\n",
      "Requirement already satisfied: typing-inspection>=0.4.0 in ./prototype-virtual-env/lib/python3.13/site-packages (from pydantic-settings<3.0.0,>=2.4.0->langchain-community) (0.4.0)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in ./prototype-virtual-env/lib/python3.13/site-packages (from requests<3,>=2->langchain-community) (3.4.1)\n",
      "Requirement already satisfied: idna<4,>=2.5 in ./prototype-virtual-env/lib/python3.13/site-packages (from requests<3,>=2->langchain-community) (3.10)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in ./prototype-virtual-env/lib/python3.13/site-packages (from requests<3,>=2->langchain-community) (2.4.0)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in ./prototype-virtual-env/lib/python3.13/site-packages (from requests<3,>=2->langchain-community) (2025.1.31)\n",
      "Requirement already satisfied: anyio in ./prototype-virtual-env/lib/python3.13/site-packages (from httpx<1,>=0.23.0->langsmith<0.4,>=0.1.125->langchain-community) (4.9.0)\n",
      "Requirement already satisfied: httpcore==1.* in ./prototype-virtual-env/lib/python3.13/site-packages (from httpx<1,>=0.23.0->langsmith<0.4,>=0.1.125->langchain-community) (1.0.8)\n",
      "Requirement already satisfied: h11<0.15,>=0.13 in ./prototype-virtual-env/lib/python3.13/site-packages (from httpcore==1.*->httpx<1,>=0.23.0->langsmith<0.4,>=0.1.125->langchain-community) (0.14.0)\n",
      "Requirement already satisfied: jsonpointer>=1.9 in ./prototype-virtual-env/lib/python3.13/site-packages (from jsonpatch<2.0,>=1.33->langchain-core<1.0.0,>=0.3.51->langchain-community) (3.0.0)\n",
      "Requirement already satisfied: annotated-types>=0.6.0 in ./prototype-virtual-env/lib/python3.13/site-packages (from pydantic<3.0.0,>=2.7.4->langchain<1.0.0,>=0.3.23->langchain-community) (0.7.0)\n",
      "Requirement already satisfied: pydantic-core==2.33.1 in ./prototype-virtual-env/lib/python3.13/site-packages (from pydantic<3.0.0,>=2.7.4->langchain<1.0.0,>=0.3.23->langchain-community) (2.33.1)\n",
      "Requirement already satisfied: mypy-extensions>=0.3.0 in ./prototype-virtual-env/lib/python3.13/site-packages (from typing-inspect<1,>=0.4.0->dataclasses-json<0.7,>=0.5.7->langchain-community) (1.0.0)\n",
      "Requirement already satisfied: sniffio>=1.1 in ./prototype-virtual-env/lib/python3.13/site-packages (from anyio->httpx<1,>=0.23.0->langsmith<0.4,>=0.1.125->langchain-community) (1.3.1)\n",
      "Requirement already satisfied: langchain-openai in ./prototype-virtual-env/lib/python3.13/site-packages (0.3.14)\n",
      "Requirement already satisfied: langchain-core<1.0.0,>=0.3.53 in ./prototype-virtual-env/lib/python3.13/site-packages (from langchain-openai) (0.3.54)\n",
      "Requirement already satisfied: openai<2.0.0,>=1.68.2 in ./prototype-virtual-env/lib/python3.13/site-packages (from langchain-openai) (1.75.0)\n",
      "Requirement already satisfied: tiktoken<1,>=0.7 in ./prototype-virtual-env/lib/python3.13/site-packages (from langchain-openai) (0.9.0)\n",
      "Requirement already satisfied: langsmith<0.4,>=0.1.125 in ./prototype-virtual-env/lib/python3.13/site-packages (from langchain-core<1.0.0,>=0.3.53->langchain-openai) (0.3.32)\n",
      "Requirement already satisfied: tenacity!=8.4.0,<10.0.0,>=8.1.0 in ./prototype-virtual-env/lib/python3.13/site-packages (from langchain-core<1.0.0,>=0.3.53->langchain-openai) (9.1.2)\n",
      "Requirement already satisfied: jsonpatch<2.0,>=1.33 in ./prototype-virtual-env/lib/python3.13/site-packages (from langchain-core<1.0.0,>=0.3.53->langchain-openai) (1.33)\n",
      "Requirement already satisfied: PyYAML>=5.3 in ./prototype-virtual-env/lib/python3.13/site-packages (from langchain-core<1.0.0,>=0.3.53->langchain-openai) (6.0.2)\n",
      "Requirement already satisfied: packaging<25,>=23.2 in ./prototype-virtual-env/lib/python3.13/site-packages (from langchain-core<1.0.0,>=0.3.53->langchain-openai) (24.2)\n",
      "Requirement already satisfied: typing-extensions>=4.7 in ./prototype-virtual-env/lib/python3.13/site-packages (from langchain-core<1.0.0,>=0.3.53->langchain-openai) (4.13.2)\n",
      "Requirement already satisfied: pydantic<3.0.0,>=2.7.4 in ./prototype-virtual-env/lib/python3.13/site-packages (from langchain-core<1.0.0,>=0.3.53->langchain-openai) (2.11.3)\n",
      "Requirement already satisfied: anyio<5,>=3.5.0 in ./prototype-virtual-env/lib/python3.13/site-packages (from openai<2.0.0,>=1.68.2->langchain-openai) (4.9.0)\n",
      "Requirement already satisfied: distro<2,>=1.7.0 in ./prototype-virtual-env/lib/python3.13/site-packages (from openai<2.0.0,>=1.68.2->langchain-openai) (1.9.0)\n",
      "Requirement already satisfied: httpx<1,>=0.23.0 in ./prototype-virtual-env/lib/python3.13/site-packages (from openai<2.0.0,>=1.68.2->langchain-openai) (0.28.1)\n",
      "Requirement already satisfied: jiter<1,>=0.4.0 in ./prototype-virtual-env/lib/python3.13/site-packages (from openai<2.0.0,>=1.68.2->langchain-openai) (0.9.0)\n",
      "Requirement already satisfied: sniffio in ./prototype-virtual-env/lib/python3.13/site-packages (from openai<2.0.0,>=1.68.2->langchain-openai) (1.3.1)\n",
      "Requirement already satisfied: tqdm>4 in ./prototype-virtual-env/lib/python3.13/site-packages (from openai<2.0.0,>=1.68.2->langchain-openai) (4.67.1)\n",
      "Requirement already satisfied: regex>=2022.1.18 in ./prototype-virtual-env/lib/python3.13/site-packages (from tiktoken<1,>=0.7->langchain-openai) (2024.11.6)\n",
      "Requirement already satisfied: requests>=2.26.0 in ./prototype-virtual-env/lib/python3.13/site-packages (from tiktoken<1,>=0.7->langchain-openai) (2.32.3)\n",
      "Requirement already satisfied: idna>=2.8 in ./prototype-virtual-env/lib/python3.13/site-packages (from anyio<5,>=3.5.0->openai<2.0.0,>=1.68.2->langchain-openai) (3.10)\n",
      "Requirement already satisfied: certifi in ./prototype-virtual-env/lib/python3.13/site-packages (from httpx<1,>=0.23.0->openai<2.0.0,>=1.68.2->langchain-openai) (2025.1.31)\n",
      "Requirement already satisfied: httpcore==1.* in ./prototype-virtual-env/lib/python3.13/site-packages (from httpx<1,>=0.23.0->openai<2.0.0,>=1.68.2->langchain-openai) (1.0.8)\n",
      "Requirement already satisfied: h11<0.15,>=0.13 in ./prototype-virtual-env/lib/python3.13/site-packages (from httpcore==1.*->httpx<1,>=0.23.0->openai<2.0.0,>=1.68.2->langchain-openai) (0.14.0)\n",
      "Requirement already satisfied: jsonpointer>=1.9 in ./prototype-virtual-env/lib/python3.13/site-packages (from jsonpatch<2.0,>=1.33->langchain-core<1.0.0,>=0.3.53->langchain-openai) (3.0.0)\n",
      "Requirement already satisfied: orjson<4.0.0,>=3.9.14 in ./prototype-virtual-env/lib/python3.13/site-packages (from langsmith<0.4,>=0.1.125->langchain-core<1.0.0,>=0.3.53->langchain-openai) (3.10.16)\n",
      "Requirement already satisfied: requests-toolbelt<2.0.0,>=1.0.0 in ./prototype-virtual-env/lib/python3.13/site-packages (from langsmith<0.4,>=0.1.125->langchain-core<1.0.0,>=0.3.53->langchain-openai) (1.0.0)\n",
      "Requirement already satisfied: zstandard<0.24.0,>=0.23.0 in ./prototype-virtual-env/lib/python3.13/site-packages (from langsmith<0.4,>=0.1.125->langchain-core<1.0.0,>=0.3.53->langchain-openai) (0.23.0)\n",
      "Requirement already satisfied: annotated-types>=0.6.0 in ./prototype-virtual-env/lib/python3.13/site-packages (from pydantic<3.0.0,>=2.7.4->langchain-core<1.0.0,>=0.3.53->langchain-openai) (0.7.0)\n",
      "Requirement already satisfied: pydantic-core==2.33.1 in ./prototype-virtual-env/lib/python3.13/site-packages (from pydantic<3.0.0,>=2.7.4->langchain-core<1.0.0,>=0.3.53->langchain-openai) (2.33.1)\n",
      "Requirement already satisfied: typing-inspection>=0.4.0 in ./prototype-virtual-env/lib/python3.13/site-packages (from pydantic<3.0.0,>=2.7.4->langchain-core<1.0.0,>=0.3.53->langchain-openai) (0.4.0)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in ./prototype-virtual-env/lib/python3.13/site-packages (from requests>=2.26.0->tiktoken<1,>=0.7->langchain-openai) (3.4.1)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in ./prototype-virtual-env/lib/python3.13/site-packages (from requests>=2.26.0->tiktoken<1,>=0.7->langchain-openai) (2.4.0)\n",
      "Requirement already satisfied: faiss-cpu in ./prototype-virtual-env/lib/python3.13/site-packages (1.10.0)\n",
      "Requirement already satisfied: numpy<3.0,>=1.25.0 in ./prototype-virtual-env/lib/python3.13/site-packages (from faiss-cpu) (2.2.4)\n",
      "Requirement already satisfied: packaging in ./prototype-virtual-env/lib/python3.13/site-packages (from faiss-cpu) (24.2)\n",
      "Requirement already satisfied: gdown in ./prototype-virtual-env/lib/python3.13/site-packages (5.2.0)\n",
      "Requirement already satisfied: beautifulsoup4 in ./prototype-virtual-env/lib/python3.13/site-packages (from gdown) (4.13.4)\n",
      "Requirement already satisfied: filelock in ./prototype-virtual-env/lib/python3.13/site-packages (from gdown) (3.18.0)\n",
      "Requirement already satisfied: requests[socks] in ./prototype-virtual-env/lib/python3.13/site-packages (from gdown) (2.32.3)\n",
      "Requirement already satisfied: tqdm in ./prototype-virtual-env/lib/python3.13/site-packages (from gdown) (4.67.1)\n",
      "Requirement already satisfied: soupsieve>1.2 in ./prototype-virtual-env/lib/python3.13/site-packages (from beautifulsoup4->gdown) (2.6)\n",
      "Requirement already satisfied: typing-extensions>=4.0.0 in ./prototype-virtual-env/lib/python3.13/site-packages (from beautifulsoup4->gdown) (4.13.2)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in ./prototype-virtual-env/lib/python3.13/site-packages (from requests[socks]->gdown) (3.4.1)\n",
      "Requirement already satisfied: idna<4,>=2.5 in ./prototype-virtual-env/lib/python3.13/site-packages (from requests[socks]->gdown) (3.10)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in ./prototype-virtual-env/lib/python3.13/site-packages (from requests[socks]->gdown) (2.4.0)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in ./prototype-virtual-env/lib/python3.13/site-packages (from requests[socks]->gdown) (2025.1.31)\n",
      "Requirement already satisfied: PySocks!=1.5.7,>=1.5.6 in ./prototype-virtual-env/lib/python3.13/site-packages (from requests[socks]->gdown) (1.7.1)\n"
     ]
    }
   ],
   "source": [
    "!pip install huggingface_hub\n",
    "!pip install -U langchain\n",
    "!pip install -U langchain-community\n",
    "!pip install -U langchain-openai\n",
    "!pip install -U faiss-cpu\n",
    "!pip install -U gdown"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1074ddcd",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import json\n",
    "import gdown\n",
    "import shutil\n",
    "import subprocess\n",
    "from getpass import getpass\n",
    "from langchain.chains import RetrievalQA\n",
    "from huggingface_hub import hf_hub_download\n",
    "from langchain_openai import OpenAIEmbeddings\n",
    "from langchain.chat_models import ChatOpenAI\n",
    "from langchain.prompts import PromptTemplate\n",
    "from langchain_core.documents import Document\n",
    "from langchain_community.vectorstores import FAISS\n",
    "from langchain.text_splitter import RecursiveCharacterTextSplitter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "5313479c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "모델 다운로드 완료 : /Users/aza1200/.cache/huggingface/hub/models--ggerganov--whisper.cpp/snapshots/5359861c739e955e79d9a303bcbc70fb988958b1/ggml-small.bin\n"
     ]
    }
   ],
   "source": [
    "# 파일 이름 설정\n",
    "LECTURE_FILE_NAME = \"01\"\n",
    "MODEL_NAME = \"ggml-small.bin\"\n",
    "\n",
    "os.makedirs(\"./AX_Data\", exist_ok=True)\n",
    "os.makedirs(f\"./Sound/{LECTURE_FILE_NAME}\", exist_ok=True)\n",
    "os.makedirs(f\"./Text/{LECTURE_FILE_NAME}\", exist_ok=True)\n",
    "\n",
    "# 🔽 모델 다운로드\n",
    "MODEL_PATH = hf_hub_download(\n",
    "    repo_id=\"ggerganov/whisper.cpp\",\n",
    "    filename=MODEL_NAME\n",
    ")\n",
    "print(f\"모델 다운로드 완료 : {MODEL_PATH}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "8784df8f",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Downloading...\n",
      "From (original): https://drive.google.com/uc?id=1h9lxMPkahHB-dn7p4QBXkMwOZpgVaoSB\n",
      "From (redirected): https://drive.google.com/uc?id=1h9lxMPkahHB-dn7p4QBXkMwOZpgVaoSB&confirm=t&uuid=c14dc6c5-2e22-458f-802d-e3587f360faf\n",
      "To: /Users/aza1200/Desktop/RAG/AX_Data/01.mp4\n",
      "100%|████████████████████████████████████████| 495M/495M [00:06<00:00, 72.0MB/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ 영상 다운로드 완료: ./AX_Data/01.mp4\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "video_path = f\"./AX_Data/{LECTURE_FILE_NAME}.mp4\"\n",
    "# Google Drive에서 영상 다운로드\n",
    "file_id = \"1h9lxMPkahHB-dn7p4QBXkMwOZpgVaoSB\"  # Google Drive 공유 링크에서 file_id 추출\n",
    "url = f\"https://drive.google.com/uc?id={file_id}\"\n",
    "\n",
    "# 다운로드\n",
    "gdown.download(url, video_path, quiet=False)\n",
    "\n",
    "print(f\"✅ 영상 다운로드 완료: {video_path}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "id": "81bf134e",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "ffmpeg version 7.1.1 Copyright (c) 2000-2025 the FFmpeg developers\n",
      "  built with Apple clang version 16.0.0 (clang-1600.0.26.6)\n",
      "  configuration: --prefix=/opt/homebrew/Cellar/ffmpeg/7.1.1_2 --enable-shared --enable-pthreads --enable-version3 --cc=clang --host-cflags= --host-ldflags='-Wl,-ld_classic' --enable-ffplay --enable-gnutls --enable-gpl --enable-libaom --enable-libaribb24 --enable-libbluray --enable-libdav1d --enable-libharfbuzz --enable-libjxl --enable-libmp3lame --enable-libopus --enable-librav1e --enable-librist --enable-librubberband --enable-libsnappy --enable-libsrt --enable-libssh --enable-libsvtav1 --enable-libtesseract --enable-libtheora --enable-libvidstab --enable-libvmaf --enable-libvorbis --enable-libvpx --enable-libwebp --enable-libx264 --enable-libx265 --enable-libxml2 --enable-libxvid --enable-lzma --enable-libfontconfig --enable-libfreetype --enable-frei0r --enable-libass --enable-libopencore-amrnb --enable-libopencore-amrwb --enable-libopenjpeg --enable-libspeex --enable-libsoxr --enable-libzmq --enable-libzimg --disable-libjack --disable-indev=jack --enable-videotoolbox --enable-audiotoolbox --enable-neon\n",
      "  libavutil      59. 39.100 / 59. 39.100\n",
      "  libavcodec     61. 19.101 / 61. 19.101\n",
      "  libavformat    61.  7.100 / 61.  7.100\n",
      "  libavdevice    61.  3.100 / 61.  3.100\n",
      "  libavfilter    10.  4.100 / 10.  4.100\n",
      "  libswscale      8.  3.100 /  8.  3.100\n",
      "  libswresample   5.  3.100 /  5.  3.100\n",
      "  libpostproc    58.  3.100 / 58.  3.100\n",
      "Input #0, mov,mp4,m4a,3gp,3g2,mj2, from './AX_Data/01.mp4':\n",
      "  Metadata:\n",
      "    major_brand     : isom\n",
      "    minor_version   : 512\n",
      "    compatible_brands: isomiso2\n",
      "    creation_time   : 2025-04-16T00:26:51.000000Z\n",
      "  Duration: 04:00:00.06, start: 0.000000, bitrate: 275 kb/s\n",
      "  Stream #0:0[0x2](eng): Video: h264 (Constrained Baseline) (avc1 / 0x31637661), yuv420p(progressive), 1920x1080, 250 kb/s, 16 fps, 16 tbr, 10k tbn (default)\n",
      "      Metadata:\n",
      "        creation_time   : 2025-04-16T00:26:51.000000Z\n",
      "        vendor_id       : [0][0][0][0]\n",
      "  Stream #0:1[0x1](eng): Audio: aac (LC) (mp4a / 0x6134706D), 16000 Hz, mono, fltp, 22 kb/s (default)\n",
      "      Metadata:\n",
      "        creation_time   : 2025-04-16T00:26:51.000000Z\n",
      "        vendor_id       : [0][0][0][0]\n",
      "Stream mapping:\n",
      "  Stream #0:1 -> #0:0 (aac (native) -> pcm_s16le (native))\n",
      "Press [q] to stop, [?] for help\n",
      "[segment @ 0x127e0ea50] Opening './Sound/01/chunk_000.wav' for writing\n",
      "Output #0, segment, to './Sound/01/chunk_%03d.wav':\n",
      "  Metadata:\n",
      "    major_brand     : isom\n",
      "    minor_version   : 512\n",
      "    compatible_brands: isomiso2\n",
      "    encoder         : Lavf61.7.100\n",
      "  Stream #0:0(eng): Audio: pcm_s16le, 16000 Hz, mono, s16, 256 kb/s (default)\n",
      "      Metadata:\n",
      "        creation_time   : 2025-04-16T00:26:51.000000Z\n",
      "        vendor_id       : [0][0][0][0]\n",
      "        encoder         : Lavc61.19.101 pcm_s16le\n",
      "[segment @ 0x127e0ea50] Opening './Sound/01/chunk_001.wav' for writing\n",
      "[segment @ 0x127e0ea50] Opening './Sound/01/chunk_002.wav' for writing\n",
      "[segment @ 0x127e0ea50] Opening './Sound/01/chunk_003.wav' for writing\n",
      "[segment @ 0x127e0ea50] Opening './Sound/01/chunk_004.wav' for writing\n",
      "[segment @ 0x127e0ea50] Opening './Sound/01/chunk_005.wav' for writing\n",
      "[segment @ 0x127e0ea50] Opening './Sound/01/chunk_006.wav' for writing\n",
      "[segment @ 0x127e0ea50] Opening './Sound/01/chunk_007.wav' for writing\n",
      "[segment @ 0x127e0ea50] Opening './Sound/01/chunk_008.wav' for writing\n",
      "[segment @ 0x127e0ea50] Opening './Sound/01/chunk_009.wav' for writing\n",
      "[segment @ 0x127e0ea50] Opening './Sound/01/chunk_010.wav' for writing\n",
      "[segment @ 0x127e0ea50] Opening './Sound/01/chunk_011.wav' for writing\n",
      "[segment @ 0x127e0ea50] Opening './Sound/01/chunk_012.wav' for writing\n",
      "[segment @ 0x127e0ea50] Opening './Sound/01/chunk_013.wav' for writing\n",
      "[segment @ 0x127e0ea50] Opening './Sound/01/chunk_014.wav' for writing\n",
      "[segment @ 0x127e0ea50] Opening './Sound/01/chunk_015.wav' for writing\n",
      "[segment @ 0x127e0ea50] Opening './Sound/01/chunk_016.wav' for writing\n",
      "[segment @ 0x127e0ea50] Opening './Sound/01/chunk_017.wav' for writing\n",
      "[segment @ 0x127e0ea50] Opening './Sound/01/chunk_018.wav' for writing\n",
      "[segment @ 0x127e0ea50] Opening './Sound/01/chunk_019.wav' for writing\n",
      "[segment @ 0x127e0ea50] Opening './Sound/01/chunk_020.wav' for writing\n",
      "[segment @ 0x127e0ea50] Opening './Sound/01/chunk_021.wav' for writing\n",
      "[segment @ 0x127e0ea50] Opening './Sound/01/chunk_022.wav' for writing\n",
      "[segment @ 0x127e0ea50] Opening './Sound/01/chunk_023.wav' for writing\n",
      "[segment @ 0x127e0ea50] Opening './Sound/01/chunk_024.wav' for writing\n",
      "[segment @ 0x127e0ea50] Opening './Sound/01/chunk_025.wav' for writing\n",
      "[segment @ 0x127e0ea50] Opening './Sound/01/chunk_026.wav' for writing\n",
      "[segment @ 0x127e0ea50] Opening './Sound/01/chunk_027.wav' for writing\n",
      "[segment @ 0x127e0ea50] Opening './Sound/01/chunk_028.wav' for writing\n",
      "[segment @ 0x127e0ea50] Opening './Sound/01/chunk_029.wav' for writing\n",
      "[segment @ 0x127e0ea50] Opening './Sound/01/chunk_030.wav' for writing\n",
      "[segment @ 0x127e0ea50] Opening './Sound/01/chunk_031.wav' for writing\n",
      "[segment @ 0x127e0ea50] Opening './Sound/01/chunk_032.wav' for writing\n",
      "[segment @ 0x127e0ea50] Opening './Sound/01/chunk_033.wav' for writing\n",
      "[segment @ 0x127e0ea50] Opening './Sound/01/chunk_034.wav' for writing\n",
      "[segment @ 0x127e0ea50] Opening './Sound/01/chunk_035.wav' for writing\n",
      "[segment @ 0x127e0ea50] Opening './Sound/01/chunk_036.wav' for writing\n",
      "[segment @ 0x127e0ea50] Opening './Sound/01/chunk_037.wav' for writing\n",
      "[segment @ 0x127e0ea50] Opening './Sound/01/chunk_038.wav' for writing\n",
      "[segment @ 0x127e0ea50] Opening './Sound/01/chunk_039.wav' for writing\n",
      "size=N/A time=01:19:54.17 bitrate=N/A speed=2.38e+03x    \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ 무음 제거 + 2분 자르기 한 번에 완료\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[segment @ 0x127e0ea50] Opening './Sound/01/chunk_040.wav' for writing\n",
      "[segment @ 0x127e0ea50] Opening './Sound/01/chunk_041.wav' for writing\n",
      "[out#0/segment @ 0x600000280180] video:0KiB audio:155851KiB subtitle:0KiB other streams:0KiB global headers:0KiB muxing overhead: unknown\n",
      "size=N/A time=01:23:07.21 bitrate=N/A speed=2.09e+03x    \n"
     ]
    }
   ],
   "source": [
    "chunk_output_dir = f\"./Sound/{LECTURE_FILE_NAME}\"\n",
    "\n",
    "os.system(\n",
    "    f\"ffmpeg -y -i {video_path} \"\n",
    "    f\"-vn -ar 16000 -ac 1 \"\n",
    "    f\"-af silenceremove=start_periods=1:start_duration=1:start_threshold=-45dB:\"\n",
    "    f\"stop_periods=-1:stop_duration=1:stop_threshold=-45dB,\"\n",
    "    f\"aresample=async=1 \"\n",
    "    f\"-f segment -segment_time 120 -c:a pcm_s16le \"\n",
    "    f\"{chunk_output_dir}/chunk_%03d.wav\"\n",
    ")\n",
    "\n",
    "print(\"✅ 무음 제거 + 2분 자르기 한 번에 완료\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "id": "dd972961",
   "metadata": {},
   "outputs": [],
   "source": [
    "def whisper_cpp_transcribe(file_name, model_name):\n",
    "    model_label = model_name.replace(\".bin\", \"\")\n",
    "    audio_path = f\"./Sound/{LECTURE_FILE_NAME}/{file_name}.wav\"\n",
    "    output_file_prefix = f\"./Text/{LECTURE_FILE_NAME}/{file_name}_{model_label}\"\n",
    "\n",
    "    os.makedirs(f\"./Text/{LECTURE_FILE_NAME}\", exist_ok=True)\n",
    "\n",
    "    cmd = [\n",
    "        \"./whisper.cpp/build/bin/whisper-cli\",\n",
    "        \"-m\", MODEL_PATH,\n",
    "        \"-f\", audio_path,\n",
    "        \"-l\", \"ko\",\n",
    "        \"-oj\",\n",
    "        \"-of\", output_file_prefix,\n",
    "    ]\n",
    "\n",
    "    subprocess.run(cmd)\n",
    "    print(f\"✅ {file_name} 처리 완료 → {output_file_prefix}.json\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "id": "e5c54aeb",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "whisper_init_from_file_with_params_no_state: loading model from '/Users/aza1200/.cache/huggingface/hub/models--ggerganov--whisper.cpp/snapshots/5359861c739e955e79d9a303bcbc70fb988958b1/ggml-small.bin'\n",
      "whisper_init_with_params_no_state: use gpu    = 1\n",
      "whisper_init_with_params_no_state: flash attn = 0\n",
      "whisper_init_with_params_no_state: gpu_device = 0\n",
      "whisper_init_with_params_no_state: dtw        = 0\n",
      "whisper_init_with_params_no_state: devices    = 3\n",
      "whisper_init_with_params_no_state: backends   = 3\n",
      "whisper_model_load: loading model\n",
      "whisper_model_load: n_vocab       = 51865\n",
      "whisper_model_load: n_audio_ctx   = 1500\n",
      "whisper_model_load: n_audio_state = 768\n",
      "whisper_model_load: n_audio_head  = 12\n",
      "whisper_model_load: n_audio_layer = 12\n",
      "whisper_model_load: n_text_ctx    = 448\n",
      "whisper_model_load: n_text_state  = 768\n",
      "whisper_model_load: n_text_head   = 12\n",
      "whisper_model_load: n_text_layer  = 12\n",
      "whisper_model_load: n_mels        = 80\n",
      "whisper_model_load: ftype         = 1\n",
      "whisper_model_load: qntvr         = 0\n",
      "whisper_model_load: type          = 3 (small)\n",
      "whisper_model_load: adding 1608 extra tokens\n",
      "whisper_model_load: n_langs       = 99\n",
      "whisper_model_load:        Metal total size =   487.01 MB\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "📦 chunk count: 42\n",
      "🎵 chunk files: ['chunk_000.wav', 'chunk_001.wav', 'chunk_002.wav', 'chunk_003.wav', 'chunk_004.wav'] ...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "whisper_model_load: model size    =  487.01 MB\n",
      "whisper_backend_init_gpu: using Metal backend\n",
      "ggml_metal_init: allocating\n",
      "ggml_metal_init: found device: Apple M4 Pro\n",
      "ggml_metal_init: picking default device: Apple M4 Pro\n",
      "ggml_metal_load_library: using embedded metal library\n",
      "ggml_metal_init: GPU name:   Apple M4 Pro\n",
      "ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)\n",
      "ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)\n",
      "ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)\n",
      "ggml_metal_init: simdgroup reduction   = true\n",
      "ggml_metal_init: simdgroup matrix mul. = true\n",
      "ggml_metal_init: has residency sets    = true\n",
      "ggml_metal_init: has bfloat            = true\n",
      "ggml_metal_init: use bfloat            = false\n",
      "ggml_metal_init: hasUnifiedMemory      = true\n",
      "ggml_metal_init: recommendedMaxWorkingSetSize  = 38654.71 MB\n",
      "ggml_metal_init: skipping kernel_get_rows_bf16                     (not supported)\n",
      "ggml_metal_init: skipping kernel_mul_mv_bf16_f32                   (not supported)\n",
      "ggml_metal_init: skipping kernel_mul_mv_bf16_f32_1row              (not supported)\n",
      "ggml_metal_init: skipping kernel_mul_mv_bf16_f32_l4                (not supported)\n",
      "ggml_metal_init: skipping kernel_mul_mv_bf16_bf16                  (not supported)\n",
      "ggml_metal_init: skipping kernel_mul_mv_id_bf16_f32                (not supported)\n",
      "ggml_metal_init: skipping kernel_mul_mm_bf16_f32                   (not supported)\n",
      "ggml_metal_init: skipping kernel_mul_mm_id_bf16_f32                (not supported)\n",
      "ggml_metal_init: skipping kernel_flash_attn_ext_bf16_h64           (not supported)\n",
      "ggml_metal_init: skipping kernel_flash_attn_ext_bf16_h80           (not supported)\n",
      "ggml_metal_init: skipping kernel_flash_attn_ext_bf16_h96           (not supported)\n",
      "ggml_metal_init: skipping kernel_flash_attn_ext_bf16_h112          (not supported)\n",
      "ggml_metal_init: skipping kernel_flash_attn_ext_bf16_h128          (not supported)\n",
      "ggml_metal_init: skipping kernel_flash_attn_ext_bf16_h192          (not supported)\n",
      "ggml_metal_init: skipping kernel_flash_attn_ext_bf16_hk192_hv128   (not supported)\n",
      "ggml_metal_init: skipping kernel_flash_attn_ext_bf16_h256          (not supported)\n",
      "ggml_metal_init: skipping kernel_flash_attn_ext_vec_bf16_h128      (not supported)\n",
      "ggml_metal_init: skipping kernel_flash_attn_ext_vec_bf16_h192      (not supported)\n",
      "ggml_metal_init: skipping kernel_flash_attn_ext_vec_bf16_hk192_hv128 (not supported)\n",
      "ggml_metal_init: skipping kernel_flash_attn_ext_vec_bf16_h256      (not supported)\n",
      "ggml_metal_init: skipping kernel_cpy_f32_bf16                      (not supported)\n",
      "ggml_metal_init: skipping kernel_cpy_bf16_f32                      (not supported)\n",
      "ggml_metal_init: skipping kernel_cpy_bf16_bf16                     (not supported)\n",
      "whisper_backend_init: using BLAS backend\n",
      "whisper_init_state: kv self size  =   18.87 MB\n",
      "whisper_init_state: kv cross size =   56.62 MB\n",
      "whisper_init_state: kv pad  size  =    4.72 MB\n",
      "whisper_init_state: compute buffer (conv)   =   23.37 MB\n",
      "whisper_init_state: compute buffer (encode) =  128.01 MB\n",
      "whisper_init_state: compute buffer (cross)  =    6.18 MB\n",
      "whisper_init_state: compute buffer (decode) =   98.19 MB\n",
      "\n",
      "system_info: n_threads = 4 / 14 | WHISPER : COREML = 0 | OPENVINO = 0 | Metal : EMBED_LIBRARY = 1 | CPU : NEON = 1 | ARM_FMA = 1 | FP16_VA = 1 | MATMUL_INT8 = 1 | DOTPROD = 1 | SME = 1 | ACCELERATE = 1 | AARCH64_REPACK = 1 | \n",
      "\n",
      "main: processing './Sound/01/chunk_000.wav' (1920946 samples, 120.1 sec), 4 threads, 1 processors, 5 beams + best of 5, lang = ko, task = transcribe, timestamps = 1 ...\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "[00:00:00.000 --> 00:00:20.980]   (인터뷰)\n",
      "[00:00:20.980 --> 00:00:25.600]   혹시 그 여러분 젠킨스라는 투를 해본 적이 있다\n",
      "[00:00:25.600 --> 00:00:27.440]   젠킨스 써본 적 있다\n",
      "[00:00:27.440 --> 00:00:29.640]   (인터뷰)\n",
      "[00:00:29.640 --> 00:00:32.480]   혹시 써보실 때 어느 정도 쓰시니 까지 써보셨어요\n",
      "[00:00:32.480 --> 00:00:36.160]   그냥 사용자 입장에서 파이프라인이 짜는 정도?\n",
      "[00:00:36.160 --> 00:00:39.580]   짜고 젠킨스로 그러면 뭘 하셨어요 혹시?\n",
      "[00:00:39.580 --> 00:00:40.740]   (인터뷰)\n",
      "[00:00:40.740 --> 00:00:44.720]   씨아이만 했어요 젠킨스로\n",
      "[00:00:44.720 --> 00:00:47.160]   저희는 어제 컨테이너 빌드 를 했기 때문에\n",
      "[00:00:47.160 --> 00:00:50.040]   오늘은 rgo cd로 별풀할 건데\n",
      "[00:00:50.040 --> 00:00:52.920]   kt 안에서 이제 vmv4 대상으로는\n",
      "[00:00:52.920 --> 00:00:55.440]   cd2로 젠킨스를 쓰고 있거든요\n",
      "[00:00:55.440 --> 00:01:06.560]   그래서 그래서 이제 저희 개발맥을 써서\n",
      "[00:01:06.560 --> 00:01:08.960]   간단하게 이제 토커업을 해서 이미지 가져와서 안에\n",
      "[00:01:08.960 --> 00:01:12.520]   어떻게 부성되어 있고 어떻게 쓰는 지 정도만 확인을 하고\n",
      "[00:01:12.520 --> 00:01:16.600]   그 다음에 저희가 교육용으로 구성해서 rgo cd로 넘어갈게요\n",
      "[00:01:16.600 --> 00:01:22.240]   그러면 다들 토커업을 쓸 수 있는 개발맥을 가져와 보시고\n",
      "[00:01:22.240 --> 00:01:29.200]   그걸 토커업에 이렇게 젠킨스 슬래쉬 젠킨스로 검색을 하면\n",
      "[00:01:29.200 --> 00:01:32.520]   젠킨스라는 오픈소스를 만드는 곳에서\n",
      "[00:01:32.520 --> 00:01:35.440]   배포하는 이미지가 있거든요 컨테이너 이미지\n",
      "[00:01:35.440 --> 00:01:38.520]   이거를 각자 터미널 로컬로 토커풀 받으셔가지고\n",
      "[00:01:38.520 --> 00:01:40.560]   레이티스트로 받으셔도 돼요\n",
      "[00:01:40.560 --> 00:01:43.360]   카피해서 명령어 넣은 다음에\n",
      "[00:01:43.360 --> 00:01:47.280]   이제 로컬PC에 다운로드 받아보겠습니다\n",
      "[00:01:47.280 --> 00:01:52.040]   수업을 직접 들어가서 구성도 되고 명령어를 드릴게요\n",
      "\n",
      "✅ chunk_000 처리 완료 → ./Text/01/chunk_000_ggml-small.json\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "output_json: saving output to './Text/01/chunk_000_ggml-small.json'\n",
      "\n",
      "whisper_print_timings:     load time =   178.23 ms\n",
      "whisper_print_timings:     fallbacks =   0 p /   0 h\n",
      "whisper_print_timings:      mel time =    28.44 ms\n",
      "whisper_print_timings:   sample time =   307.59 ms /  1984 runs (     0.16 ms per run)\n",
      "whisper_print_timings:   encode time =   614.41 ms /     5 runs (   122.88 ms per run)\n",
      "whisper_print_timings:   decode time =    35.53 ms /     9 runs (     3.95 ms per run)\n",
      "whisper_print_timings:   batchd time =  1803.40 ms /  1953 runs (     0.92 ms per run)\n",
      "whisper_print_timings:   prompt time =    83.31 ms /   681 runs (     0.12 ms per run)\n",
      "whisper_print_timings:    total time =  3136.04 ms\n",
      "ggml_metal_free: deallocating\n",
      "whisper_init_from_file_with_params_no_state: loading model from '/Users/aza1200/.cache/huggingface/hub/models--ggerganov--whisper.cpp/snapshots/5359861c739e955e79d9a303bcbc70fb988958b1/ggml-small.bin'\n",
      "whisper_init_with_params_no_state: use gpu    = 1\n",
      "whisper_init_with_params_no_state: flash attn = 0\n",
      "whisper_init_with_params_no_state: gpu_device = 0\n",
      "whisper_init_with_params_no_state: dtw        = 0\n",
      "whisper_init_with_params_no_state: devices    = 3\n",
      "whisper_init_with_params_no_state: backends   = 3\n",
      "whisper_model_load: loading model\n",
      "whisper_model_load: n_vocab       = 51865\n",
      "whisper_model_load: n_audio_ctx   = 1500\n",
      "whisper_model_load: n_audio_state = 768\n",
      "whisper_model_load: n_audio_head  = 12\n",
      "whisper_model_load: n_audio_layer = 12\n",
      "whisper_model_load: n_text_ctx    = 448\n",
      "whisper_model_load: n_text_state  = 768\n",
      "whisper_model_load: n_text_head   = 12\n",
      "whisper_model_load: n_text_layer  = 12\n",
      "whisper_model_load: n_mels        = 80\n",
      "whisper_model_load: ftype         = 1\n",
      "whisper_model_load: qntvr         = 0\n",
      "whisper_model_load: type          = 3 (small)\n",
      "whisper_model_load: adding 1608 extra tokens\n",
      "whisper_model_load: n_langs       = 99\n",
      "whisper_model_load:        Metal total size =   487.01 MB\n",
      "whisper_model_load: model size    =  487.01 MB\n",
      "whisper_backend_init_gpu: using Metal backend\n",
      "ggml_metal_init: allocating\n",
      "ggml_metal_init: found device: Apple M4 Pro\n",
      "ggml_metal_init: picking default device: Apple M4 Pro\n",
      "ggml_metal_load_library: using embedded metal library\n",
      "ggml_metal_init: GPU name:   Apple M4 Pro\n",
      "ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)\n",
      "ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)\n",
      "ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)\n",
      "ggml_metal_init: simdgroup reduction   = true\n",
      "ggml_metal_init: simdgroup matrix mul. = true\n",
      "ggml_metal_init: has residency sets    = true\n",
      "ggml_metal_init: has bfloat            = true\n",
      "ggml_metal_init: use bfloat            = false\n",
      "ggml_metal_init: hasUnifiedMemory      = true\n",
      "ggml_metal_init: recommendedMaxWorkingSetSize  = 38654.71 MB\n",
      "ggml_metal_init: skipping kernel_get_rows_bf16                     (not supported)\n",
      "ggml_metal_init: skipping kernel_mul_mv_bf16_f32                   (not supported)\n",
      "ggml_metal_init: skipping kernel_mul_mv_bf16_f32_1row              (not supported)\n",
      "ggml_metal_init: skipping kernel_mul_mv_bf16_f32_l4                (not supported)\n",
      "ggml_metal_init: skipping kernel_mul_mv_bf16_bf16                  (not supported)\n",
      "ggml_metal_init: skipping kernel_mul_mv_id_bf16_f32                (not supported)\n",
      "ggml_metal_init: skipping kernel_mul_mm_bf16_f32                   (not supported)\n",
      "ggml_metal_init: skipping kernel_mul_mm_id_bf16_f32                (not supported)\n",
      "ggml_metal_init: skipping kernel_flash_attn_ext_bf16_h64           (not supported)\n",
      "ggml_metal_init: skipping kernel_flash_attn_ext_bf16_h80           (not supported)\n",
      "ggml_metal_init: skipping kernel_flash_attn_ext_bf16_h96           (not supported)\n",
      "ggml_metal_init: skipping kernel_flash_attn_ext_bf16_h112          (not supported)\n",
      "ggml_metal_init: skipping kernel_flash_attn_ext_bf16_h128          (not supported)\n",
      "ggml_metal_init: skipping kernel_flash_attn_ext_bf16_h192          (not supported)\n",
      "ggml_metal_init: skipping kernel_flash_attn_ext_bf16_hk192_hv128   (not supported)\n",
      "ggml_metal_init: skipping kernel_flash_attn_ext_bf16_h256          (not supported)\n",
      "ggml_metal_init: skipping kernel_flash_attn_ext_vec_bf16_h128      (not supported)\n",
      "ggml_metal_init: skipping kernel_flash_attn_ext_vec_bf16_h192      (not supported)\n",
      "ggml_metal_init: skipping kernel_flash_attn_ext_vec_bf16_hk192_hv128 (not supported)\n",
      "ggml_metal_init: skipping kernel_flash_attn_ext_vec_bf16_h256      (not supported)\n",
      "ggml_metal_init: skipping kernel_cpy_f32_bf16                      (not supported)\n",
      "ggml_metal_init: skipping kernel_cpy_bf16_f32                      (not supported)\n",
      "ggml_metal_init: skipping kernel_cpy_bf16_bf16                     (not supported)\n",
      "whisper_backend_init: using BLAS backend\n",
      "whisper_init_state: kv self size  =   18.87 MB\n",
      "whisper_init_state: kv cross size =   56.62 MB\n",
      "whisper_init_state: kv pad  size  =    4.72 MB\n",
      "whisper_init_state: compute buffer (conv)   =   23.37 MB\n",
      "whisper_init_state: compute buffer (encode) =  128.01 MB\n",
      "whisper_init_state: compute buffer (cross)  =    6.18 MB\n",
      "whisper_init_state: compute buffer (decode) =   98.19 MB\n",
      "\n",
      "system_info: n_threads = 4 / 14 | WHISPER : COREML = 0 | OPENVINO = 0 | Metal : EMBED_LIBRARY = 1 | CPU : NEON = 1 | ARM_FMA = 1 | FP16_VA = 1 | MATMUL_INT8 = 1 | DOTPROD = 1 | SME = 1 | ACCELERATE = 1 | AARCH64_REPACK = 1 | \n",
      "\n",
      "main: processing './Sound/01/chunk_001.wav' (1919954 samples, 120.0 sec), 4 threads, 1 processors, 5 beams + best of 5, lang = ko, task = transcribe, timestamps = 1 ...\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "[00:00:00.000 --> 00:00:09.880]   공사는 4월 27일까지 예정되어 있으며 공사의 기간 중 다소의 소음과 진동이 발생될 수 있고\n",
      "[00:00:09.880 --> 00:00:16.640]   비상 엘리베이서 오랜 제한의 불편사항이 출해될 수 있으니 양해해 주시기 바랍니다.\n",
      "[00:00:16.640 --> 00:00:19.120]   다시 한 번 알려드립니다.\n",
      "[00:00:19.120 --> 00:00:25.120]   현재 지경동 살칭에서 교육장 공축 공사가 진행되고 있습니다.\n",
      "[00:00:25.120 --> 00:00:32.960]   공사는 4월 27일까지 예정되어 있으며 공사의 기간 중 다소의 소음과 진동이 발생될 수 있고\n",
      "[00:00:32.960 --> 00:00:39.680]   비상 엘리베이서 오랜 제한의 불편사항이 출해물 수 있으니 양해해 주시기 바랍니다.\n",
      "[00:00:39.680 --> 00:00:41.680]   감사합니다.\n",
      "[00:00:41.680 --> 00:00:45.680]   다음 되셨나요?\n",
      "[00:00:45.680 --> 00:00:51.520]   도커플 되셨으면 저는 이제 사내 환경에서 보여드릴 거고 여러분은 편하게 개발맥에서 하시면 되는데\n",
      "[00:00:51.520 --> 00:00:56.280]   도커플 컨테이너를 실행하는 명령원은 잘 계시겠죠?\n",
      "[00:00:56.280 --> 00:00:59.360]   도커플은 대본으로 돌릴 거고\n",
      "[00:00:59.360 --> 00:01:01.360]   그다음에 컨테이너 이름 저하고\n",
      "[00:01:01.360 --> 00:01:04.200]   진킨스 뭐 더비스 이런 거 하고\n",
      "[00:01:04.200 --> 00:01:05.120]   포트를 열어줘야 돼요.\n",
      "[00:01:05.120 --> 00:01:09.600]   8080에 서비스 포트가 8080이라서 그냥 편하게 열어주시고\n",
      "[00:01:09.600 --> 00:01:11.840]   그다음에 컨테이너 이미지 있겠죠.\n",
      "[00:01:11.840 --> 00:01:15.200]   오레이 키스 이런 거 이길죠.\n",
      "[00:01:15.200 --> 00:01:19.400]   뭐 에러가 날 수도 있긴 한데 어쨌든 이런 식으로 명령어를 쓰셔서\n",
      "[00:01:19.400 --> 00:01:23.680]   지금 혹시 다른 거 8080 포트로 돌리고 계시면\n",
      "[00:01:23.680 --> 00:01:28.360]   옆에 있는 포트 포딩 해주는 포트를 그냥 커스테면서 바꿔주시면 되고\n",
      "[00:01:28.360 --> 00:01:31.360]   그렇게 해서 한 번 프로세스 띄워볼게요.\n",
      "[00:01:31.360 --> 00:01:34.760]   도커플 어느 명령어 실행할게요.\n",
      "[00:01:34.760 --> 00:01:36.760]   도커플이 오래 걸려요?\n",
      "[00:01:36.760 --> 00:01:38.200]   아 그래요?\n",
      "[00:01:38.200 --> 00:01:41.440]   혹시 컨테이너 프로세스 띄우셨나요?\n",
      "[00:01:41.440 --> 00:01:46.680]   만약에 제 명령어가 잘 안 먹으면 성공하신 분이 그냥 슬레게 공교를 주셔도 돼요.\n",
      "[00:01:46.680 --> 00:01:50.160]   저는 약간 가이드 시겨드로만 드리는 거라서\n",
      "[00:01:50.160 --> 00:01:54.600]   혹시 이진홍 님 승인됐다고 하는데\n",
      "[00:01:54.600 --> 00:01:59.560]   멤버 추가 해볼게요.\n",
      "\n",
      "✅ chunk_001 처리 완료 → ./Text/01/chunk_001_ggml-small.json\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "output_json: saving output to './Text/01/chunk_001_ggml-small.json'\n",
      "\n",
      "whisper_print_timings:     load time =   163.20 ms\n",
      "whisper_print_timings:     fallbacks =   0 p /   0 h\n",
      "whisper_print_timings:      mel time =    39.31 ms\n",
      "whisper_print_timings:   sample time =   453.63 ms /  2748 runs (     0.17 ms per run)\n",
      "whisper_print_timings:   encode time =   600.58 ms /     5 runs (   120.12 ms per run)\n",
      "whisper_print_timings:   decode time =     0.00 ms /     1 runs (     0.00 ms per run)\n",
      "whisper_print_timings:   batchd time =  2509.28 ms /  2726 runs (     0.92 ms per run)\n",
      "whisper_print_timings:   prompt time =    96.01 ms /   799 runs (     0.12 ms per run)\n",
      "whisper_print_timings:    total time =  3934.10 ms\n",
      "ggml_metal_free: deallocating\n",
      "whisper_init_from_file_with_params_no_state: loading model from '/Users/aza1200/.cache/huggingface/hub/models--ggerganov--whisper.cpp/snapshots/5359861c739e955e79d9a303bcbc70fb988958b1/ggml-small.bin'\n",
      "whisper_init_with_params_no_state: use gpu    = 1\n",
      "whisper_init_with_params_no_state: flash attn = 0\n",
      "whisper_init_with_params_no_state: gpu_device = 0\n",
      "whisper_init_with_params_no_state: dtw        = 0\n",
      "whisper_init_with_params_no_state: devices    = 3\n",
      "whisper_init_with_params_no_state: backends   = 3\n",
      "whisper_model_load: loading model\n",
      "whisper_model_load: n_vocab       = 51865\n",
      "whisper_model_load: n_audio_ctx   = 1500\n",
      "whisper_model_load: n_audio_state = 768\n",
      "whisper_model_load: n_audio_head  = 12\n",
      "whisper_model_load: n_audio_layer = 12\n",
      "whisper_model_load: n_text_ctx    = 448\n",
      "whisper_model_load: n_text_state  = 768\n",
      "whisper_model_load: n_text_head   = 12\n",
      "whisper_model_load: n_text_layer  = 12\n",
      "whisper_model_load: n_mels        = 80\n",
      "whisper_model_load: ftype         = 1\n",
      "whisper_model_load: qntvr         = 0\n",
      "whisper_model_load: type          = 3 (small)\n",
      "whisper_model_load: adding 1608 extra tokens\n",
      "whisper_model_load: n_langs       = 99\n",
      "whisper_model_load:        Metal total size =   487.01 MB\n",
      "whisper_model_load: model size    =  487.01 MB\n",
      "whisper_backend_init_gpu: using Metal backend\n",
      "ggml_metal_init: allocating\n",
      "ggml_metal_init: found device: Apple M4 Pro\n",
      "ggml_metal_init: picking default device: Apple M4 Pro\n",
      "ggml_metal_load_library: using embedded metal library\n",
      "ggml_metal_init: GPU name:   Apple M4 Pro\n",
      "ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)\n",
      "ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)\n",
      "ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)\n",
      "ggml_metal_init: simdgroup reduction   = true\n",
      "ggml_metal_init: simdgroup matrix mul. = true\n",
      "ggml_metal_init: has residency sets    = true\n",
      "ggml_metal_init: has bfloat            = true\n",
      "ggml_metal_init: use bfloat            = false\n",
      "ggml_metal_init: hasUnifiedMemory      = true\n",
      "ggml_metal_init: recommendedMaxWorkingSetSize  = 38654.71 MB\n",
      "ggml_metal_init: skipping kernel_get_rows_bf16                     (not supported)\n",
      "ggml_metal_init: skipping kernel_mul_mv_bf16_f32                   (not supported)\n",
      "ggml_metal_init: skipping kernel_mul_mv_bf16_f32_1row              (not supported)\n",
      "ggml_metal_init: skipping kernel_mul_mv_bf16_f32_l4                (not supported)\n",
      "ggml_metal_init: skipping kernel_mul_mv_bf16_bf16                  (not supported)\n",
      "ggml_metal_init: skipping kernel_mul_mv_id_bf16_f32                (not supported)\n",
      "ggml_metal_init: skipping kernel_mul_mm_bf16_f32                   (not supported)\n",
      "ggml_metal_init: skipping kernel_mul_mm_id_bf16_f32                (not supported)\n",
      "ggml_metal_init: skipping kernel_flash_attn_ext_bf16_h64           (not supported)\n",
      "ggml_metal_init: skipping kernel_flash_attn_ext_bf16_h80           (not supported)\n",
      "ggml_metal_init: skipping kernel_flash_attn_ext_bf16_h96           (not supported)\n",
      "ggml_metal_init: skipping kernel_flash_attn_ext_bf16_h112          (not supported)\n",
      "ggml_metal_init: skipping kernel_flash_attn_ext_bf16_h128          (not supported)\n",
      "ggml_metal_init: skipping kernel_flash_attn_ext_bf16_h192          (not supported)\n",
      "ggml_metal_init: skipping kernel_flash_attn_ext_bf16_hk192_hv128   (not supported)\n",
      "ggml_metal_init: skipping kernel_flash_attn_ext_bf16_h256          (not supported)\n",
      "ggml_metal_init: skipping kernel_flash_attn_ext_vec_bf16_h128      (not supported)\n",
      "ggml_metal_init: skipping kernel_flash_attn_ext_vec_bf16_h192      (not supported)\n",
      "ggml_metal_init: skipping kernel_flash_attn_ext_vec_bf16_hk192_hv128 (not supported)\n",
      "ggml_metal_init: skipping kernel_flash_attn_ext_vec_bf16_h256      (not supported)\n",
      "ggml_metal_init: skipping kernel_cpy_f32_bf16                      (not supported)\n",
      "ggml_metal_init: skipping kernel_cpy_bf16_f32                      (not supported)\n",
      "ggml_metal_init: skipping kernel_cpy_bf16_bf16                     (not supported)\n",
      "whisper_backend_init: using BLAS backend\n",
      "whisper_init_state: kv self size  =   18.87 MB\n",
      "whisper_init_state: kv cross size =   56.62 MB\n",
      "whisper_init_state: kv pad  size  =    4.72 MB\n",
      "whisper_init_state: compute buffer (conv)   =   23.37 MB\n",
      "whisper_init_state: compute buffer (encode) =  128.01 MB\n",
      "whisper_init_state: compute buffer (cross)  =    6.18 MB\n",
      "whisper_init_state: compute buffer (decode) =   98.19 MB\n",
      "\n",
      "system_info: n_threads = 4 / 14 | WHISPER : COREML = 0 | OPENVINO = 0 | Metal : EMBED_LIBRARY = 1 | CPU : NEON = 1 | ARM_FMA = 1 | FP16_VA = 1 | MATMUL_INT8 = 1 | DOTPROD = 1 | SME = 1 | ACCELERATE = 1 | AARCH64_REPACK = 1 | \n",
      "\n",
      "main: processing './Sound/01/chunk_002.wav' (1919188 samples, 119.9 sec), 4 threads, 1 processors, 5 beams + best of 5, lang = ko, task = transcribe, timestamps = 1 ...\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "[00:00:00.000 --> 00:00:02.000]   기품지력시팀 맞으세요?\n",
      "[00:00:02.000 --> 00:00:04.760]   네 드디어 얻어드렸습니다.\n",
      "[00:00:04.760 --> 00:00:06.760]   감사합니다.\n",
      "[00:00:06.760 --> 00:00:11.940]   그래도 진호님은 오늘 배포할 이미지가 아직 없을 거잖아요.\n",
      "[00:00:11.940 --> 00:00:18.280]   acr에다가 그거 이제 옆에 동지분 거 같이 오늘 배포 실습만 하는 거라서\n",
      "[00:00:18.280 --> 00:00:22.160]   호주 배꼽하고 ci는 다 흐름만 이해해줄 것 같아요.\n",
      "[00:00:22.160 --> 00:00:27.760]   처음에 이제 그래서 컨테이너 프로세스 띄우고 로커러스트 8080으로 웹접습 하면\n",
      "[00:00:27.760 --> 00:00:31.980]   이렇게 처음에 어드민 세덕을 하는 화면이 나와야 되거든요.\n",
      "[00:00:31.980 --> 00:00:35.980]   저희 나오셨네요.\n",
      "[00:00:35.980 --> 00:00:37.980]   저희는 저 화면이 안되고.\n",
      "[00:00:37.980 --> 00:00:39.980]   언급하시려고 하고 있었어요.\n",
      "[00:00:39.980 --> 00:00:41.980]   아 그래? 잠시만요.\n",
      "[00:00:41.980 --> 00:00:49.020]   아니 이게 이미지가 약간 다른데.\n",
      "[00:00:49.020 --> 00:00:55.640]   그러면 그 벌랍진 킨스로 이제 어드민 패스워드가 필요한데\n",
      "[00:00:55.640 --> 00:01:00.620]   이게 저랑 이미지가 약간 달라질 거 같고 그 경우에는 그냥 터미널 하나 더 여셔서\n",
      "[00:01:00.620 --> 00:01:01.620]   선생님께 공유해 드릴게요.\n",
      "[00:01:01.620 --> 00:01:03.620]   터미널 하나 더 열어서\n",
      "[00:01:03.620 --> 00:01:06.620]   김소원님께서 공유해 주셨네요.\n",
      "[00:01:06.620 --> 00:01:07.620]   터미널 하나 더 열어서.\n",
      "[00:01:07.620 --> 00:01:13.620]   아니 아까 데몬 뒤로 띄었으니까 그냥 바로 그 터미널에서 이 아래 명료 복사 무척 하시면 되거든요.\n",
      "[00:01:13.620 --> 00:01:17.620]   그럼 이게 초기 세덕 비밀번호가 이제 프로세스에 남아 있는 거예요.\n",
      "[00:01:17.620 --> 00:01:22.620]   그래서 이걸로 확인하셔서 넣으시면 저랑 같은 화면이 나올 겁니다.\n",
      "[00:01:22.620 --> 00:01:26.620]   플로그인 깔면 됩니다.\n",
      "[00:01:26.620 --> 00:01:29.600]   그냥 추천해 주는 걸로 다 갈아주시면 되요.\n",
      "[00:01:29.600 --> 00:01:33.600]   네.\n",
      "[00:01:33.600 --> 00:01:35.600]   아니 안 보여요.\n",
      "[00:01:35.600 --> 00:01:59.600]   (댓글 읽는 중)\n",
      "\n",
      "✅ chunk_002 처리 완료 → ./Text/01/chunk_002_ggml-small.json\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "output_json: saving output to './Text/01/chunk_002_ggml-small.json'\n",
      "\n",
      "whisper_print_timings:     load time =   177.87 ms\n",
      "whisper_print_timings:     fallbacks =   1 p /   0 h\n",
      "whisper_print_timings:      mel time =    28.41 ms\n",
      "whisper_print_timings:   sample time =   362.43 ms /  2240 runs (     0.16 ms per run)\n",
      "whisper_print_timings:   encode time =   580.33 ms /     5 runs (   116.07 ms per run)\n",
      "whisper_print_timings:   decode time =   153.64 ms /    37 runs (     4.15 ms per run)\n",
      "whisper_print_timings:   batchd time =  1964.52 ms /  2176 runs (     0.90 ms per run)\n",
      "whisper_print_timings:   prompt time =   119.10 ms /  1042 runs (     0.11 ms per run)\n",
      "whisper_print_timings:    total time =  3464.11 ms\n",
      "ggml_metal_free: deallocating\n",
      "whisper_init_from_file_with_params_no_state: loading model from '/Users/aza1200/.cache/huggingface/hub/models--ggerganov--whisper.cpp/snapshots/5359861c739e955e79d9a303bcbc70fb988958b1/ggml-small.bin'\n",
      "whisper_init_with_params_no_state: use gpu    = 1\n",
      "whisper_init_with_params_no_state: flash attn = 0\n",
      "whisper_init_with_params_no_state: gpu_device = 0\n",
      "whisper_init_with_params_no_state: dtw        = 0\n",
      "whisper_init_with_params_no_state: devices    = 3\n",
      "whisper_init_with_params_no_state: backends   = 3\n",
      "whisper_model_load: loading model\n",
      "whisper_model_load: n_vocab       = 51865\n",
      "whisper_model_load: n_audio_ctx   = 1500\n",
      "whisper_model_load: n_audio_state = 768\n",
      "whisper_model_load: n_audio_head  = 12\n",
      "whisper_model_load: n_audio_layer = 12\n",
      "whisper_model_load: n_text_ctx    = 448\n",
      "whisper_model_load: n_text_state  = 768\n",
      "whisper_model_load: n_text_head   = 12\n",
      "whisper_model_load: n_text_layer  = 12\n",
      "whisper_model_load: n_mels        = 80\n",
      "whisper_model_load: ftype         = 1\n",
      "whisper_model_load: qntvr         = 0\n",
      "whisper_model_load: type          = 3 (small)\n",
      "whisper_model_load: adding 1608 extra tokens\n",
      "whisper_model_load: n_langs       = 99\n",
      "whisper_model_load:        Metal total size =   487.01 MB\n",
      "whisper_model_load: model size    =  487.01 MB\n",
      "whisper_backend_init_gpu: using Metal backend\n",
      "ggml_metal_init: allocating\n",
      "ggml_metal_init: found device: Apple M4 Pro\n",
      "ggml_metal_init: picking default device: Apple M4 Pro\n",
      "ggml_metal_load_library: using embedded metal library\n",
      "ggml_metal_init: GPU name:   Apple M4 Pro\n",
      "ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)\n",
      "ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)\n",
      "ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)\n",
      "ggml_metal_init: simdgroup reduction   = true\n",
      "ggml_metal_init: simdgroup matrix mul. = true\n",
      "ggml_metal_init: has residency sets    = true\n",
      "ggml_metal_init: has bfloat            = true\n",
      "ggml_metal_init: use bfloat            = false\n",
      "ggml_metal_init: hasUnifiedMemory      = true\n",
      "ggml_metal_init: recommendedMaxWorkingSetSize  = 38654.71 MB\n",
      "ggml_metal_init: skipping kernel_get_rows_bf16                     (not supported)\n",
      "ggml_metal_init: skipping kernel_mul_mv_bf16_f32                   (not supported)\n",
      "ggml_metal_init: skipping kernel_mul_mv_bf16_f32_1row              (not supported)\n",
      "ggml_metal_init: skipping kernel_mul_mv_bf16_f32_l4                (not supported)\n",
      "ggml_metal_init: skipping kernel_mul_mv_bf16_bf16                  (not supported)\n",
      "ggml_metal_init: skipping kernel_mul_mv_id_bf16_f32                (not supported)\n",
      "ggml_metal_init: skipping kernel_mul_mm_bf16_f32                   (not supported)\n",
      "ggml_metal_init: skipping kernel_mul_mm_id_bf16_f32                (not supported)\n",
      "ggml_metal_init: skipping kernel_flash_attn_ext_bf16_h64           (not supported)\n",
      "ggml_metal_init: skipping kernel_flash_attn_ext_bf16_h80           (not supported)\n",
      "ggml_metal_init: skipping kernel_flash_attn_ext_bf16_h96           (not supported)\n",
      "ggml_metal_init: skipping kernel_flash_attn_ext_bf16_h112          (not supported)\n",
      "ggml_metal_init: skipping kernel_flash_attn_ext_bf16_h128          (not supported)\n",
      "ggml_metal_init: skipping kernel_flash_attn_ext_bf16_h192          (not supported)\n",
      "ggml_metal_init: skipping kernel_flash_attn_ext_bf16_hk192_hv128   (not supported)\n",
      "ggml_metal_init: skipping kernel_flash_attn_ext_bf16_h256          (not supported)\n",
      "ggml_metal_init: skipping kernel_flash_attn_ext_vec_bf16_h128      (not supported)\n",
      "ggml_metal_init: skipping kernel_flash_attn_ext_vec_bf16_h192      (not supported)\n",
      "ggml_metal_init: skipping kernel_flash_attn_ext_vec_bf16_hk192_hv128 (not supported)\n",
      "ggml_metal_init: skipping kernel_flash_attn_ext_vec_bf16_h256      (not supported)\n",
      "ggml_metal_init: skipping kernel_cpy_f32_bf16                      (not supported)\n",
      "ggml_metal_init: skipping kernel_cpy_bf16_f32                      (not supported)\n",
      "ggml_metal_init: skipping kernel_cpy_bf16_bf16                     (not supported)\n",
      "whisper_backend_init: using BLAS backend\n",
      "whisper_init_state: kv self size  =   18.87 MB\n",
      "whisper_init_state: kv cross size =   56.62 MB\n",
      "whisper_init_state: kv pad  size  =    4.72 MB\n",
      "whisper_init_state: compute buffer (conv)   =   23.37 MB\n",
      "whisper_init_state: compute buffer (encode) =  128.01 MB\n",
      "whisper_init_state: compute buffer (cross)  =    6.18 MB\n",
      "whisper_init_state: compute buffer (decode) =   98.19 MB\n",
      "\n",
      "system_info: n_threads = 4 / 14 | WHISPER : COREML = 0 | OPENVINO = 0 | Metal : EMBED_LIBRARY = 1 | CPU : NEON = 1 | ARM_FMA = 1 | FP16_VA = 1 | MATMUL_INT8 = 1 | DOTPROD = 1 | SME = 1 | ACCELERATE = 1 | AARCH64_REPACK = 1 | \n",
      "\n",
      "main: processing './Sound/01/chunk_003.wav' (1920202 samples, 120.0 sec), 4 threads, 1 processors, 5 beams + best of 5, lang = ko, task = transcribe, timestamps = 1 ...\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "[00:00:00.000 --> 00:00:02.000]   괜찮았으시다면 안 됐겠지?\n",
      "[00:00:30.000 --> 00:00:32.000]   괜찮았으시다면 안 됐겠지?\n",
      "[00:00:32.000 --> 00:00:34.000]   괜찮았으시다면 안 됐겠지?\n",
      "[00:00:34.000 --> 00:00:36.000]   괜찮았으시다면 안 됐겠지?\n",
      "[00:00:36.000 --> 00:00:38.000]   괜찮았으시다면 안 됐겠지?\n",
      "[00:00:38.000 --> 00:00:40.000]   괜찮았으시다면 안 됐겠지?\n",
      "[00:00:40.000 --> 00:00:42.000]   괜찮았으시다면 안 됐겠지?\n",
      "[00:00:42.000 --> 00:00:44.000]   괜찮았으시다면 안 됐겠지?\n",
      "[00:00:44.000 --> 00:00:46.000]   괜찮았으시다면 안 됐겠지?\n",
      "[00:00:46.000 --> 00:00:48.000]   괜찮았으시다면 안 됐겠지?\n",
      "[00:00:48.000 --> 00:00:50.000]   괜찮았으시다면 안 됐겠지?\n",
      "[00:00:50.000 --> 00:00:52.000]   괜찮았으시다면 안 됐겠지?\n",
      "[00:00:52.000 --> 00:00:54.000]   괜찮았으시다면 안 됐겠지?\n",
      "[00:00:54.000 --> 00:00:56.000]   괜찮았으시다면 안 됐겠지?\n",
      "[00:00:56.000 --> 00:00:58.000]   괜찮았으시다면 안 됐겠지?\n",
      "[00:00:58.000 --> 00:01:00.000]   괜찮았으시다면 안 됐겠지?\n",
      "[00:01:00.000 --> 00:01:02.000]   괜찮았으시다면 안 됐겠지?\n",
      "[00:01:02.000 --> 00:01:04.000]   괜찮았으시다면 안 됐겠지?\n",
      "[00:01:04.000 --> 00:01:06.000]   괜찮았으시다면 안 됐겠지?\n",
      "[00:01:06.000 --> 00:01:08.000]   괜찮았으시다면 안 됐겠지?\n",
      "[00:01:08.000 --> 00:01:10.000]   괜찮았으시다면 안 됐겠지?\n",
      "[00:01:10.000 --> 00:01:12.000]   괜찮았으시다면 안 됐겠지?\n",
      "[00:01:12.000 --> 00:01:14.000]   괜찮았으시다면 안 됐겠지?\n",
      "[00:01:14.000 --> 00:01:16.000]   괜찮았으시다면 안 됐겠지?\n",
      "[00:01:16.000 --> 00:01:18.000]   괜찮았으시다면 안 됐겠지?\n",
      "[00:01:18.000 --> 00:01:20.000]   괜찮았으시다면 안 됐겠지?\n",
      "[00:01:20.000 --> 00:01:22.000]   괜찮았으시다면 안 됐겠지?\n",
      "[00:01:22.000 --> 00:01:24.000]   괜찮았으시다면 안 됐겠지?\n",
      "[00:01:24.000 --> 00:01:26.000]   괜찮았으시다면 안 됐겠지?\n",
      "[00:01:26.000 --> 00:01:28.000]   괜찮았으시다면 안 됐겠지?\n",
      "[00:01:28.000 --> 00:01:30.000]   괜찮았으시다면 안 됐겠지?\n",
      "[00:01:30.000 --> 00:01:32.000]   괜찮았으시다면 안 됐겠지?\n",
      "[00:01:32.000 --> 00:01:34.000]   괜찮았으시다면 안 됐겠지?\n",
      "[00:01:34.000 --> 00:01:36.000]   괜찮았으시다면 안 됐겠지?\n",
      "[00:01:36.000 --> 00:01:38.000]   괜찮았으시다면 안 됐겠지?\n",
      "[00:01:38.000 --> 00:01:40.000]   괜찮았으시다면 안 됐겠지?\n",
      "[00:01:40.000 --> 00:01:42.000]   괜찮았으시다면 안 됐겠지?\n",
      "[00:01:42.000 --> 00:01:44.000]   괜찮았으시다면 안 됐겠지?\n",
      "[00:01:44.000 --> 00:01:46.000]   괜찮았으시다면 안 됐겠지?\n",
      "[00:01:46.000 --> 00:01:48.000]   괜찮았으시다면 안 됐겠지?\n",
      "[00:01:48.000 --> 00:01:50.000]   괜찮았으시다면 안 됐겠지?\n",
      "[00:01:50.000 --> 00:01:52.000]   괜찮았으시다면 안 됐겠지?\n",
      "[00:01:52.000 --> 00:01:54.000]   괜찮았으시다면 안 됐겠지?\n",
      "[00:01:54.000 --> 00:01:56.000]   괜찮았으시다면 안 됐겠지?\n",
      "\n",
      "✅ chunk_003 처리 완료 → ./Text/01/chunk_003_ggml-small.json\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "output_json: saving output to './Text/01/chunk_003_ggml-small.json'\n",
      "\n",
      "whisper_print_timings:     load time =   184.33 ms\n",
      "whisper_print_timings:     fallbacks =   1 p /   0 h\n",
      "whisper_print_timings:      mel time =    28.74 ms\n",
      "whisper_print_timings:   sample time =   604.46 ms /  3434 runs (     0.18 ms per run)\n",
      "whisper_print_timings:   encode time =   617.41 ms /     5 runs (   123.48 ms per run)\n",
      "whisper_print_timings:   decode time =   419.76 ms /   109 runs (     3.85 ms per run)\n",
      "whisper_print_timings:   batchd time =  3871.62 ms /  3301 runs (     1.17 ms per run)\n",
      "whisper_print_timings:   prompt time =    84.81 ms /   657 runs (     0.13 ms per run)\n",
      "whisper_print_timings:    total time =  5878.70 ms\n",
      "ggml_metal_free: deallocating\n",
      "whisper_init_from_file_with_params_no_state: loading model from '/Users/aza1200/.cache/huggingface/hub/models--ggerganov--whisper.cpp/snapshots/5359861c739e955e79d9a303bcbc70fb988958b1/ggml-small.bin'\n",
      "whisper_init_with_params_no_state: use gpu    = 1\n",
      "whisper_init_with_params_no_state: flash attn = 0\n",
      "whisper_init_with_params_no_state: gpu_device = 0\n",
      "whisper_init_with_params_no_state: dtw        = 0\n",
      "whisper_init_with_params_no_state: devices    = 3\n",
      "whisper_init_with_params_no_state: backends   = 3\n",
      "whisper_model_load: loading model\n",
      "whisper_model_load: n_vocab       = 51865\n",
      "whisper_model_load: n_audio_ctx   = 1500\n",
      "whisper_model_load: n_audio_state = 768\n",
      "whisper_model_load: n_audio_head  = 12\n",
      "whisper_model_load: n_audio_layer = 12\n",
      "whisper_model_load: n_text_ctx    = 448\n",
      "whisper_model_load: n_text_state  = 768\n",
      "whisper_model_load: n_text_head   = 12\n",
      "whisper_model_load: n_text_layer  = 12\n",
      "whisper_model_load: n_mels        = 80\n",
      "whisper_model_load: ftype         = 1\n",
      "whisper_model_load: qntvr         = 0\n",
      "whisper_model_load: type          = 3 (small)\n",
      "whisper_model_load: adding 1608 extra tokens\n",
      "whisper_model_load: n_langs       = 99\n",
      "whisper_model_load:        Metal total size =   487.01 MB\n",
      "whisper_model_load: model size    =  487.01 MB\n",
      "whisper_backend_init_gpu: using Metal backend\n",
      "ggml_metal_init: allocating\n",
      "ggml_metal_init: found device: Apple M4 Pro\n",
      "ggml_metal_init: picking default device: Apple M4 Pro\n",
      "ggml_metal_load_library: using embedded metal library\n",
      "ggml_metal_init: GPU name:   Apple M4 Pro\n",
      "ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)\n",
      "ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)\n",
      "ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)\n",
      "ggml_metal_init: simdgroup reduction   = true\n",
      "ggml_metal_init: simdgroup matrix mul. = true\n",
      "ggml_metal_init: has residency sets    = true\n",
      "ggml_metal_init: has bfloat            = true\n",
      "ggml_metal_init: use bfloat            = false\n",
      "ggml_metal_init: hasUnifiedMemory      = true\n",
      "ggml_metal_init: recommendedMaxWorkingSetSize  = 38654.71 MB\n",
      "ggml_metal_init: skipping kernel_get_rows_bf16                     (not supported)\n",
      "ggml_metal_init: skipping kernel_mul_mv_bf16_f32                   (not supported)\n",
      "ggml_metal_init: skipping kernel_mul_mv_bf16_f32_1row              (not supported)\n",
      "ggml_metal_init: skipping kernel_mul_mv_bf16_f32_l4                (not supported)\n",
      "ggml_metal_init: skipping kernel_mul_mv_bf16_bf16                  (not supported)\n",
      "ggml_metal_init: skipping kernel_mul_mv_id_bf16_f32                (not supported)\n",
      "ggml_metal_init: skipping kernel_mul_mm_bf16_f32                   (not supported)\n",
      "ggml_metal_init: skipping kernel_mul_mm_id_bf16_f32                (not supported)\n",
      "ggml_metal_init: skipping kernel_flash_attn_ext_bf16_h64           (not supported)\n",
      "ggml_metal_init: skipping kernel_flash_attn_ext_bf16_h80           (not supported)\n",
      "ggml_metal_init: skipping kernel_flash_attn_ext_bf16_h96           (not supported)\n",
      "ggml_metal_init: skipping kernel_flash_attn_ext_bf16_h112          (not supported)\n",
      "ggml_metal_init: skipping kernel_flash_attn_ext_bf16_h128          (not supported)\n",
      "ggml_metal_init: skipping kernel_flash_attn_ext_bf16_h192          (not supported)\n",
      "ggml_metal_init: skipping kernel_flash_attn_ext_bf16_hk192_hv128   (not supported)\n",
      "ggml_metal_init: skipping kernel_flash_attn_ext_bf16_h256          (not supported)\n",
      "ggml_metal_init: skipping kernel_flash_attn_ext_vec_bf16_h128      (not supported)\n",
      "ggml_metal_init: skipping kernel_flash_attn_ext_vec_bf16_h192      (not supported)\n",
      "ggml_metal_init: skipping kernel_flash_attn_ext_vec_bf16_hk192_hv128 (not supported)\n",
      "ggml_metal_init: skipping kernel_flash_attn_ext_vec_bf16_h256      (not supported)\n",
      "ggml_metal_init: skipping kernel_cpy_f32_bf16                      (not supported)\n",
      "ggml_metal_init: skipping kernel_cpy_bf16_f32                      (not supported)\n",
      "ggml_metal_init: skipping kernel_cpy_bf16_bf16                     (not supported)\n",
      "whisper_backend_init: using BLAS backend\n",
      "whisper_init_state: kv self size  =   18.87 MB\n",
      "whisper_init_state: kv cross size =   56.62 MB\n",
      "whisper_init_state: kv pad  size  =    4.72 MB\n",
      "whisper_init_state: compute buffer (conv)   =   23.37 MB\n",
      "whisper_init_state: compute buffer (encode) =  128.01 MB\n",
      "whisper_init_state: compute buffer (cross)  =    6.18 MB\n",
      "whisper_init_state: compute buffer (decode) =   98.19 MB\n",
      "\n",
      "system_info: n_threads = 4 / 14 | WHISPER : COREML = 0 | OPENVINO = 0 | Metal : EMBED_LIBRARY = 1 | CPU : NEON = 1 | ARM_FMA = 1 | FP16_VA = 1 | MATMUL_INT8 = 1 | DOTPROD = 1 | SME = 1 | ACCELERATE = 1 | AARCH64_REPACK = 1 | \n",
      "\n",
      "main: processing './Sound/01/chunk_004.wav' (1920258 samples, 120.0 sec), 4 threads, 1 processors, 5 beams + best of 5, lang = ko, task = transcribe, timestamps = 1 ...\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "[00:00:00.000 --> 00:00:01.440]   - 네. - 네.\n",
      "[00:00:01.440 --> 00:00:03.780]   - 여기 진짜 진짜 잘 찍는 거 같아요. - 네.\n",
      "[00:00:03.780 --> 00:00:06.100]   - 진짜 이쁘다. - 진짜 이쁘다.\n",
      "[00:00:06.100 --> 00:00:08.480]   - 네. - 되셨어요?\n",
      "[00:00:08.480 --> 00:00:11.320]   - 네, 네. 아, 됐어요. - 여기다가 이거 넣는 거죠.\n",
      "[00:00:11.320 --> 00:00:12.600]   - 이거 한 번 더? - 네.\n",
      "[00:00:12.600 --> 00:00:13.940]   - 오. - 됐어.\n",
      "[00:00:13.940 --> 00:00:16.380]   - 됐어. - 됐어, 됐어, 됐어.\n",
      "[00:00:16.380 --> 00:00:18.200]   - 됐지? - 네.\n",
      "[00:00:18.200 --> 00:00:19.680]   - 네. - 됐어.\n",
      "[00:00:19.680 --> 00:00:22.500]   - 오, 나 이렇게 재밌었어요. - 됐어, 됐어.\n",
      "[00:00:22.500 --> 00:00:24.140]   - 네네. - 그럼 셀프 씨는.\n",
      "[00:00:24.140 --> 00:00:25.740]   - 셀프 씨는? - 네.\n",
      "[00:00:25.740 --> 00:00:27.580]   - 셀프 씨는. - 셀프 씨는.\n",
      "[00:00:27.580 --> 00:00:28.940]   - 이거 좀 해. - 지금?\n",
      "[00:00:28.940 --> 00:00:30.160]   - 응, 응.\n",
      "[00:00:30.160 --> 00:00:34.440]   - 셀프 씨는. - 콘셉트에서 죽이고 한 거 다시 해 볼자.\n",
      "[00:00:34.440 --> 00:00:36.440]   - 셀프 씨는. - 더 내리고.\n",
      "[00:00:36.440 --> 00:00:37.880]   - 셀프 씨는.\n",
      "[00:00:37.880 --> 00:00:41.160]   - 중시 다른 분들은 다 이 화면 나오셨나요? - 네.\n",
      "[00:00:41.160 --> 00:00:42.560]   - 됐어요, 됐어요? - 네.\n",
      "[00:00:42.560 --> 00:00:45.120]   그리고 초기에 이제 저랑 설정이 다른 이유는\n",
      "[00:00:45.120 --> 00:00:49.120]   지금 제가 보여드린 이 화면의 이미지는 약간 안에\n",
      "[00:00:49.120 --> 00:00:51.480]   초기 설정이 좀 스킵된 이미지고\n",
      "[00:00:51.480 --> 00:00:54.440]   여러분들 쓰시면 도커브에 있는 거, 레이디스타 바로 긁어 왔으니까\n",
      "[00:00:54.440 --> 00:00:56.560]   초기 설정이 필요한 거일 거예요.\n",
      "[00:00:56.560 --> 00:01:00.340]   그래서 젠키스가 추천해 주는 클러그인 다음 받아도 되고\n",
      "[00:01:00.340 --> 00:01:02.820]   그냥 바로바로 패스해서 넘긴 다음에\n",
      "[00:01:02.820 --> 00:01:04.940]   저희 계정은 이제 그냥 편한 대로 만들면 됩니다.\n",
      "[00:01:04.940 --> 00:01:11.980]   저는 주로 테스트할 때 그냥 어드민, 어드민, 어드민, 어드민이 어드민을 넣기 때문에\n",
      "[00:01:11.980 --> 00:01:14.780]   이렇게 그냥 하셔도 되고 편하신 대로 그러니까\n",
      "[00:01:14.780 --> 00:01:18.140]   각자 본인의 젠키스 서비스에 들어갈 거라서\n",
      "[00:01:18.140 --> 00:01:21.860]   편하게 한번 만드신 다음에 어드민 유저 만들어주시고\n",
      "[00:01:21.860 --> 00:01:26.260]   이렇게 로그인 하면 어드민 만내면 이제 젠키스 홈에 들어갈 수 있어요.\n",
      "[00:01:26.260 --> 00:01:29.060]   음, 되셨나요?\n",
      "[00:01:29.060 --> 00:01:32.380]   자, 그러면은 젠키스를 간단하게 천천히 둘러보고\n",
      "[00:01:32.380 --> 00:01:34.480]   젠키스로 직접 배포까지는 안 할 거거든요.\n",
      "[00:01:34.480 --> 00:01:38.440]   저희가 배포할 VM을 25개를 꾸려두진 않아서\n",
      "[00:01:38.440 --> 00:01:40.280]   우선 그냥 젠키스란 툴이 뭔지만\n",
      "[00:01:40.280 --> 00:01:42.920]   좀 간단하게 둘러보는 시간을 받겠습니다.\n",
      "[00:01:42.920 --> 00:01:46.720]   첫 번째로 만약에 여러분들이 서비스 본인의 팀에 가서\n",
      "[00:01:46.720 --> 00:01:51.400]   각자 우리는 VM 보고를 할 거다라는 자정 하에 이제 준비를 하는 건데\n",
      "[00:01:51.400 --> 00:01:57.200]   이런 젠키스의 어드민 권한을 부여한 이미지를 저희 팀에서 제공하고 있어요, 대국 스팀에서.\n",
      "[00:01:57.200 --> 00:02:00.300]   그래서 여러분들이 받을 것도 동일하게 어드민이\n",
      "\n",
      "✅ chunk_004 처리 완료 → ./Text/01/chunk_004_ggml-small.json\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "output_json: saving output to './Text/01/chunk_004_ggml-small.json'\n",
      "\n",
      "whisper_print_timings:     load time =   175.72 ms\n",
      "whisper_print_timings:     fallbacks =   0 p /   0 h\n",
      "whisper_print_timings:      mel time =    29.08 ms\n",
      "whisper_print_timings:   sample time =   558.55 ms /  3342 runs (     0.17 ms per run)\n",
      "whisper_print_timings:   encode time =   604.75 ms /     5 runs (   120.95 ms per run)\n",
      "whisper_print_timings:   decode time =   112.09 ms /    26 runs (     4.31 ms per run)\n",
      "whisper_print_timings:   batchd time =  3096.16 ms /  3294 runs (     0.94 ms per run)\n",
      "whisper_print_timings:   prompt time =   103.73 ms /   882 runs (     0.12 ms per run)\n",
      "whisper_print_timings:    total time =  4749.73 ms\n",
      "ggml_metal_free: deallocating\n",
      "whisper_init_from_file_with_params_no_state: loading model from '/Users/aza1200/.cache/huggingface/hub/models--ggerganov--whisper.cpp/snapshots/5359861c739e955e79d9a303bcbc70fb988958b1/ggml-small.bin'\n",
      "whisper_init_with_params_no_state: use gpu    = 1\n",
      "whisper_init_with_params_no_state: flash attn = 0\n",
      "whisper_init_with_params_no_state: gpu_device = 0\n",
      "whisper_init_with_params_no_state: dtw        = 0\n",
      "whisper_init_with_params_no_state: devices    = 3\n",
      "whisper_init_with_params_no_state: backends   = 3\n",
      "whisper_model_load: loading model\n",
      "whisper_model_load: n_vocab       = 51865\n",
      "whisper_model_load: n_audio_ctx   = 1500\n",
      "whisper_model_load: n_audio_state = 768\n",
      "whisper_model_load: n_audio_head  = 12\n",
      "whisper_model_load: n_audio_layer = 12\n",
      "whisper_model_load: n_text_ctx    = 448\n",
      "whisper_model_load: n_text_state  = 768\n",
      "whisper_model_load: n_text_head   = 12\n",
      "whisper_model_load: n_text_layer  = 12\n",
      "whisper_model_load: n_mels        = 80\n",
      "whisper_model_load: ftype         = 1\n",
      "whisper_model_load: qntvr         = 0\n",
      "whisper_model_load: type          = 3 (small)\n",
      "whisper_model_load: adding 1608 extra tokens\n",
      "whisper_model_load: n_langs       = 99\n",
      "whisper_model_load:        Metal total size =   487.01 MB\n",
      "whisper_model_load: model size    =  487.01 MB\n",
      "whisper_backend_init_gpu: using Metal backend\n",
      "ggml_metal_init: allocating\n",
      "ggml_metal_init: found device: Apple M4 Pro\n",
      "ggml_metal_init: picking default device: Apple M4 Pro\n",
      "ggml_metal_load_library: using embedded metal library\n",
      "ggml_metal_init: GPU name:   Apple M4 Pro\n",
      "ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)\n",
      "ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)\n",
      "ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)\n",
      "ggml_metal_init: simdgroup reduction   = true\n",
      "ggml_metal_init: simdgroup matrix mul. = true\n",
      "ggml_metal_init: has residency sets    = true\n",
      "ggml_metal_init: has bfloat            = true\n",
      "ggml_metal_init: use bfloat            = false\n",
      "ggml_metal_init: hasUnifiedMemory      = true\n",
      "ggml_metal_init: recommendedMaxWorkingSetSize  = 38654.71 MB\n",
      "ggml_metal_init: skipping kernel_get_rows_bf16                     (not supported)\n",
      "ggml_metal_init: skipping kernel_mul_mv_bf16_f32                   (not supported)\n",
      "ggml_metal_init: skipping kernel_mul_mv_bf16_f32_1row              (not supported)\n",
      "ggml_metal_init: skipping kernel_mul_mv_bf16_f32_l4                (not supported)\n",
      "ggml_metal_init: skipping kernel_mul_mv_bf16_bf16                  (not supported)\n",
      "ggml_metal_init: skipping kernel_mul_mv_id_bf16_f32                (not supported)\n",
      "ggml_metal_init: skipping kernel_mul_mm_bf16_f32                   (not supported)\n",
      "ggml_metal_init: skipping kernel_mul_mm_id_bf16_f32                (not supported)\n",
      "ggml_metal_init: skipping kernel_flash_attn_ext_bf16_h64           (not supported)\n",
      "ggml_metal_init: skipping kernel_flash_attn_ext_bf16_h80           (not supported)\n",
      "ggml_metal_init: skipping kernel_flash_attn_ext_bf16_h96           (not supported)\n",
      "ggml_metal_init: skipping kernel_flash_attn_ext_bf16_h112          (not supported)\n",
      "ggml_metal_init: skipping kernel_flash_attn_ext_bf16_h128          (not supported)\n",
      "ggml_metal_init: skipping kernel_flash_attn_ext_bf16_h192          (not supported)\n",
      "ggml_metal_init: skipping kernel_flash_attn_ext_bf16_hk192_hv128   (not supported)\n",
      "ggml_metal_init: skipping kernel_flash_attn_ext_bf16_h256          (not supported)\n",
      "ggml_metal_init: skipping kernel_flash_attn_ext_vec_bf16_h128      (not supported)\n",
      "ggml_metal_init: skipping kernel_flash_attn_ext_vec_bf16_h192      (not supported)\n",
      "ggml_metal_init: skipping kernel_flash_attn_ext_vec_bf16_hk192_hv128 (not supported)\n",
      "ggml_metal_init: skipping kernel_flash_attn_ext_vec_bf16_h256      (not supported)\n",
      "ggml_metal_init: skipping kernel_cpy_f32_bf16                      (not supported)\n",
      "ggml_metal_init: skipping kernel_cpy_bf16_f32                      (not supported)\n",
      "ggml_metal_init: skipping kernel_cpy_bf16_bf16                     (not supported)\n",
      "whisper_backend_init: using BLAS backend\n",
      "whisper_init_state: kv self size  =   18.87 MB\n",
      "whisper_init_state: kv cross size =   56.62 MB\n",
      "whisper_init_state: kv pad  size  =    4.72 MB\n",
      "whisper_init_state: compute buffer (conv)   =   23.37 MB\n",
      "whisper_init_state: compute buffer (encode) =  128.01 MB\n",
      "whisper_init_state: compute buffer (cross)  =    6.18 MB\n",
      "whisper_init_state: compute buffer (decode) =   98.19 MB\n",
      "\n",
      "system_info: n_threads = 4 / 14 | WHISPER : COREML = 0 | OPENVINO = 0 | Metal : EMBED_LIBRARY = 1 | CPU : NEON = 1 | ARM_FMA = 1 | FP16_VA = 1 | MATMUL_INT8 = 1 | DOTPROD = 1 | SME = 1 | ACCELERATE = 1 | AARCH64_REPACK = 1 | \n",
      "\n",
      "main: processing './Sound/01/chunk_005.wav' (1920041 samples, 120.0 sec), 4 threads, 1 processors, 5 beams + best of 5, lang = ko, task = transcribe, timestamps = 1 ...\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "[00:00:00.000 --> 00:00:04.500]   그래서 지금 보이는 화면이 여러분들이 직접 쓸 젠키스라고 생각하면 됩니다.\n",
      "[00:00:04.500 --> 00:00:07.000]   만약에 VNBF를 하시는 경우라면\n",
      "[00:00:07.000 --> 00:00:12.000]   그래서 어드밍 권한이랑 유저 권한이랑 다르게 어드밍만 할 수 있는 게 뭐가 있냐\n",
      "[00:00:12.000 --> 00:00:15.000]   이 왼쪽에 젠키스 관리라는 탭이 있어요.\n",
      "[00:00:15.000 --> 00:00:20.500]   젠키스 관리 탭에 들어가서 이런 에러 같은 거는 그냥 간단히 무시해 주시면 되고요.\n",
      "[00:00:20.500 --> 00:00:23.000]   무시해본 무시하고 일단 지금은\n",
      "[00:00:23.000 --> 00:00:27.000]   들어가서 뭐 이런 게 있죠.\n",
      "[00:00:27.000 --> 00:00:31.000]   뭐 시스템 컴피규레이션 같은 것도 볼 수 있고\n",
      "[00:00:31.000 --> 00:00:33.000]   젠키스에 대한 전반적인 설정.\n",
      "[00:00:33.000 --> 00:00:37.000]   그래서 시스템 설정에 이 젠키스라는 게 사실\n",
      "[00:00:37.000 --> 00:00:40.000]   DB는 쓸 수는 있지만 잘 안 쓰고요.\n",
      "[00:00:40.000 --> 00:00:45.000]   젠키스는 다 서버 내부에 8시스템을 가지고 있어요.\n",
      "[00:00:45.000 --> 00:00:47.000]   그러니까 8시스템을\n",
      "[00:00:47.000 --> 00:00:52.000]   그러니까 젠키스의 모든 정보는 리눅스 8시스템을 저장이 됩니다.\n",
      "[00:00:52.000 --> 00:00:56.500]   그리고 거기에 대한 홈패스가 이 바와 젠키스 언더바 홈이라고 생각하시면 돼요.\n",
      "[00:00:56.500 --> 00:01:00.500]   젠키스에서 이제 이루어지는 모든 액션이나\n",
      "[00:01:00.500 --> 00:01:03.500]   잡이라든가 좀 더 그걸 잡이라든가 아니면\n",
      "[00:01:03.500 --> 00:01:08.500]   액세스 로그라든가 잡히스토리, 빌드 히스토리 이런 거에 대한 모든 정보가\n",
      "[00:01:08.500 --> 00:01:11.500]   이 바, 젠키스, 홈 아래에 있다 라고 기억하시면 되고\n",
      "[00:01:11.500 --> 00:01:15.500]   좀다가 잡 하나 만든 다음에 컴테이너 안에 들어가서 한번 보겠습니다.\n",
      "[00:01:15.500 --> 00:01:18.500]   이거 이슈큐터라는 거는\n",
      "[00:01:18.500 --> 00:01:21.500]   젠키스가 이제 서버실 분들은 알겠고\n",
      "[00:01:21.500 --> 00:01:23.500]   좀 모르실 수도 있는데\n",
      "[00:01:23.500 --> 00:01:26.500]   잡이라는 기본 단위로 이제 배포를 해요.\n",
      "[00:01:26.500 --> 00:01:27.500]   그 잡이 이제\n",
      "[00:01:27.500 --> 00:01:29.500]   알고시디를 해보셨다면\n",
      "[00:01:29.500 --> 00:01:31.500]   하나의 뭐라 해야 될까?\n",
      "[00:01:31.500 --> 00:01:32.500]   어플리케이션이라고 생각을 하시면 돼요.\n",
      "[00:01:32.500 --> 00:01:35.500]   알고시디에서는 어플리케이션을 만들어서\n",
      "[00:01:35.500 --> 00:01:37.500]   기탑 연동에서 야물로 배포를 하잖아요.\n",
      "[00:01:37.500 --> 00:01:42.500]   젠키스에서는 그 잡 단위가 바로 어플리케이션의 뜻한다라고 생각하시면 되고\n",
      "[00:01:42.500 --> 00:01:46.500]   그거를 수행할 이슈큐터 수를 여기에서 설정할 수 있습니다.\n",
      "[00:01:46.500 --> 00:01:50.500]   그러니까 젠키스가 최대로 띄울 수 있는 프로세스의 수라고 생각하시면 돼요.\n",
      "[00:01:50.500 --> 00:01:52.500]   생각하시면 되고\n",
      "[00:01:52.500 --> 00:01:54.500]   그 다음에\n",
      "[00:01:54.500 --> 00:02:00.500]   젠키스 url 이런 거는 다 로코로스트 8080이나 ip로 붙어서 이제 쓰실 것 같고\n",
      "\n",
      "✅ chunk_005 처리 완료 → ./Text/01/chunk_005_ggml-small.json\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "output_json: saving output to './Text/01/chunk_005_ggml-small.json'\n",
      "\n",
      "whisper_print_timings:     load time =   179.59 ms\n",
      "whisper_print_timings:     fallbacks =   0 p /   0 h\n",
      "whisper_print_timings:      mel time =    28.20 ms\n",
      "whisper_print_timings:   sample time =   465.72 ms /  3063 runs (     0.15 ms per run)\n",
      "whisper_print_timings:   encode time =   571.11 ms /     5 runs (   114.22 ms per run)\n",
      "whisper_print_timings:   decode time =   114.33 ms /    27 runs (     4.23 ms per run)\n",
      "whisper_print_timings:   batchd time =  2748.98 ms /  3014 runs (     0.91 ms per run)\n",
      "whisper_print_timings:   prompt time =    94.67 ms /   825 runs (     0.11 ms per run)\n",
      "whisper_print_timings:    total time =  4270.99 ms\n",
      "ggml_metal_free: deallocating\n",
      "whisper_init_from_file_with_params_no_state: loading model from '/Users/aza1200/.cache/huggingface/hub/models--ggerganov--whisper.cpp/snapshots/5359861c739e955e79d9a303bcbc70fb988958b1/ggml-small.bin'\n",
      "whisper_init_with_params_no_state: use gpu    = 1\n",
      "whisper_init_with_params_no_state: flash attn = 0\n",
      "whisper_init_with_params_no_state: gpu_device = 0\n",
      "whisper_init_with_params_no_state: dtw        = 0\n",
      "whisper_init_with_params_no_state: devices    = 3\n",
      "whisper_init_with_params_no_state: backends   = 3\n",
      "whisper_model_load: loading model\n",
      "whisper_model_load: n_vocab       = 51865\n",
      "whisper_model_load: n_audio_ctx   = 1500\n",
      "whisper_model_load: n_audio_state = 768\n",
      "whisper_model_load: n_audio_head  = 12\n",
      "whisper_model_load: n_audio_layer = 12\n",
      "whisper_model_load: n_text_ctx    = 448\n",
      "whisper_model_load: n_text_state  = 768\n",
      "whisper_model_load: n_text_head   = 12\n",
      "whisper_model_load: n_text_layer  = 12\n",
      "whisper_model_load: n_mels        = 80\n",
      "whisper_model_load: ftype         = 1\n",
      "whisper_model_load: qntvr         = 0\n",
      "whisper_model_load: type          = 3 (small)\n",
      "whisper_model_load: adding 1608 extra tokens\n",
      "whisper_model_load: n_langs       = 99\n",
      "whisper_model_load:        Metal total size =   487.01 MB\n",
      "whisper_model_load: model size    =  487.01 MB\n",
      "whisper_backend_init_gpu: using Metal backend\n",
      "ggml_metal_init: allocating\n",
      "ggml_metal_init: found device: Apple M4 Pro\n",
      "ggml_metal_init: picking default device: Apple M4 Pro\n",
      "ggml_metal_load_library: using embedded metal library\n",
      "ggml_metal_init: GPU name:   Apple M4 Pro\n",
      "ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)\n",
      "ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)\n",
      "ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)\n",
      "ggml_metal_init: simdgroup reduction   = true\n",
      "ggml_metal_init: simdgroup matrix mul. = true\n",
      "ggml_metal_init: has residency sets    = true\n",
      "ggml_metal_init: has bfloat            = true\n",
      "ggml_metal_init: use bfloat            = false\n",
      "ggml_metal_init: hasUnifiedMemory      = true\n",
      "ggml_metal_init: recommendedMaxWorkingSetSize  = 38654.71 MB\n",
      "ggml_metal_init: skipping kernel_get_rows_bf16                     (not supported)\n",
      "ggml_metal_init: skipping kernel_mul_mv_bf16_f32                   (not supported)\n",
      "ggml_metal_init: skipping kernel_mul_mv_bf16_f32_1row              (not supported)\n",
      "ggml_metal_init: skipping kernel_mul_mv_bf16_f32_l4                (not supported)\n",
      "ggml_metal_init: skipping kernel_mul_mv_bf16_bf16                  (not supported)\n",
      "ggml_metal_init: skipping kernel_mul_mv_id_bf16_f32                (not supported)\n",
      "ggml_metal_init: skipping kernel_mul_mm_bf16_f32                   (not supported)\n",
      "ggml_metal_init: skipping kernel_mul_mm_id_bf16_f32                (not supported)\n",
      "ggml_metal_init: skipping kernel_flash_attn_ext_bf16_h64           (not supported)\n",
      "ggml_metal_init: skipping kernel_flash_attn_ext_bf16_h80           (not supported)\n",
      "ggml_metal_init: skipping kernel_flash_attn_ext_bf16_h96           (not supported)\n",
      "ggml_metal_init: skipping kernel_flash_attn_ext_bf16_h112          (not supported)\n",
      "ggml_metal_init: skipping kernel_flash_attn_ext_bf16_h128          (not supported)\n",
      "ggml_metal_init: skipping kernel_flash_attn_ext_bf16_h192          (not supported)\n",
      "ggml_metal_init: skipping kernel_flash_attn_ext_bf16_hk192_hv128   (not supported)\n",
      "ggml_metal_init: skipping kernel_flash_attn_ext_bf16_h256          (not supported)\n",
      "ggml_metal_init: skipping kernel_flash_attn_ext_vec_bf16_h128      (not supported)\n",
      "ggml_metal_init: skipping kernel_flash_attn_ext_vec_bf16_h192      (not supported)\n",
      "ggml_metal_init: skipping kernel_flash_attn_ext_vec_bf16_hk192_hv128 (not supported)\n",
      "ggml_metal_init: skipping kernel_flash_attn_ext_vec_bf16_h256      (not supported)\n",
      "ggml_metal_init: skipping kernel_cpy_f32_bf16                      (not supported)\n",
      "ggml_metal_init: skipping kernel_cpy_bf16_f32                      (not supported)\n",
      "ggml_metal_init: skipping kernel_cpy_bf16_bf16                     (not supported)\n",
      "whisper_backend_init: using BLAS backend\n",
      "whisper_init_state: kv self size  =   18.87 MB\n",
      "whisper_init_state: kv cross size =   56.62 MB\n",
      "whisper_init_state: kv pad  size  =    4.72 MB\n",
      "whisper_init_state: compute buffer (conv)   =   23.37 MB\n",
      "whisper_init_state: compute buffer (encode) =  128.01 MB\n",
      "whisper_init_state: compute buffer (cross)  =    6.18 MB\n",
      "whisper_init_state: compute buffer (decode) =   98.19 MB\n",
      "\n",
      "system_info: n_threads = 4 / 14 | WHISPER : COREML = 0 | OPENVINO = 0 | Metal : EMBED_LIBRARY = 1 | CPU : NEON = 1 | ARM_FMA = 1 | FP16_VA = 1 | MATMUL_INT8 = 1 | DOTPROD = 1 | SME = 1 | ACCELERATE = 1 | AARCH64_REPACK = 1 | \n",
      "\n",
      "main: processing './Sound/01/chunk_006.wav' (1920349 samples, 120.0 sec), 4 threads, 1 processors, 5 beams + best of 5, lang = ko, task = transcribe, timestamps = 1 ...\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "[00:00:00.000 --> 00:00:04.040]   저희는 이쪽에 있어서는 뭐 DNS 같은 거는 안 하고 있기 때문에\n",
      "[00:00:04.040 --> 00:00:07.680]   서비스 팀마다 자체적으로 쓰는 거라서 이렇게 쓰고 있습니다\n",
      "[00:00:07.680 --> 00:00:11.320]   그래서 이런 설정이 있구나 라고 쪽쪽 둘러보신 다음에\n",
      "[00:00:11.320 --> 00:00:15.600]   그 다음에도 팀 가셔서 하실 수 있는 게 플러그인 설치가 있어요\n",
      "[00:00:15.600 --> 00:00:18.440]   그래서 팀마다 만약에 좀 저희 팀에서\n",
      "[00:00:18.440 --> 00:00:21.800]   이제 좀 공통적으로 필요한 플러그인들을 설치를 해서 드릴 건데\n",
      "[00:00:21.800 --> 00:00:25.040]   그 외에도 여러분들이 필요한 플러그인들이 있을 수 있잖아요\n",
      "[00:00:25.040 --> 00:00:29.880]   제가 방금 드린 슬랙 채널에 드린 이 URL에 한번 들어가 보시면\n",
      "[00:00:29.880 --> 00:00:32.440]   플러그인 점 잼킨스 점 IO라는 게 있군요\n",
      "[00:00:32.440 --> 00:00:37.440]   여기 들어가 보시면 잼킨스에 다 갖다 붙일 수 있는 모든 플러그인들이 다 여기 있습니다\n",
      "[00:00:37.440 --> 00:00:42.360]   그래서 여기서 간단하게 우리는 기텁 엔터프라이즈 쓰니까\n",
      "[00:00:42.360 --> 00:00:48.480]   예를 들어 검색을 해보면 기텁이랑 인태그레이션 할 수 있는 여러 가지 플러그인이 있죠\n",
      "[00:00:48.480 --> 00:00:51.920]   잼킨스에서 기텁을 활용하기 위한 여러 가지 툴들이 있어요\n",
      "[00:00:51.920 --> 00:00:58.360]   예를 들어 우리는 기텁 엔터프라이즈에 대한 인증을 기텁 오어스를 써서 하고 싶어\n",
      "[00:00:58.360 --> 00:01:04.000]   그러면은 이런 이런 이런 플러그인을 한번 쭉 둘러보시고\n",
      "[00:01:04.000 --> 00:01:07.000]   이거에 대한 설치를 어떻게 하느냐\n",
      "[00:01:07.000 --> 00:01:13.960]   파우트 인스토리 누르셔가지고 직접 이 따렉트 업로드 하는 파일을 받으셔서\n",
      "[00:01:13.960 --> 00:01:15.760]   컨테이너 내부로 넣으시면 됩니다\n",
      "[00:01:15.760 --> 00:01:18.760]   컨테이너 내부로 넣을 때 아까 얘기한 것처럼\n",
      "[00:01:18.760 --> 00:01:23.000]   잼킨스는 모든 정보를 파이 시스템 안에 갖고 있다고 했어요\n",
      "[00:01:23.000 --> 00:01:26.040]   그러면 거기에 당연히 플러그인이 저장하는 폴더가 있거든요\n",
      "[00:01:26.040 --> 00:01:31.880]   거기에 밀어넣고 리브트 한 번 하면 잼킨스가 바로 플러그인을 긁어다가 설치하는 구조라서\n",
      "[00:01:31.880 --> 00:01:35.480]   이렇게 매뉴얼하게 설치를 해야 하는 경우이고\n",
      "[00:01:35.480 --> 00:01:41.040]   원래 이제 공중망에서 여러분들 지금 맥에서 하면은 바로 그냥 하우트 인스토리에서\n",
      "[00:01:41.040 --> 00:01:43.800]   GUI 같은 걸로 바로 설치가 될 거예요\n",
      "[00:01:43.800 --> 00:01:46.800]   근데 그 이유는 이제 이 공중망이랑 연결이 되어 있기 때문이고\n",
      "[00:01:46.800 --> 00:01:51.000]   산해망에서는 방금 보여드린 것처럼 매뉴얼하게 설치를 해주셔야 됩니다\n",
      "[00:01:51.000 --> 00:01:53.400]   간단하게 청구만 하신 거예요\n",
      "[00:01:53.400 --> 00:01:55.400]   그리고 또 플러그인이 엄청 많거든요\n",
      "[00:01:55.400 --> 00:02:00.040]   한 2천 개 정도 되는데 우리도 애정 있으니까 애정에 관련된 플러멘이 또 뭐가 있을까\n",
      "\n",
      "✅ chunk_006 처리 완료 → ./Text/01/chunk_006_ggml-small.json\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "output_json: saving output to './Text/01/chunk_006_ggml-small.json'\n",
      "\n",
      "whisper_print_timings:     load time =   169.30 ms\n",
      "whisper_print_timings:     fallbacks =   0 p /   0 h\n",
      "whisper_print_timings:      mel time =    28.36 ms\n",
      "whisper_print_timings:   sample time =   458.28 ms /  3003 runs (     0.15 ms per run)\n",
      "whisper_print_timings:   encode time =   574.00 ms /     5 runs (   114.80 ms per run)\n",
      "whisper_print_timings:   decode time =     7.81 ms /     2 runs (     3.90 ms per run)\n",
      "whisper_print_timings:   batchd time =  2718.33 ms /  2982 runs (     0.91 ms per run)\n",
      "whisper_print_timings:   prompt time =    73.47 ms /   603 runs (     0.12 ms per run)\n",
      "whisper_print_timings:    total time =  4091.72 ms\n",
      "ggml_metal_free: deallocating\n",
      "whisper_init_from_file_with_params_no_state: loading model from '/Users/aza1200/.cache/huggingface/hub/models--ggerganov--whisper.cpp/snapshots/5359861c739e955e79d9a303bcbc70fb988958b1/ggml-small.bin'\n",
      "whisper_init_with_params_no_state: use gpu    = 1\n",
      "whisper_init_with_params_no_state: flash attn = 0\n",
      "whisper_init_with_params_no_state: gpu_device = 0\n",
      "whisper_init_with_params_no_state: dtw        = 0\n",
      "whisper_init_with_params_no_state: devices    = 3\n",
      "whisper_init_with_params_no_state: backends   = 3\n",
      "whisper_model_load: loading model\n",
      "whisper_model_load: n_vocab       = 51865\n",
      "whisper_model_load: n_audio_ctx   = 1500\n",
      "whisper_model_load: n_audio_state = 768\n",
      "whisper_model_load: n_audio_head  = 12\n",
      "whisper_model_load: n_audio_layer = 12\n",
      "whisper_model_load: n_text_ctx    = 448\n",
      "whisper_model_load: n_text_state  = 768\n",
      "whisper_model_load: n_text_head   = 12\n",
      "whisper_model_load: n_text_layer  = 12\n",
      "whisper_model_load: n_mels        = 80\n",
      "whisper_model_load: ftype         = 1\n",
      "whisper_model_load: qntvr         = 0\n",
      "whisper_model_load: type          = 3 (small)\n",
      "whisper_model_load: adding 1608 extra tokens\n",
      "whisper_model_load: n_langs       = 99\n",
      "whisper_model_load:        Metal total size =   487.01 MB\n",
      "whisper_model_load: model size    =  487.01 MB\n",
      "whisper_backend_init_gpu: using Metal backend\n",
      "ggml_metal_init: allocating\n",
      "ggml_metal_init: found device: Apple M4 Pro\n",
      "ggml_metal_init: picking default device: Apple M4 Pro\n",
      "ggml_metal_load_library: using embedded metal library\n",
      "ggml_metal_init: GPU name:   Apple M4 Pro\n",
      "ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)\n",
      "ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)\n",
      "ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)\n",
      "ggml_metal_init: simdgroup reduction   = true\n",
      "ggml_metal_init: simdgroup matrix mul. = true\n",
      "ggml_metal_init: has residency sets    = true\n",
      "ggml_metal_init: has bfloat            = true\n",
      "ggml_metal_init: use bfloat            = false\n",
      "ggml_metal_init: hasUnifiedMemory      = true\n",
      "ggml_metal_init: recommendedMaxWorkingSetSize  = 38654.71 MB\n",
      "ggml_metal_init: skipping kernel_get_rows_bf16                     (not supported)\n",
      "ggml_metal_init: skipping kernel_mul_mv_bf16_f32                   (not supported)\n",
      "ggml_metal_init: skipping kernel_mul_mv_bf16_f32_1row              (not supported)\n",
      "ggml_metal_init: skipping kernel_mul_mv_bf16_f32_l4                (not supported)\n",
      "ggml_metal_init: skipping kernel_mul_mv_bf16_bf16                  (not supported)\n",
      "ggml_metal_init: skipping kernel_mul_mv_id_bf16_f32                (not supported)\n",
      "ggml_metal_init: skipping kernel_mul_mm_bf16_f32                   (not supported)\n",
      "ggml_metal_init: skipping kernel_mul_mm_id_bf16_f32                (not supported)\n",
      "ggml_metal_init: skipping kernel_flash_attn_ext_bf16_h64           (not supported)\n",
      "ggml_metal_init: skipping kernel_flash_attn_ext_bf16_h80           (not supported)\n",
      "ggml_metal_init: skipping kernel_flash_attn_ext_bf16_h96           (not supported)\n",
      "ggml_metal_init: skipping kernel_flash_attn_ext_bf16_h112          (not supported)\n",
      "ggml_metal_init: skipping kernel_flash_attn_ext_bf16_h128          (not supported)\n",
      "ggml_metal_init: skipping kernel_flash_attn_ext_bf16_h192          (not supported)\n",
      "ggml_metal_init: skipping kernel_flash_attn_ext_bf16_hk192_hv128   (not supported)\n",
      "ggml_metal_init: skipping kernel_flash_attn_ext_bf16_h256          (not supported)\n",
      "ggml_metal_init: skipping kernel_flash_attn_ext_vec_bf16_h128      (not supported)\n",
      "ggml_metal_init: skipping kernel_flash_attn_ext_vec_bf16_h192      (not supported)\n",
      "ggml_metal_init: skipping kernel_flash_attn_ext_vec_bf16_hk192_hv128 (not supported)\n",
      "ggml_metal_init: skipping kernel_flash_attn_ext_vec_bf16_h256      (not supported)\n",
      "ggml_metal_init: skipping kernel_cpy_f32_bf16                      (not supported)\n",
      "ggml_metal_init: skipping kernel_cpy_bf16_f32                      (not supported)\n",
      "ggml_metal_init: skipping kernel_cpy_bf16_bf16                     (not supported)\n",
      "whisper_backend_init: using BLAS backend\n",
      "whisper_init_state: kv self size  =   18.87 MB\n",
      "whisper_init_state: kv cross size =   56.62 MB\n",
      "whisper_init_state: kv pad  size  =    4.72 MB\n",
      "whisper_init_state: compute buffer (conv)   =   23.37 MB\n",
      "whisper_init_state: compute buffer (encode) =  128.01 MB\n",
      "whisper_init_state: compute buffer (cross)  =    6.18 MB\n",
      "whisper_init_state: compute buffer (decode) =   98.19 MB\n",
      "\n",
      "system_info: n_threads = 4 / 14 | WHISPER : COREML = 0 | OPENVINO = 0 | Metal : EMBED_LIBRARY = 1 | CPU : NEON = 1 | ARM_FMA = 1 | FP16_VA = 1 | MATMUL_INT8 = 1 | DOTPROD = 1 | SME = 1 | ACCELERATE = 1 | AARCH64_REPACK = 1 | \n",
      "\n",
      "main: processing './Sound/01/chunk_007.wav' (1919661 samples, 120.0 sec), 4 threads, 1 processors, 5 beams + best of 5, lang = ko, task = transcribe, timestamps = 1 ...\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "[00:00:00.000 --> 00:00:04.800]   애저를 보시면 애저를 크레덴션 쓰는 것도 있고\n",
      "[00:00:04.800 --> 00:00:07.900]   엔트라이디, 키볼트 이런 연동도 있고\n",
      "[00:00:07.900 --> 00:00:12.200]   애저를 앱서비스, 컨테이너에서 굉장히 많이 있네요\n",
      "[00:00:12.200 --> 00:00:15.600]   컨테이너 레지스트링, 애저를 펑션 연동할 수도 있고\n",
      "[00:00:15.600 --> 00:00:18.900]   이런 식으로 여러가지 플러그인들이 있는데\n",
      "[00:00:18.900 --> 00:00:22.100]   이 아래 헬스스코어를 좀 보는 게 중요합니다\n",
      "[00:00:22.100 --> 00:00:25.460]   헬스스코어라는 거는 이 플러그인이 얼마나\n",
      "[00:00:25.460 --> 00:00:31.060]   뭐랄까 사용자들이 많이 쓰고 좀 호환성이 되어 있느냐에 대한 그런 수치예요\n",
      "[00:00:31.060 --> 00:00:35.660]   그래서 좀 플러그인을 사실 전부 다 오픈소스라서 가져다 쓸 때\n",
      "[00:00:35.660 --> 00:00:39.260]   이거를 사용할 때 무리가 없으려면 이런 수치를 좀 보는 것도 좋고\n",
      "[00:00:39.260 --> 00:00:44.660]   또 젠키스 플러그인에서 다른 사람들은 뭐를 많이 쓰고 있을까 좀 궁금하죠\n",
      "[00:00:44.660 --> 00:00:47.160]   그럼 이 아래에서 트렌딩 이런 거 있어요\n",
      "[00:00:47.160 --> 00:00:52.660]   그래서 이 아래에 있는 10개의 플러그인들은 다른 사람들이 많이 쓰는 거거든요\n",
      "[00:00:52.660 --> 00:00:55.660]   그래서 이 중에 하나로 진짜 단순하게 이런 거 있죠\n",
      "[00:00:55.660 --> 00:01:01.360]   뭐 다크테마 이런 거, 젠키스 까맣게 바꿔 주는 거 이런 것도 있고\n",
      "[00:01:01.360 --> 00:01:03.360]   매트립 확인하는 것도 있네요\n",
      "[00:01:03.360 --> 00:01:07.660]   그리고 쏘나큐브라고 해서 이제 안에 취약점 검토하는 것들\n",
      "[00:01:07.660 --> 00:01:11.960]   뭐 이런 호환 관련된 오픈소스도 연동해서 쓰고\n",
      "[00:01:11.960 --> 00:01:18.160]   빌드타임아웃이라고 해서 잡마다 이제 타임아웃 시간을 세팅한 다음에\n",
      "[00:01:18.160 --> 00:01:22.660]   뭐 쓰는 이런 플러그인도 있네요 이렇게 해서 이제 사실 저희 팀에서\n",
      "[00:01:22.660 --> 00:01:25.360]   공통적으로 이런 표준을 제공해드리긴 하지만\n",
      "[00:01:25.360 --> 00:01:29.160]   팀마다 모든 커스텀 된 쪽을 이제 다를 수 없기 때문에\n",
      "[00:01:29.160 --> 00:01:32.860]   이런 식으로 해서 본인의 이 젠키스를 꾸려나갈 수 있다라는 장점이 있습니다\n",
      "[00:01:32.860 --> 00:01:41.160]   젠키스가 둘이 플러그인이 많아서 그리고 다시 이제 어드민으로 들어와서\n",
      "[00:01:41.160 --> 00:01:46.660]   뭐가 있을까 시스템 로그 모든 젠키스 로그 이런 거 있네요\n",
      "[00:01:46.660 --> 00:01:50.560]   젠키스에 대한 모든 로그를 볼 수 있는 이런 거 중요하겠죠\n",
      "[00:01:50.560 --> 00:01:52.420]   로그 보는 거\n",
      "[00:01:52.420 --> 00:01:56.760]   젠키스에서 뭐 뭔가 문제가 생겼으면 로그를 보면 확확하게 쉬우니까\n",
      "[00:01:56.760 --> 00:02:00.060]   그래서 이런 로그 보는 것도 있고 네 그렇습니다\n",
      "\n",
      "✅ chunk_007 처리 완료 → ./Text/01/chunk_007_ggml-small.json\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "output_json: saving output to './Text/01/chunk_007_ggml-small.json'\n",
      "\n",
      "whisper_print_timings:     load time =   185.15 ms\n",
      "whisper_print_timings:     fallbacks =   0 p /   0 h\n",
      "whisper_print_timings:      mel time =    28.28 ms\n",
      "whisper_print_timings:   sample time =   416.08 ms /  2865 runs (     0.15 ms per run)\n",
      "whisper_print_timings:   encode time =   577.09 ms /     5 runs (   115.42 ms per run)\n",
      "whisper_print_timings:   decode time =     4.15 ms /     1 runs (     4.15 ms per run)\n",
      "whisper_print_timings:   batchd time =  2554.25 ms /  2842 runs (     0.90 ms per run)\n",
      "whisper_print_timings:   prompt time =    95.56 ms /   814 runs (     0.12 ms per run)\n",
      "whisper_print_timings:    total time =  3927.02 ms\n",
      "ggml_metal_free: deallocating\n",
      "whisper_init_from_file_with_params_no_state: loading model from '/Users/aza1200/.cache/huggingface/hub/models--ggerganov--whisper.cpp/snapshots/5359861c739e955e79d9a303bcbc70fb988958b1/ggml-small.bin'\n",
      "whisper_init_with_params_no_state: use gpu    = 1\n",
      "whisper_init_with_params_no_state: flash attn = 0\n",
      "whisper_init_with_params_no_state: gpu_device = 0\n",
      "whisper_init_with_params_no_state: dtw        = 0\n",
      "whisper_init_with_params_no_state: devices    = 3\n",
      "whisper_init_with_params_no_state: backends   = 3\n",
      "whisper_model_load: loading model\n",
      "whisper_model_load: n_vocab       = 51865\n",
      "whisper_model_load: n_audio_ctx   = 1500\n",
      "whisper_model_load: n_audio_state = 768\n",
      "whisper_model_load: n_audio_head  = 12\n",
      "whisper_model_load: n_audio_layer = 12\n",
      "whisper_model_load: n_text_ctx    = 448\n",
      "whisper_model_load: n_text_state  = 768\n",
      "whisper_model_load: n_text_head   = 12\n",
      "whisper_model_load: n_text_layer  = 12\n",
      "whisper_model_load: n_mels        = 80\n",
      "whisper_model_load: ftype         = 1\n",
      "whisper_model_load: qntvr         = 0\n",
      "whisper_model_load: type          = 3 (small)\n",
      "whisper_model_load: adding 1608 extra tokens\n",
      "whisper_model_load: n_langs       = 99\n",
      "whisper_model_load:        Metal total size =   487.01 MB\n",
      "whisper_model_load: model size    =  487.01 MB\n",
      "whisper_backend_init_gpu: using Metal backend\n",
      "ggml_metal_init: allocating\n",
      "ggml_metal_init: found device: Apple M4 Pro\n",
      "ggml_metal_init: picking default device: Apple M4 Pro\n",
      "ggml_metal_load_library: using embedded metal library\n",
      "ggml_metal_init: GPU name:   Apple M4 Pro\n",
      "ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)\n",
      "ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)\n",
      "ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)\n",
      "ggml_metal_init: simdgroup reduction   = true\n",
      "ggml_metal_init: simdgroup matrix mul. = true\n",
      "ggml_metal_init: has residency sets    = true\n",
      "ggml_metal_init: has bfloat            = true\n",
      "ggml_metal_init: use bfloat            = false\n",
      "ggml_metal_init: hasUnifiedMemory      = true\n",
      "ggml_metal_init: recommendedMaxWorkingSetSize  = 38654.71 MB\n",
      "ggml_metal_init: skipping kernel_get_rows_bf16                     (not supported)\n",
      "ggml_metal_init: skipping kernel_mul_mv_bf16_f32                   (not supported)\n",
      "ggml_metal_init: skipping kernel_mul_mv_bf16_f32_1row              (not supported)\n",
      "ggml_metal_init: skipping kernel_mul_mv_bf16_f32_l4                (not supported)\n",
      "ggml_metal_init: skipping kernel_mul_mv_bf16_bf16                  (not supported)\n",
      "ggml_metal_init: skipping kernel_mul_mv_id_bf16_f32                (not supported)\n",
      "ggml_metal_init: skipping kernel_mul_mm_bf16_f32                   (not supported)\n",
      "ggml_metal_init: skipping kernel_mul_mm_id_bf16_f32                (not supported)\n",
      "ggml_metal_init: skipping kernel_flash_attn_ext_bf16_h64           (not supported)\n",
      "ggml_metal_init: skipping kernel_flash_attn_ext_bf16_h80           (not supported)\n",
      "ggml_metal_init: skipping kernel_flash_attn_ext_bf16_h96           (not supported)\n",
      "ggml_metal_init: skipping kernel_flash_attn_ext_bf16_h112          (not supported)\n",
      "ggml_metal_init: skipping kernel_flash_attn_ext_bf16_h128          (not supported)\n",
      "ggml_metal_init: skipping kernel_flash_attn_ext_bf16_h192          (not supported)\n",
      "ggml_metal_init: skipping kernel_flash_attn_ext_bf16_hk192_hv128   (not supported)\n",
      "ggml_metal_init: skipping kernel_flash_attn_ext_bf16_h256          (not supported)\n",
      "ggml_metal_init: skipping kernel_flash_attn_ext_vec_bf16_h128      (not supported)\n",
      "ggml_metal_init: skipping kernel_flash_attn_ext_vec_bf16_h192      (not supported)\n",
      "ggml_metal_init: skipping kernel_flash_attn_ext_vec_bf16_hk192_hv128 (not supported)\n",
      "ggml_metal_init: skipping kernel_flash_attn_ext_vec_bf16_h256      (not supported)\n",
      "ggml_metal_init: skipping kernel_cpy_f32_bf16                      (not supported)\n",
      "ggml_metal_init: skipping kernel_cpy_bf16_f32                      (not supported)\n",
      "ggml_metal_init: skipping kernel_cpy_bf16_bf16                     (not supported)\n",
      "whisper_backend_init: using BLAS backend\n",
      "whisper_init_state: kv self size  =   18.87 MB\n",
      "whisper_init_state: kv cross size =   56.62 MB\n",
      "whisper_init_state: kv pad  size  =    4.72 MB\n",
      "whisper_init_state: compute buffer (conv)   =   23.37 MB\n",
      "whisper_init_state: compute buffer (encode) =  128.01 MB\n",
      "whisper_init_state: compute buffer (cross)  =    6.18 MB\n",
      "whisper_init_state: compute buffer (decode) =   98.19 MB\n",
      "\n",
      "system_info: n_threads = 4 / 14 | WHISPER : COREML = 0 | OPENVINO = 0 | Metal : EMBED_LIBRARY = 1 | CPU : NEON = 1 | ARM_FMA = 1 | FP16_VA = 1 | MATMUL_INT8 = 1 | DOTPROD = 1 | SME = 1 | ACCELERATE = 1 | AARCH64_REPACK = 1 | \n",
      "\n",
      "main: processing './Sound/01/chunk_008.wav' (1919608 samples, 120.0 sec), 4 threads, 1 processors, 5 beams + best of 5, lang = ko, task = transcribe, timestamps = 1 ...\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "[00:00:00.000 --> 00:00:03.680]   그리고 하나 좀 특이한 게 스크립트 콘솔이라고 해서\n",
      "[00:00:03.680 --> 00:00:07.120]   젠킨스에다가 직접 명령을 내릴 수 있어요.\n",
      "[00:00:07.120 --> 00:00:10.320]   근데 이 언어는 기본적으로 그루비 언어거든요.\n",
      "[00:00:10.320 --> 00:00:14.080]   그래서 그루비 같은 경우는 그냥 AI가 더 잘 만들어주니까.\n",
      "[00:00:14.080 --> 00:00:15.820]   그루비 언어 공부할 수 필요 없이\n",
      "[00:00:15.820 --> 00:00:21.200]   그냥 젠킨스, 스크립트 콘솔에다가 쓸 이런 명령어를 그루비로 짜달라고 하면 알아서 잘 짜주거든요.\n",
      "[00:00:21.200 --> 00:00:23.800]   특히 어떨 때 좀 중요하냐면\n",
      "[00:00:23.800 --> 00:00:27.200]   만약에 우리가 젠킨스를 얼마나 쓰고 있는지 알고 싶다.\n",
      "[00:00:27.200 --> 00:00:30.080]   근데 만약에 파이프라인이 한 100개예요.\n",
      "[00:00:30.080 --> 00:00:34.300]   파이프라인이 100개고 그 안에 각각 빌드 히스토리가 상천 개씩 있어요.\n",
      "[00:00:34.300 --> 00:00:39.800]   그러면 수동으로 뽑기 힘드니까 거기에 대한 이제 그루비 스크립트를 짜서\n",
      "[00:00:39.800 --> 00:00:44.340]   API를 쓰는 거죠. API로, 내부 API로.\n",
      "[00:00:44.340 --> 00:00:49.960]   안에 있는 그런 정보들을 꼽아올 수 있는 그런 유용함도 있고\n",
      "[00:00:49.960 --> 00:00:52.220]   그런 식으로 기능이 다양하기 때문에\n",
      "[00:00:52.220 --> 00:00:57.420]   저는 소견이 여기까지만 하고요. 그럼 젠킨스에서 잡을 어떻게 만드냐.\n",
      "[00:00:57.420 --> 00:01:02.140]   우리는 이제 VN 100포용으로 젠킨스를 쓸 거기 때문에 메인 홈으로 들어오셔서\n",
      "[00:01:02.140 --> 00:01:07.520]   이 잡을 만들 겁니다. CREATE 잡을 한번 눌러주세요.\n",
      "[00:01:07.520 --> 00:01:16.160]   여기에 뭐 IT 미름은 그냥 편하게 CD, 파이프라인 이런 거, 테스트 이런 거 하셔도 되겠네요.\n",
      "[00:01:16.160 --> 00:01:19.900]   그냥 또 아래에도 되게 약간 옵션이 많죠.\n",
      "[00:01:19.900 --> 00:01:25.480]   이거를 좀 가시적으로 기능상 구분하기 좋게 만들려면 먼저 폴더를 만들어 볼게요.\n",
      "[00:01:25.480 --> 00:01:30.880]   폴더로 지정해서 폴더가 만들어질 건데\n",
      "[00:01:30.880 --> 00:01:35.440]   이거 그냥 세이브 하시고 폴더가 생겼죠.\n",
      "[00:01:35.440 --> 00:01:39.480]   그래서 만약에 홈으로 다시 나가보면은 이런 폴더가 하나 생길 거예요.\n",
      "[00:01:39.480 --> 00:01:44.780]   폴더 표시로 퓨스 체크도 되고 그 다음에 폴더 하나 생겼죠.\n",
      "[00:01:44.780 --> 00:01:48.020]   이 아래 이제 직접 액션을 취할 잡을 한번 만들어 보겠습니다.\n",
      "[00:01:48.020 --> 00:01:52.760]   이 폴더 아래로 들어와서 잡을 하나 만들고\n",
      "[00:01:52.760 --> 00:01:58.360]   잡은 뭐 실질적으로 하는 테스트로 할게요.\n",
      "[00:01:58.360 --> 00:02:01.860]   이 폴더 안에 있는 잡에는\n",
      "\n",
      "✅ chunk_008 처리 완료 → ./Text/01/chunk_008_ggml-small.json\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "output_json: saving output to './Text/01/chunk_008_ggml-small.json'\n",
      "\n",
      "whisper_print_timings:     load time =   178.89 ms\n",
      "whisper_print_timings:     fallbacks =   0 p /   0 h\n",
      "whisper_print_timings:      mel time =    29.94 ms\n",
      "whisper_print_timings:   sample time =   445.05 ms /  2802 runs (     0.16 ms per run)\n",
      "whisper_print_timings:   encode time =   588.42 ms /     5 runs (   117.68 ms per run)\n",
      "whisper_print_timings:   decode time =   119.44 ms /    27 runs (     4.42 ms per run)\n",
      "whisper_print_timings:   batchd time =  2503.38 ms /  2753 runs (     0.91 ms per run)\n",
      "whisper_print_timings:   prompt time =    98.04 ms /   837 runs (     0.12 ms per run)\n",
      "whisper_print_timings:    total time =  4032.27 ms\n",
      "ggml_metal_free: deallocating\n",
      "whisper_init_from_file_with_params_no_state: loading model from '/Users/aza1200/.cache/huggingface/hub/models--ggerganov--whisper.cpp/snapshots/5359861c739e955e79d9a303bcbc70fb988958b1/ggml-small.bin'\n",
      "whisper_init_with_params_no_state: use gpu    = 1\n",
      "whisper_init_with_params_no_state: flash attn = 0\n",
      "whisper_init_with_params_no_state: gpu_device = 0\n",
      "whisper_init_with_params_no_state: dtw        = 0\n",
      "whisper_init_with_params_no_state: devices    = 3\n",
      "whisper_init_with_params_no_state: backends   = 3\n",
      "whisper_model_load: loading model\n",
      "whisper_model_load: n_vocab       = 51865\n",
      "whisper_model_load: n_audio_ctx   = 1500\n",
      "whisper_model_load: n_audio_state = 768\n",
      "whisper_model_load: n_audio_head  = 12\n",
      "whisper_model_load: n_audio_layer = 12\n",
      "whisper_model_load: n_text_ctx    = 448\n",
      "whisper_model_load: n_text_state  = 768\n",
      "whisper_model_load: n_text_head   = 12\n",
      "whisper_model_load: n_text_layer  = 12\n",
      "whisper_model_load: n_mels        = 80\n",
      "whisper_model_load: ftype         = 1\n",
      "whisper_model_load: qntvr         = 0\n",
      "whisper_model_load: type          = 3 (small)\n",
      "whisper_model_load: adding 1608 extra tokens\n",
      "whisper_model_load: n_langs       = 99\n",
      "whisper_model_load:        Metal total size =   487.01 MB\n",
      "whisper_model_load: model size    =  487.01 MB\n",
      "whisper_backend_init_gpu: using Metal backend\n",
      "ggml_metal_init: allocating\n",
      "ggml_metal_init: found device: Apple M4 Pro\n",
      "ggml_metal_init: picking default device: Apple M4 Pro\n",
      "ggml_metal_load_library: using embedded metal library\n",
      "ggml_metal_init: GPU name:   Apple M4 Pro\n",
      "ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)\n",
      "ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)\n",
      "ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)\n",
      "ggml_metal_init: simdgroup reduction   = true\n",
      "ggml_metal_init: simdgroup matrix mul. = true\n",
      "ggml_metal_init: has residency sets    = true\n",
      "ggml_metal_init: has bfloat            = true\n",
      "ggml_metal_init: use bfloat            = false\n",
      "ggml_metal_init: hasUnifiedMemory      = true\n",
      "ggml_metal_init: recommendedMaxWorkingSetSize  = 38654.71 MB\n",
      "ggml_metal_init: skipping kernel_get_rows_bf16                     (not supported)\n",
      "ggml_metal_init: skipping kernel_mul_mv_bf16_f32                   (not supported)\n",
      "ggml_metal_init: skipping kernel_mul_mv_bf16_f32_1row              (not supported)\n",
      "ggml_metal_init: skipping kernel_mul_mv_bf16_f32_l4                (not supported)\n",
      "ggml_metal_init: skipping kernel_mul_mv_bf16_bf16                  (not supported)\n",
      "ggml_metal_init: skipping kernel_mul_mv_id_bf16_f32                (not supported)\n",
      "ggml_metal_init: skipping kernel_mul_mm_bf16_f32                   (not supported)\n",
      "ggml_metal_init: skipping kernel_mul_mm_id_bf16_f32                (not supported)\n",
      "ggml_metal_init: skipping kernel_flash_attn_ext_bf16_h64           (not supported)\n",
      "ggml_metal_init: skipping kernel_flash_attn_ext_bf16_h80           (not supported)\n",
      "ggml_metal_init: skipping kernel_flash_attn_ext_bf16_h96           (not supported)\n",
      "ggml_metal_init: skipping kernel_flash_attn_ext_bf16_h112          (not supported)\n",
      "ggml_metal_init: skipping kernel_flash_attn_ext_bf16_h128          (not supported)\n",
      "ggml_metal_init: skipping kernel_flash_attn_ext_bf16_h192          (not supported)\n",
      "ggml_metal_init: skipping kernel_flash_attn_ext_bf16_hk192_hv128   (not supported)\n",
      "ggml_metal_init: skipping kernel_flash_attn_ext_bf16_h256          (not supported)\n",
      "ggml_metal_init: skipping kernel_flash_attn_ext_vec_bf16_h128      (not supported)\n",
      "ggml_metal_init: skipping kernel_flash_attn_ext_vec_bf16_h192      (not supported)\n",
      "ggml_metal_init: skipping kernel_flash_attn_ext_vec_bf16_hk192_hv128 (not supported)\n",
      "ggml_metal_init: skipping kernel_flash_attn_ext_vec_bf16_h256      (not supported)\n",
      "ggml_metal_init: skipping kernel_cpy_f32_bf16                      (not supported)\n",
      "ggml_metal_init: skipping kernel_cpy_bf16_f32                      (not supported)\n",
      "ggml_metal_init: skipping kernel_cpy_bf16_bf16                     (not supported)\n",
      "whisper_backend_init: using BLAS backend\n",
      "whisper_init_state: kv self size  =   18.87 MB\n",
      "whisper_init_state: kv cross size =   56.62 MB\n",
      "whisper_init_state: kv pad  size  =    4.72 MB\n",
      "whisper_init_state: compute buffer (conv)   =   23.37 MB\n",
      "whisper_init_state: compute buffer (encode) =  128.01 MB\n",
      "whisper_init_state: compute buffer (cross)  =    6.18 MB\n",
      "whisper_init_state: compute buffer (decode) =   98.19 MB\n",
      "\n",
      "system_info: n_threads = 4 / 14 | WHISPER : COREML = 0 | OPENVINO = 0 | Metal : EMBED_LIBRARY = 1 | CPU : NEON = 1 | ARM_FMA = 1 | FP16_VA = 1 | MATMUL_INT8 = 1 | DOTPROD = 1 | SME = 1 | ACCELERATE = 1 | AARCH64_REPACK = 1 | \n",
      "\n",
      "main: processing './Sound/01/chunk_009.wav' (1920146 samples, 120.0 sec), 4 threads, 1 processors, 5 beams + best of 5, lang = ko, task = transcribe, timestamps = 1 ...\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "[00:00:00.000 --> 00:00:06.800]   파이프라인으로 할 거예요 파이프라인을 써야 우리가 명령어를 써서 배플할 수 있기 때문에 파이프라인을 쓸 거고\n",
      "[00:00:06.800 --> 00:00:11.320]   이름은 편하게 하시고 파이프라인만 골라주시면 됩니다 파이프라인\n",
      "[00:00:11.320 --> 00:00:19.100]   자 이제 쭉 내려보시면 직접 스크립트를 짤 수 있는 파이프라인 라인이 있는데\n",
      "[00:00:19.100 --> 00:00:26.280]   어 이때 직접 이 파이프라인 스크립트를 여기에 그냥 이렇게 쓰셔도 되고\n",
      "[00:00:26.280 --> 00:00:32.400]   scm이라고 해서 기시랑 연동을 할 수 있어요 젠킨 스파일이라는 파일을 기수로 관리할 수 있기 때문에\n",
      "[00:00:32.400 --> 00:00:37.520]   이거를 어 만약에 나는 기텁이랑 연동하고 싶다 그러면 여기에 이 젠킨 스파일이\n",
      "[00:00:37.520 --> 00:00:47.000]   저장된 레포스토리 url에 넣고 인증 넣으신 다음에 브랭치 설정 하시면 이 젠킨 스파일이라는 패스까지 정해 주시면\n",
      "[00:00:47.000 --> 00:00:53.940]   알아서 이제 이 빌드 잡이 될 때 젠킨스가 기질러서 젠킨 스파일 정보를 불러온 다음에 실행하게 될 거예요\n",
      "[00:00:53.940 --> 00:01:01.040]   우리는 그렇게까지 하지 않고 여기서 할 건데 여기 아주 쉬운 스니펫이 하나 있습니다\n",
      "[00:01:01.040 --> 00:01:07.940]   여기 패러 월드 하면 짠 바로 그룹이 생기거든요 이게 기본적으로 언어가 그룹이 베이스이고\n",
      "[00:01:07.940 --> 00:01:14.440]   야물로도 할 수 있다고 제가 어제는 잠깐 말씀드렸는데 야물들은 또 플러그인이 필요해요 그래서 야물을 쓰시고 싶으시면\n",
      "[00:01:14.440 --> 00:01:25.040]   이제 플러그인을 설치를 하시면 됩니다 그래서 단단하게 어플라이 하고 저장하고 이제 다시 폴더 1 테스트 잡으러 나가보시면\n",
      "[00:01:25.040 --> 00:01:31.440]   이제 아무것도 안 떠 있죠 지금 이 상태에서 왼쪽에 있는 지금 빌드 버튼을 눌러볼게요\n",
      "[00:01:31.440 --> 00:01:39.940]   왼쪽 아래 빌드 히스토리라는 탭 아래 이렇게 초록색 체크 표시 뜨면서 언제 2월에 빌드 했고 뭐 이런 식으로 되어 있거든요\n",
      "[00:01:39.940 --> 00:01:45.440]   여러분 아마 그 타임존 설정 안 돼 있으면 아마 한 9시간 정도 차이 날 건데 그건 그냥 무시해 주시고\n",
      "[00:01:45.440 --> 00:01:56.740]   그래서 이 빌드 히스토리에 들어가 보시면 어떤 잡을 얘가 수행했는지 확인할 수 있어요 그래서 누가 이 잡을 트리거 했고 이 잡을 어 시작부터 끝까지\n",
      "[00:01:56.740 --> 00:02:00.020]   어 수행하는데 얼마나 걸렸고 이 자배될..\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "output_json: saving output to './Text/01/chunk_009_ggml-small.json'\n",
      "\n",
      "whisper_print_timings:     load time =   189.83 ms\n",
      "whisper_print_timings:     fallbacks =   0 p /   0 h\n",
      "whisper_print_timings:      mel time =    28.57 ms\n",
      "whisper_print_timings:   sample time =   463.02 ms /  2842 runs (     0.16 ms per run)\n",
      "whisper_print_timings:   encode time =   712.47 ms /     6 runs (   118.74 ms per run)\n",
      "whisper_print_timings:   decode time =   200.02 ms /    47 runs (     4.26 ms per run)\n",
      "whisper_print_timings:   batchd time =  2549.89 ms /  2771 runs (     0.92 ms per run)\n",
      "whisper_print_timings:   prompt time =    97.68 ms /   803 runs (     0.12 ms per run)\n",
      "whisper_print_timings:    total time =  4320.52 ms\n",
      "ggml_metal_free: deallocating\n",
      "whisper_init_from_file_with_params_no_state: loading model from '/Users/aza1200/.cache/huggingface/hub/models--ggerganov--whisper.cpp/snapshots/5359861c739e955e79d9a303bcbc70fb988958b1/ggml-small.bin'\n",
      "whisper_init_with_params_no_state: use gpu    = 1\n",
      "whisper_init_with_params_no_state: flash attn = 0\n",
      "whisper_init_with_params_no_state: gpu_device = 0\n",
      "whisper_init_with_params_no_state: dtw        = 0\n",
      "whisper_init_with_params_no_state: devices    = 3\n",
      "whisper_init_with_params_no_state: backends   = 3\n",
      "whisper_model_load: loading model\n",
      "whisper_model_load: n_vocab       = 51865\n",
      "whisper_model_load: n_audio_ctx   = 1500\n",
      "whisper_model_load: n_audio_state = 768\n",
      "whisper_model_load: n_audio_head  = 12\n",
      "whisper_model_load: n_audio_layer = 12\n",
      "whisper_model_load: n_text_ctx    = 448\n",
      "whisper_model_load: n_text_state  = 768\n",
      "whisper_model_load: n_text_head   = 12\n",
      "whisper_model_load: n_text_layer  = 12\n",
      "whisper_model_load: n_mels        = 80\n",
      "whisper_model_load: ftype         = 1\n",
      "whisper_model_load: qntvr         = 0\n",
      "whisper_model_load: type          = 3 (small)\n",
      "whisper_model_load: adding 1608 extra tokens\n",
      "whisper_model_load: n_langs       = 99\n",
      "whisper_model_load:        Metal total size =   487.01 MB\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "✅ chunk_009 처리 완료 → ./Text/01/chunk_009_ggml-small.json\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "whisper_model_load: model size    =  487.01 MB\n",
      "whisper_backend_init_gpu: using Metal backend\n",
      "ggml_metal_init: allocating\n",
      "ggml_metal_init: found device: Apple M4 Pro\n",
      "ggml_metal_init: picking default device: Apple M4 Pro\n",
      "ggml_metal_load_library: using embedded metal library\n",
      "ggml_metal_init: GPU name:   Apple M4 Pro\n",
      "ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)\n",
      "ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)\n",
      "ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)\n",
      "ggml_metal_init: simdgroup reduction   = true\n",
      "ggml_metal_init: simdgroup matrix mul. = true\n",
      "ggml_metal_init: has residency sets    = true\n",
      "ggml_metal_init: has bfloat            = true\n",
      "ggml_metal_init: use bfloat            = false\n",
      "ggml_metal_init: hasUnifiedMemory      = true\n",
      "ggml_metal_init: recommendedMaxWorkingSetSize  = 38654.71 MB\n",
      "ggml_metal_init: skipping kernel_get_rows_bf16                     (not supported)\n",
      "ggml_metal_init: skipping kernel_mul_mv_bf16_f32                   (not supported)\n",
      "ggml_metal_init: skipping kernel_mul_mv_bf16_f32_1row              (not supported)\n",
      "ggml_metal_init: skipping kernel_mul_mv_bf16_f32_l4                (not supported)\n",
      "ggml_metal_init: skipping kernel_mul_mv_bf16_bf16                  (not supported)\n",
      "ggml_metal_init: skipping kernel_mul_mv_id_bf16_f32                (not supported)\n",
      "ggml_metal_init: skipping kernel_mul_mm_bf16_f32                   (not supported)\n",
      "ggml_metal_init: skipping kernel_mul_mm_id_bf16_f32                (not supported)\n",
      "ggml_metal_init: skipping kernel_flash_attn_ext_bf16_h64           (not supported)\n",
      "ggml_metal_init: skipping kernel_flash_attn_ext_bf16_h80           (not supported)\n",
      "ggml_metal_init: skipping kernel_flash_attn_ext_bf16_h96           (not supported)\n",
      "ggml_metal_init: skipping kernel_flash_attn_ext_bf16_h112          (not supported)\n",
      "ggml_metal_init: skipping kernel_flash_attn_ext_bf16_h128          (not supported)\n",
      "ggml_metal_init: skipping kernel_flash_attn_ext_bf16_h192          (not supported)\n",
      "ggml_metal_init: skipping kernel_flash_attn_ext_bf16_hk192_hv128   (not supported)\n",
      "ggml_metal_init: skipping kernel_flash_attn_ext_bf16_h256          (not supported)\n",
      "ggml_metal_init: skipping kernel_flash_attn_ext_vec_bf16_h128      (not supported)\n",
      "ggml_metal_init: skipping kernel_flash_attn_ext_vec_bf16_h192      (not supported)\n",
      "ggml_metal_init: skipping kernel_flash_attn_ext_vec_bf16_hk192_hv128 (not supported)\n",
      "ggml_metal_init: skipping kernel_flash_attn_ext_vec_bf16_h256      (not supported)\n",
      "ggml_metal_init: skipping kernel_cpy_f32_bf16                      (not supported)\n",
      "ggml_metal_init: skipping kernel_cpy_bf16_f32                      (not supported)\n",
      "ggml_metal_init: skipping kernel_cpy_bf16_bf16                     (not supported)\n",
      "whisper_backend_init: using BLAS backend\n",
      "whisper_init_state: kv self size  =   18.87 MB\n",
      "whisper_init_state: kv cross size =   56.62 MB\n",
      "whisper_init_state: kv pad  size  =    4.72 MB\n",
      "whisper_init_state: compute buffer (conv)   =   23.37 MB\n",
      "whisper_init_state: compute buffer (encode) =  128.01 MB\n",
      "whisper_init_state: compute buffer (cross)  =    6.18 MB\n",
      "whisper_init_state: compute buffer (decode) =   98.19 MB\n",
      "\n",
      "system_info: n_threads = 4 / 14 | WHISPER : COREML = 0 | OPENVINO = 0 | Metal : EMBED_LIBRARY = 1 | CPU : NEON = 1 | ARM_FMA = 1 | FP16_VA = 1 | MATMUL_INT8 = 1 | DOTPROD = 1 | SME = 1 | ACCELERATE = 1 | AARCH64_REPACK = 1 | \n",
      "\n",
      "main: processing './Sound/01/chunk_010.wav' (1920351 samples, 120.0 sec), 4 threads, 1 processors, 5 beams + best of 5, lang = ko, task = transcribe, timestamps = 1 ...\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "[00:00:00.000 --> 00:00:05.280]   룩을 보고 싶다 그러면 왼쪽에 컨솔 아웃 루시키라는 설정 이 템이 있거든요\n",
      "[00:00:05.280 --> 00:00:09.200]   컨솔 아웃으로 들어가 보셔서 우리 스크립트가 그냥 헬로 월드였죠\n",
      "[00:00:09.200 --> 00:00:12.400]   좀 헬로 월드 찍히는 그런 스크립트를 볼 수 있습니다\n",
      "[00:00:12.400 --> 00:00:19.080]   그러면 이제 잡이 잘 동작하는 건 확인을 했고 우리는 배포를 헬로 월드로 할 건 아니잖아요\n",
      "[00:00:19.080 --> 00:00:22.760]   그래서 다시 이 잡에 대한 수정을 해줄 건데\n",
      "[00:00:22.760 --> 00:00:31.860]   자 이제 VM 배포를 할 때 좀 필수 요소가 있어요 VM 배포 시\n",
      "[00:00:31.860 --> 00:00:38.760]   먼저 어디에 배포할지 이거는 어느 VM에 배포할지겠죠\n",
      "[00:00:38.760 --> 00:00:44.960]   다음에 VM에 속할 수 있는 아이디패스워드가 필요해요\n",
      "[00:00:44.960 --> 00:00:51.460]   기본적으로 젠키스가 이 타겟 배포 VM을 타겟 VM이라고 할게요\n",
      "[00:00:51.460 --> 00:00:58.360]   젠키스가 글씨 잘 보이시네요 타겟 VM에 배꼽하는 방식은\n",
      "[00:00:58.360 --> 00:01:08.540]   젠키스는 명령을 내리는 할 만한 젠키스는 타겟 VM의 SSH로\n",
      "[00:01:08.540 --> 00:01:10.940]   따라서 이 위에 있는 3가지 정보가 필요한 거예요\n",
      "[00:01:10.940 --> 00:01:14.140]   VM에 배포할지 이건 IP겠죠\n",
      "[00:01:14.140 --> 00:01:18.180]   이 VM에 IP 그 다음에 VM에 접속할 수 있는 아이디랑 패스워드\n",
      "[00:01:18.180 --> 00:01:20.540]   이 패스워드를 뜻하는 겁니다\n",
      "[00:01:20.540 --> 00:01:24.180]   그거를 젠키스가 모두 변수와 해석, 하이프라인에다가 녹여내는 것까지\n",
      "[00:01:24.180 --> 00:01:30.760]   오늘 보여드릴 건데 따라서 이 젠키스는 타겟 VM에 명령을 내리는 역할만 하는 거예요\n",
      "[00:01:30.760 --> 00:01:35.620]   젠키스 안에서 돌아가는 잡은 잡 안에 있는 앞으로 보여드릴 스프릭트는\n",
      "[00:01:35.620 --> 00:01:39.020]   이 명령어를 고스란히 떠서 VM이 받아서 실행하는 거예요\n",
      "[00:01:39.020 --> 00:01:44.660]   그러니까 실제적인 이 배포에 대한 역할은 타겟 VM이 하는 게 맞습니다\n",
      "[00:01:44.660 --> 00:01:49.420]   그러면 이 명령이라는 거는 뭘 포함하냐\n",
      "[00:01:49.420 --> 00:02:11.160]   여러분들이 이제 VM에다가 배포하는 경우는 잡하는\n",
      "\n",
      "✅ chunk_010 처리 완료 → ./Text/01/chunk_010_ggml-small.json\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "output_json: saving output to './Text/01/chunk_010_ggml-small.json'\n",
      "\n",
      "whisper_print_timings:     load time =   183.05 ms\n",
      "whisper_print_timings:     fallbacks =   0 p /   0 h\n",
      "whisper_print_timings:      mel time =    28.40 ms\n",
      "whisper_print_timings:   sample time =   382.84 ms /  2661 runs (     0.14 ms per run)\n",
      "whisper_print_timings:   encode time =   572.71 ms /     5 runs (   114.54 ms per run)\n",
      "whisper_print_timings:   decode time =    20.20 ms /     5 runs (     4.04 ms per run)\n",
      "whisper_print_timings:   batchd time =  2380.94 ms /  2634 runs (     0.90 ms per run)\n",
      "whisper_print_timings:   prompt time =    90.36 ms /   800 runs (     0.11 ms per run)\n",
      "whisper_print_timings:    total time =  3724.64 ms\n",
      "ggml_metal_free: deallocating\n",
      "whisper_init_from_file_with_params_no_state: loading model from '/Users/aza1200/.cache/huggingface/hub/models--ggerganov--whisper.cpp/snapshots/5359861c739e955e79d9a303bcbc70fb988958b1/ggml-small.bin'\n",
      "whisper_init_with_params_no_state: use gpu    = 1\n",
      "whisper_init_with_params_no_state: flash attn = 0\n",
      "whisper_init_with_params_no_state: gpu_device = 0\n",
      "whisper_init_with_params_no_state: dtw        = 0\n",
      "whisper_init_with_params_no_state: devices    = 3\n",
      "whisper_init_with_params_no_state: backends   = 3\n",
      "whisper_model_load: loading model\n",
      "whisper_model_load: n_vocab       = 51865\n",
      "whisper_model_load: n_audio_ctx   = 1500\n",
      "whisper_model_load: n_audio_state = 768\n",
      "whisper_model_load: n_audio_head  = 12\n",
      "whisper_model_load: n_audio_layer = 12\n",
      "whisper_model_load: n_text_ctx    = 448\n",
      "whisper_model_load: n_text_state  = 768\n",
      "whisper_model_load: n_text_head   = 12\n",
      "whisper_model_load: n_text_layer  = 12\n",
      "whisper_model_load: n_mels        = 80\n",
      "whisper_model_load: ftype         = 1\n",
      "whisper_model_load: qntvr         = 0\n",
      "whisper_model_load: type          = 3 (small)\n",
      "whisper_model_load: adding 1608 extra tokens\n",
      "whisper_model_load: n_langs       = 99\n",
      "whisper_model_load:        Metal total size =   487.01 MB\n",
      "whisper_model_load: model size    =  487.01 MB\n",
      "whisper_backend_init_gpu: using Metal backend\n",
      "ggml_metal_init: allocating\n",
      "ggml_metal_init: found device: Apple M4 Pro\n",
      "ggml_metal_init: picking default device: Apple M4 Pro\n",
      "ggml_metal_load_library: using embedded metal library\n",
      "ggml_metal_init: GPU name:   Apple M4 Pro\n",
      "ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)\n",
      "ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)\n",
      "ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)\n",
      "ggml_metal_init: simdgroup reduction   = true\n",
      "ggml_metal_init: simdgroup matrix mul. = true\n",
      "ggml_metal_init: has residency sets    = true\n",
      "ggml_metal_init: has bfloat            = true\n",
      "ggml_metal_init: use bfloat            = false\n",
      "ggml_metal_init: hasUnifiedMemory      = true\n",
      "ggml_metal_init: recommendedMaxWorkingSetSize  = 38654.71 MB\n",
      "ggml_metal_init: skipping kernel_get_rows_bf16                     (not supported)\n",
      "ggml_metal_init: skipping kernel_mul_mv_bf16_f32                   (not supported)\n",
      "ggml_metal_init: skipping kernel_mul_mv_bf16_f32_1row              (not supported)\n",
      "ggml_metal_init: skipping kernel_mul_mv_bf16_f32_l4                (not supported)\n",
      "ggml_metal_init: skipping kernel_mul_mv_bf16_bf16                  (not supported)\n",
      "ggml_metal_init: skipping kernel_mul_mv_id_bf16_f32                (not supported)\n",
      "ggml_metal_init: skipping kernel_mul_mm_bf16_f32                   (not supported)\n",
      "ggml_metal_init: skipping kernel_mul_mm_id_bf16_f32                (not supported)\n",
      "ggml_metal_init: skipping kernel_flash_attn_ext_bf16_h64           (not supported)\n",
      "ggml_metal_init: skipping kernel_flash_attn_ext_bf16_h80           (not supported)\n",
      "ggml_metal_init: skipping kernel_flash_attn_ext_bf16_h96           (not supported)\n",
      "ggml_metal_init: skipping kernel_flash_attn_ext_bf16_h112          (not supported)\n",
      "ggml_metal_init: skipping kernel_flash_attn_ext_bf16_h128          (not supported)\n",
      "ggml_metal_init: skipping kernel_flash_attn_ext_bf16_h192          (not supported)\n",
      "ggml_metal_init: skipping kernel_flash_attn_ext_bf16_hk192_hv128   (not supported)\n",
      "ggml_metal_init: skipping kernel_flash_attn_ext_bf16_h256          (not supported)\n",
      "ggml_metal_init: skipping kernel_flash_attn_ext_vec_bf16_h128      (not supported)\n",
      "ggml_metal_init: skipping kernel_flash_attn_ext_vec_bf16_h192      (not supported)\n",
      "ggml_metal_init: skipping kernel_flash_attn_ext_vec_bf16_hk192_hv128 (not supported)\n",
      "ggml_metal_init: skipping kernel_flash_attn_ext_vec_bf16_h256      (not supported)\n",
      "ggml_metal_init: skipping kernel_cpy_f32_bf16                      (not supported)\n",
      "ggml_metal_init: skipping kernel_cpy_bf16_f32                      (not supported)\n",
      "ggml_metal_init: skipping kernel_cpy_bf16_bf16                     (not supported)\n",
      "whisper_backend_init: using BLAS backend\n",
      "whisper_init_state: kv self size  =   18.87 MB\n",
      "whisper_init_state: kv cross size =   56.62 MB\n",
      "whisper_init_state: kv pad  size  =    4.72 MB\n",
      "whisper_init_state: compute buffer (conv)   =   23.37 MB\n",
      "whisper_init_state: compute buffer (encode) =  128.01 MB\n",
      "whisper_init_state: compute buffer (cross)  =    6.18 MB\n",
      "whisper_init_state: compute buffer (decode) =   98.19 MB\n",
      "\n",
      "system_info: n_threads = 4 / 14 | WHISPER : COREML = 0 | OPENVINO = 0 | Metal : EMBED_LIBRARY = 1 | CPU : NEON = 1 | ARM_FMA = 1 | FP16_VA = 1 | MATMUL_INT8 = 1 | DOTPROD = 1 | SME = 1 | ACCELERATE = 1 | AARCH64_REPACK = 1 | \n",
      "\n",
      "main: processing './Sound/01/chunk_011.wav' (1920222 samples, 120.0 sec), 4 threads, 1 processors, 5 beams + best of 5, lang = ko, task = transcribe, timestamps = 1 ...\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "[00:00:00.000 --> 00:00:09.000]   파이선은 저희는 사실 소스코드 기반을 배포하는 것은 아직 건강하지 않아요, 소스코드 관리가 안되기 때문에\n",
      "[00:00:09.000 --> 00:00:22.000]   파이선의 경우는 어제는 저희가 소파이선 하신 분들 스포라이더 할 때 점파이 파일을 소파일에 옮겨서 아니면 이런 앱, 점파이나 앱 이런 거를\n",
      "[00:00:22.000 --> 00:00:30.000]   전체 디렉토리를 앱에 옮겨서 이렇게 하셨던 것 같은데 이렇게 되면 이제 패키징이 안되어 있다라는 뜻이거든요.\n",
      "[00:00:30.000 --> 00:00:37.000]   근데 KT 안에서는 파이선도 배포하실 때 우선 패키징을 해야 되고 파이선을 패키징하는 툴로는\n",
      "[00:00:37.000 --> 00:00:46.000]   'Poetry'라는 오픈 소스나 'Wheel' 이런 게 있어요. 이런 거를 써서 파이선을 점 자르나 점 와르처럼 만든다고 생각하시면 됩니다.\n",
      "[00:00:46.000 --> 00:00:55.000]   그래서 이 실행 파일을 이런 파일로 생기거든요. 이런 압축 파일로 생기는 거예요.\n",
      "[00:00:55.000 --> 00:01:04.000]   '2.5'하는 점 자르처럼. 이런 최종 파일을 뵙는다거나 아니면 NPM, 모두 NPM 같은 경우는\n",
      "[00:01:04.000 --> 00:01:17.000]   빌드 하면 디스트 폴더 생기죠. 프론텐드 하시는 분들 맞나요 제가? 이걸 압축 파일로 생기지 않나요? 점 집이었나 이런 압축 파일이 생길 거거든요.\n",
      "[00:01:17.000 --> 00:01:27.000]   이런 식으로 이제 배포를 하게 되는데 V&B 배포를 경우에는 어쨌든 장황했지만 언어별로 패키징한 정적 파일이라고 해요.\n",
      "[00:01:27.000 --> 00:01:34.000]   정적 파일들이 어제 CIA를 했다면 저희가 V&B 배포의 경우에는 블록 스토리지에다가 저장을 한다고 했잖아요.\n",
      "[00:01:34.000 --> 00:01:42.000]   'Poetry'는 이제 ACI에다가 저장을 하고. 그럼 얘는 기본적으로 지금 블록 스토리지에 저장이 되어 있을 거란 말이죠.\n",
      "[00:01:42.000 --> 00:02:00.000]   V&B를 가져다가 실행해라는 명령이 많습니다. 따라서 이 명령이라는 거는 어떤 블록 스토리지에서 어떤 파일을 다운로드 받아서 실행해.\n",
      "\n",
      "✅ chunk_011 처리 완료 → ./Text/01/chunk_011_ggml-small.json\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "output_json: saving output to './Text/01/chunk_011_ggml-small.json'\n",
      "\n",
      "whisper_print_timings:     load time =   186.22 ms\n",
      "whisper_print_timings:     fallbacks =   0 p /   0 h\n",
      "whisper_print_timings:      mel time =    28.40 ms\n",
      "whisper_print_timings:   sample time =   426.97 ms /  2865 runs (     0.15 ms per run)\n",
      "whisper_print_timings:   encode time =   689.51 ms /     6 runs (   114.92 ms per run)\n",
      "whisper_print_timings:   decode time =    16.43 ms /     4 runs (     4.11 ms per run)\n",
      "whisper_print_timings:   batchd time =  2636.05 ms /  2834 runs (     0.93 ms per run)\n",
      "whisper_print_timings:   prompt time =   116.05 ms /  1010 runs (     0.11 ms per run)\n",
      "whisper_print_timings:    total time =  4172.31 ms\n",
      "ggml_metal_free: deallocating\n",
      "whisper_init_from_file_with_params_no_state: loading model from '/Users/aza1200/.cache/huggingface/hub/models--ggerganov--whisper.cpp/snapshots/5359861c739e955e79d9a303bcbc70fb988958b1/ggml-small.bin'\n",
      "whisper_init_with_params_no_state: use gpu    = 1\n",
      "whisper_init_with_params_no_state: flash attn = 0\n",
      "whisper_init_with_params_no_state: gpu_device = 0\n",
      "whisper_init_with_params_no_state: dtw        = 0\n",
      "whisper_init_with_params_no_state: devices    = 3\n",
      "whisper_init_with_params_no_state: backends   = 3\n",
      "whisper_model_load: loading model\n",
      "whisper_model_load: n_vocab       = 51865\n",
      "whisper_model_load: n_audio_ctx   = 1500\n",
      "whisper_model_load: n_audio_state = 768\n",
      "whisper_model_load: n_audio_head  = 12\n",
      "whisper_model_load: n_audio_layer = 12\n",
      "whisper_model_load: n_text_ctx    = 448\n",
      "whisper_model_load: n_text_state  = 768\n",
      "whisper_model_load: n_text_head   = 12\n",
      "whisper_model_load: n_text_layer  = 12\n",
      "whisper_model_load: n_mels        = 80\n",
      "whisper_model_load: ftype         = 1\n",
      "whisper_model_load: qntvr         = 0\n",
      "whisper_model_load: type          = 3 (small)\n",
      "whisper_model_load: adding 1608 extra tokens\n",
      "whisper_model_load: n_langs       = 99\n",
      "whisper_model_load:        Metal total size =   487.01 MB\n",
      "whisper_model_load: model size    =  487.01 MB\n",
      "whisper_backend_init_gpu: using Metal backend\n",
      "ggml_metal_init: allocating\n",
      "ggml_metal_init: found device: Apple M4 Pro\n",
      "ggml_metal_init: picking default device: Apple M4 Pro\n",
      "ggml_metal_load_library: using embedded metal library\n",
      "ggml_metal_init: GPU name:   Apple M4 Pro\n",
      "ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)\n",
      "ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)\n",
      "ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)\n",
      "ggml_metal_init: simdgroup reduction   = true\n",
      "ggml_metal_init: simdgroup matrix mul. = true\n",
      "ggml_metal_init: has residency sets    = true\n",
      "ggml_metal_init: has bfloat            = true\n",
      "ggml_metal_init: use bfloat            = false\n",
      "ggml_metal_init: hasUnifiedMemory      = true\n",
      "ggml_metal_init: recommendedMaxWorkingSetSize  = 38654.71 MB\n",
      "ggml_metal_init: skipping kernel_get_rows_bf16                     (not supported)\n",
      "ggml_metal_init: skipping kernel_mul_mv_bf16_f32                   (not supported)\n",
      "ggml_metal_init: skipping kernel_mul_mv_bf16_f32_1row              (not supported)\n",
      "ggml_metal_init: skipping kernel_mul_mv_bf16_f32_l4                (not supported)\n",
      "ggml_metal_init: skipping kernel_mul_mv_bf16_bf16                  (not supported)\n",
      "ggml_metal_init: skipping kernel_mul_mv_id_bf16_f32                (not supported)\n",
      "ggml_metal_init: skipping kernel_mul_mm_bf16_f32                   (not supported)\n",
      "ggml_metal_init: skipping kernel_mul_mm_id_bf16_f32                (not supported)\n",
      "ggml_metal_init: skipping kernel_flash_attn_ext_bf16_h64           (not supported)\n",
      "ggml_metal_init: skipping kernel_flash_attn_ext_bf16_h80           (not supported)\n",
      "ggml_metal_init: skipping kernel_flash_attn_ext_bf16_h96           (not supported)\n",
      "ggml_metal_init: skipping kernel_flash_attn_ext_bf16_h112          (not supported)\n",
      "ggml_metal_init: skipping kernel_flash_attn_ext_bf16_h128          (not supported)\n",
      "ggml_metal_init: skipping kernel_flash_attn_ext_bf16_h192          (not supported)\n",
      "ggml_metal_init: skipping kernel_flash_attn_ext_bf16_hk192_hv128   (not supported)\n",
      "ggml_metal_init: skipping kernel_flash_attn_ext_bf16_h256          (not supported)\n",
      "ggml_metal_init: skipping kernel_flash_attn_ext_vec_bf16_h128      (not supported)\n",
      "ggml_metal_init: skipping kernel_flash_attn_ext_vec_bf16_h192      (not supported)\n",
      "ggml_metal_init: skipping kernel_flash_attn_ext_vec_bf16_hk192_hv128 (not supported)\n",
      "ggml_metal_init: skipping kernel_flash_attn_ext_vec_bf16_h256      (not supported)\n",
      "ggml_metal_init: skipping kernel_cpy_f32_bf16                      (not supported)\n",
      "ggml_metal_init: skipping kernel_cpy_bf16_f32                      (not supported)\n",
      "ggml_metal_init: skipping kernel_cpy_bf16_bf16                     (not supported)\n",
      "whisper_backend_init: using BLAS backend\n",
      "whisper_init_state: kv self size  =   18.87 MB\n",
      "whisper_init_state: kv cross size =   56.62 MB\n",
      "whisper_init_state: kv pad  size  =    4.72 MB\n",
      "whisper_init_state: compute buffer (conv)   =   23.37 MB\n",
      "whisper_init_state: compute buffer (encode) =  128.01 MB\n",
      "whisper_init_state: compute buffer (cross)  =    6.18 MB\n",
      "whisper_init_state: compute buffer (decode) =   98.19 MB\n",
      "\n",
      "system_info: n_threads = 4 / 14 | WHISPER : COREML = 0 | OPENVINO = 0 | Metal : EMBED_LIBRARY = 1 | CPU : NEON = 1 | ARM_FMA = 1 | FP16_VA = 1 | MATMUL_INT8 = 1 | DOTPROD = 1 | SME = 1 | ACCELERATE = 1 | AARCH64_REPACK = 1 | \n",
      "\n",
      "main: processing './Sound/01/chunk_012.wav' (1919938 samples, 120.0 sec), 4 threads, 1 processors, 5 beams + best of 5, lang = ko, task = transcribe, timestamps = 1 ...\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "[00:00:00.000 --> 00:00:06.480]   나는 이 세 가지 명령어 셋을 jenkins가 target vm에다 내리는 방식이에요 ssh로\n",
      "[00:00:06.480 --> 00:00:12.480]   그래서 이 크게 세 가지만 기억하시면 될 것 같아요\n",
      "[00:00:12.480 --> 00:00:17.440]   jenkins가 vm에다가 접근하기 위해서는 ip랑 id password가 필요하고\n",
      "[00:00:17.440 --> 00:00:21.360]   그러면 이렇게 jenkins가 접근해서 내리 명령어 셋은\n",
      "[00:00:21.360 --> 00:00:27.200]   target vm이 어느 블록 스토리즈에서 어떤 파일을 다운로드 받아서 실행해 나는 명령어구나\n",
      "[00:00:27.200 --> 00:00:31.600]   이렇게 부분하시면 됩니다. 말이 어렵지만\n",
      "[00:00:31.600 --> 00:00:36.880]   소스코드 그 파이프라인은 굉장히 간단하니까 바로 이해하실 수 있을 거예요\n",
      "[00:00:36.880 --> 00:00:40.400]   그래서 이렇게 이해를 한 다음에 쓰레기를 공유해 드렸습니다\n",
      "[00:00:40.400 --> 00:00:45.280]   그 방금 이제 hello world 찍은 잡 있죠 이 안에 부성으로 들어갈게요\n",
      "[00:00:45.280 --> 00:00:48.320]   방금 그 파이프라인은 다시 짜야 되기 때문에\n",
      "[00:00:48.320 --> 00:00:53.280]   부성으로 들어가서 우리가 방금 얘기한 게\n",
      "[00:00:53.280 --> 00:00:57.760]   target vm의 경우에 ip랑 id랑 password가 필요하다고 했죠\n",
      "[00:00:57.760 --> 00:01:03.040]   그거를 변수로 해서 파이프라인 안에 녹일 거라서 그걸 이제 파라미터 화를 해야 돼요\n",
      "[00:01:03.040 --> 00:01:09.520]   이 아래 옵션으로 여러분 다들 이 한글로 보이나요 각자 jenkins들\n",
      "[00:01:09.520 --> 00:01:12.800]   그러면 이 빌드는 매개 변수가 있습니다를 클릭하세요\n",
      "[00:01:12.800 --> 00:01:15.840]   이 빌드는 매개 변수가 있습니다 클립\n",
      "[00:01:15.840 --> 00:01:21.040]   순서대로 매개 변수를 추가할 때 ip랑 id랑 password 순으로 만들 거예요\n",
      "[00:01:21.040 --> 00:01:25.280]   ip는 간단하게 그냥 이 파라미터 종류가 있는 건데\n",
      "[00:01:25.280 --> 00:01:28.000]   파라미터에 대한 타입을 정해주는 거예요 지금\n",
      "[00:01:28.000 --> 00:01:31.040]   그래서 우리는 ip 정할 거니까 스트링 파라미터로\n",
      "[00:01:31.040 --> 00:01:37.520]   어 그냥 편하게 target underbar, server, underbar, ip 이렇게 할게요\n",
      "[00:01:37.520 --> 00:01:42.000]   그 다음에 이거 뭐 이건 선택 하나 안 하는 건 상관없는데 그냥 스트링 그\n",
      "[00:01:42.000 --> 00:01:46.000]   문자열을 트리밍 해 주는 거거든요 저는 기본적으로 옵션에 펴도\n",
      "[00:01:46.000 --> 00:01:50.160]   이제 ip를 담을 곳을 넣었죠 ip를 담을 곳을 넣었거든요\n",
      "[00:01:50.160 --> 00:01:54.000]   이 뒷볼트 밸류는 이제 기본 값으로 설정된 건데 우리는 그냥\n",
      "[00:01:54.000 --> 00:01:59.200]   여기다가 target, server, mic를 유녑합니다\n",
      "[00:01:59.200 --> 00:02:02.320]   이렇게 할게요\n",
      "\n",
      "✅ chunk_012 처리 완료 → ./Text/01/chunk_012_ggml-small.json\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "output_json: saving output to './Text/01/chunk_012_ggml-small.json'\n",
      "\n",
      "whisper_print_timings:     load time =   186.58 ms\n",
      "whisper_print_timings:     fallbacks =   0 p /   0 h\n",
      "whisper_print_timings:      mel time =    28.22 ms\n",
      "whisper_print_timings:   sample time =   376.17 ms /  2625 runs (     0.14 ms per run)\n",
      "whisper_print_timings:   encode time =   573.82 ms /     5 runs (   114.76 ms per run)\n",
      "whisper_print_timings:   decode time =     7.87 ms /     2 runs (     3.94 ms per run)\n",
      "whisper_print_timings:   batchd time =  2370.98 ms /  2601 runs (     0.91 ms per run)\n",
      "whisper_print_timings:   prompt time =    92.73 ms /   807 runs (     0.11 ms per run)\n",
      "whisper_print_timings:    total time =  3702.97 ms\n",
      "ggml_metal_free: deallocating\n",
      "whisper_init_from_file_with_params_no_state: loading model from '/Users/aza1200/.cache/huggingface/hub/models--ggerganov--whisper.cpp/snapshots/5359861c739e955e79d9a303bcbc70fb988958b1/ggml-small.bin'\n",
      "whisper_init_with_params_no_state: use gpu    = 1\n",
      "whisper_init_with_params_no_state: flash attn = 0\n",
      "whisper_init_with_params_no_state: gpu_device = 0\n",
      "whisper_init_with_params_no_state: dtw        = 0\n",
      "whisper_init_with_params_no_state: devices    = 3\n",
      "whisper_init_with_params_no_state: backends   = 3\n",
      "whisper_model_load: loading model\n",
      "whisper_model_load: n_vocab       = 51865\n",
      "whisper_model_load: n_audio_ctx   = 1500\n",
      "whisper_model_load: n_audio_state = 768\n",
      "whisper_model_load: n_audio_head  = 12\n",
      "whisper_model_load: n_audio_layer = 12\n",
      "whisper_model_load: n_text_ctx    = 448\n",
      "whisper_model_load: n_text_state  = 768\n",
      "whisper_model_load: n_text_head   = 12\n",
      "whisper_model_load: n_text_layer  = 12\n",
      "whisper_model_load: n_mels        = 80\n",
      "whisper_model_load: ftype         = 1\n",
      "whisper_model_load: qntvr         = 0\n",
      "whisper_model_load: type          = 3 (small)\n",
      "whisper_model_load: adding 1608 extra tokens\n",
      "whisper_model_load: n_langs       = 99\n",
      "whisper_model_load:        Metal total size =   487.01 MB\n",
      "whisper_model_load: model size    =  487.01 MB\n",
      "whisper_backend_init_gpu: using Metal backend\n",
      "ggml_metal_init: allocating\n",
      "ggml_metal_init: found device: Apple M4 Pro\n",
      "ggml_metal_init: picking default device: Apple M4 Pro\n",
      "ggml_metal_load_library: using embedded metal library\n",
      "ggml_metal_init: GPU name:   Apple M4 Pro\n",
      "ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)\n",
      "ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)\n",
      "ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)\n",
      "ggml_metal_init: simdgroup reduction   = true\n",
      "ggml_metal_init: simdgroup matrix mul. = true\n",
      "ggml_metal_init: has residency sets    = true\n",
      "ggml_metal_init: has bfloat            = true\n",
      "ggml_metal_init: use bfloat            = false\n",
      "ggml_metal_init: hasUnifiedMemory      = true\n",
      "ggml_metal_init: recommendedMaxWorkingSetSize  = 38654.71 MB\n",
      "ggml_metal_init: skipping kernel_get_rows_bf16                     (not supported)\n",
      "ggml_metal_init: skipping kernel_mul_mv_bf16_f32                   (not supported)\n",
      "ggml_metal_init: skipping kernel_mul_mv_bf16_f32_1row              (not supported)\n",
      "ggml_metal_init: skipping kernel_mul_mv_bf16_f32_l4                (not supported)\n",
      "ggml_metal_init: skipping kernel_mul_mv_bf16_bf16                  (not supported)\n",
      "ggml_metal_init: skipping kernel_mul_mv_id_bf16_f32                (not supported)\n",
      "ggml_metal_init: skipping kernel_mul_mm_bf16_f32                   (not supported)\n",
      "ggml_metal_init: skipping kernel_mul_mm_id_bf16_f32                (not supported)\n",
      "ggml_metal_init: skipping kernel_flash_attn_ext_bf16_h64           (not supported)\n",
      "ggml_metal_init: skipping kernel_flash_attn_ext_bf16_h80           (not supported)\n",
      "ggml_metal_init: skipping kernel_flash_attn_ext_bf16_h96           (not supported)\n",
      "ggml_metal_init: skipping kernel_flash_attn_ext_bf16_h112          (not supported)\n",
      "ggml_metal_init: skipping kernel_flash_attn_ext_bf16_h128          (not supported)\n",
      "ggml_metal_init: skipping kernel_flash_attn_ext_bf16_h192          (not supported)\n",
      "ggml_metal_init: skipping kernel_flash_attn_ext_bf16_hk192_hv128   (not supported)\n",
      "ggml_metal_init: skipping kernel_flash_attn_ext_bf16_h256          (not supported)\n",
      "ggml_metal_init: skipping kernel_flash_attn_ext_vec_bf16_h128      (not supported)\n",
      "ggml_metal_init: skipping kernel_flash_attn_ext_vec_bf16_h192      (not supported)\n",
      "ggml_metal_init: skipping kernel_flash_attn_ext_vec_bf16_hk192_hv128 (not supported)\n",
      "ggml_metal_init: skipping kernel_flash_attn_ext_vec_bf16_h256      (not supported)\n",
      "ggml_metal_init: skipping kernel_cpy_f32_bf16                      (not supported)\n",
      "ggml_metal_init: skipping kernel_cpy_bf16_f32                      (not supported)\n",
      "ggml_metal_init: skipping kernel_cpy_bf16_bf16                     (not supported)\n",
      "whisper_backend_init: using BLAS backend\n",
      "whisper_init_state: kv self size  =   18.87 MB\n",
      "whisper_init_state: kv cross size =   56.62 MB\n",
      "whisper_init_state: kv pad  size  =    4.72 MB\n",
      "whisper_init_state: compute buffer (conv)   =   23.37 MB\n",
      "whisper_init_state: compute buffer (encode) =  128.01 MB\n",
      "whisper_init_state: compute buffer (cross)  =    6.18 MB\n",
      "whisper_init_state: compute buffer (decode) =   98.19 MB\n",
      "\n",
      "system_info: n_threads = 4 / 14 | WHISPER : COREML = 0 | OPENVINO = 0 | Metal : EMBED_LIBRARY = 1 | CPU : NEON = 1 | ARM_FMA = 1 | FP16_VA = 1 | MATMUL_INT8 = 1 | DOTPROD = 1 | SME = 1 | ACCELERATE = 1 | AARCH64_REPACK = 1 | \n",
      "\n",
      "main: processing './Sound/01/chunk_013.wav' (1920034 samples, 120.0 sec), 4 threads, 1 processors, 5 beams + best of 5, lang = ko, task = transcribe, timestamps = 1 ...\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "[00:00:00.000 --> 00:00:04.300]   그럼 이제 IP를 넣을 수 있는 공간을 만든 거에요, 파라미터를\n",
      "[00:00:04.300 --> 00:00:07.980]   그리고 이제 뭐 만들어야 되죠? 서버에 적속한 ID를 담아야 되죠\n",
      "[00:00:07.980 --> 00:00:10.320]   이것도 똑같이 스트링 파라미터로\n",
      "[00:00:10.320 --> 00:00:16.720]   어 매개변수명은 타겟, 서버, 뭐 유저 네임 이런 거 할게요\n",
      "[00:00:16.720 --> 00:00:20.680]   또는 ID로 할까? ID로 할까? 이런 식으로\n",
      "[00:00:20.680 --> 00:00:27.840]   디콜트 밸류에도 똑같이 타겟 서버에 적속한 ID를 입력합니다\n",
      "[00:00:27.840 --> 00:00:32.080]   직접 이렇게 이제 ID까지 받을 수 있어요\n",
      "[00:00:32.080 --> 00:00:36.160]   그럼 젠킨스가 지금 IP랑 ID를 변수화해서 쓸 수 있고\n",
      "[00:00:36.160 --> 00:00:40.120]   마지막으로 우리가 서버에 적속할 패스워드를 넣어줄 건데\n",
      "[00:00:40.120 --> 00:00:45.160]   패스워드는 스트링 파라미터로 쓰게 되면 이제 자매 노출이 되겠죠\n",
      "[00:00:45.160 --> 00:00:48.440]   그래서 이 매개변수에 패스워드 파라미터라는 게 있어요\n",
      "[00:00:48.440 --> 00:00:51.840]   패스워드 파라미터를 넣을 때 쓰시면 됩니다\n",
      "[00:00:51.840 --> 00:00:57.360]   패스워드 파라미터를 넣어서 타겟, 서버, 패스워드\n",
      "[00:00:57.360 --> 00:01:01.320]   이런 식으로 하고 그냥 편하게\n",
      "[00:01:01.320 --> 00:01:04.080]   아 요거는 그게 에스터리스크 처리가 되가지고\n",
      "[00:01:04.080 --> 00:01:08.520]   그냥 어 패스워드는 빼셔도 되고\n",
      "[00:01:08.520 --> 00:01:11.560]   접속한 패스워드를 입력합니다\n",
      "[00:01:11.560 --> 00:01:14.000]   정도를 상세에 넣어둘게요\n",
      "[00:01:14.000 --> 00:01:16.680]   그 정도? 그럼 우선 날아가면 안 되니까\n",
      "[00:01:16.680 --> 00:01:18.520]   어플라이를 먼저 하고\n",
      "[00:01:18.520 --> 00:01:21.360]   보자 아직 파이프라인은 짠 단계가 아니라\n",
      "[00:01:21.360 --> 00:01:23.640]   매개변수만 넣은 단계입니다\n",
      "[00:01:23.640 --> 00:01:28.720]   그러면은 저장하게 되면 이제 잡을 메인으로 나오게 되는데\n",
      "[00:01:28.720 --> 00:01:31.560]   왼쪽에 파라미터와 함께 빌드라고 바뀌어있어요\n",
      "[00:01:31.560 --> 00:01:33.360]   아까는 지금 빌드였거든요\n",
      "[00:01:33.360 --> 00:01:36.800]   우리가 이 속성 값으로 이제 매개변수를 넣었기 때문에\n",
      "[00:01:36.800 --> 00:01:39.840]   파라미터와 함께 빌드라는 탭이 생긴 거고\n",
      "[00:01:39.840 --> 00:01:41.840]   이걸 누르게 되면은\n",
      "[00:01:41.840 --> 00:01:46.200]   방금 저장한 이 3개의 매개변수에 어떤 값을 넣을지에 대해서\n",
      "[00:01:46.200 --> 00:01:48.040]   이제 인프 값을 받을 수 있어요\n",
      "[00:01:48.040 --> 00:01:50.960]   그러면은 이렇게 환경 설정이 된 거고\n",
      "[00:01:50.960 --> 00:01:55.880]   이거를 각각 파이프라인에서 어떻게 쓰는지에 대한 설정만 이제 보여드릴게요\n",
      "[00:01:55.880 --> 00:01:58.040]   다시 구성에 들어가서\n",
      "[00:01:58.040 --> 00:01:59.360]   이제 파이프라인이 짜야 되니까\n",
      "\n",
      "✅ chunk_013 처리 완료 → ./Text/01/chunk_013_ggml-small.json\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "output_json: saving output to './Text/01/chunk_013_ggml-small.json'\n",
      "\n",
      "whisper_print_timings:     load time =   178.34 ms\n",
      "whisper_print_timings:     fallbacks =   0 p /   0 h\n",
      "whisper_print_timings:      mel time =    28.27 ms\n",
      "whisper_print_timings:   sample time =   408.08 ms /  2829 runs (     0.14 ms per run)\n",
      "whisper_print_timings:   encode time =   574.11 ms /     5 runs (   114.82 ms per run)\n",
      "whisper_print_timings:   decode time =    42.20 ms /    10 runs (     4.22 ms per run)\n",
      "whisper_print_timings:   batchd time =  2548.62 ms /  2797 runs (     0.91 ms per run)\n",
      "whisper_print_timings:   prompt time =    94.18 ms /   813 runs (     0.12 ms per run)\n",
      "whisper_print_timings:    total time =  3939.23 ms\n",
      "ggml_metal_free: deallocating\n",
      "whisper_init_from_file_with_params_no_state: loading model from '/Users/aza1200/.cache/huggingface/hub/models--ggerganov--whisper.cpp/snapshots/5359861c739e955e79d9a303bcbc70fb988958b1/ggml-small.bin'\n",
      "whisper_init_with_params_no_state: use gpu    = 1\n",
      "whisper_init_with_params_no_state: flash attn = 0\n",
      "whisper_init_with_params_no_state: gpu_device = 0\n",
      "whisper_init_with_params_no_state: dtw        = 0\n",
      "whisper_init_with_params_no_state: devices    = 3\n",
      "whisper_init_with_params_no_state: backends   = 3\n",
      "whisper_model_load: loading model\n",
      "whisper_model_load: n_vocab       = 51865\n",
      "whisper_model_load: n_audio_ctx   = 1500\n",
      "whisper_model_load: n_audio_state = 768\n",
      "whisper_model_load: n_audio_head  = 12\n",
      "whisper_model_load: n_audio_layer = 12\n",
      "whisper_model_load: n_text_ctx    = 448\n",
      "whisper_model_load: n_text_state  = 768\n",
      "whisper_model_load: n_text_head   = 12\n",
      "whisper_model_load: n_text_layer  = 12\n",
      "whisper_model_load: n_mels        = 80\n",
      "whisper_model_load: ftype         = 1\n",
      "whisper_model_load: qntvr         = 0\n",
      "whisper_model_load: type          = 3 (small)\n",
      "whisper_model_load: adding 1608 extra tokens\n",
      "whisper_model_load: n_langs       = 99\n",
      "whisper_model_load:        Metal total size =   487.01 MB\n",
      "whisper_model_load: model size    =  487.01 MB\n",
      "whisper_backend_init_gpu: using Metal backend\n",
      "ggml_metal_init: allocating\n",
      "ggml_metal_init: found device: Apple M4 Pro\n",
      "ggml_metal_init: picking default device: Apple M4 Pro\n",
      "ggml_metal_load_library: using embedded metal library\n",
      "ggml_metal_init: GPU name:   Apple M4 Pro\n",
      "ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)\n",
      "ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)\n",
      "ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)\n",
      "ggml_metal_init: simdgroup reduction   = true\n",
      "ggml_metal_init: simdgroup matrix mul. = true\n",
      "ggml_metal_init: has residency sets    = true\n",
      "ggml_metal_init: has bfloat            = true\n",
      "ggml_metal_init: use bfloat            = false\n",
      "ggml_metal_init: hasUnifiedMemory      = true\n",
      "ggml_metal_init: recommendedMaxWorkingSetSize  = 38654.71 MB\n",
      "ggml_metal_init: skipping kernel_get_rows_bf16                     (not supported)\n",
      "ggml_metal_init: skipping kernel_mul_mv_bf16_f32                   (not supported)\n",
      "ggml_metal_init: skipping kernel_mul_mv_bf16_f32_1row              (not supported)\n",
      "ggml_metal_init: skipping kernel_mul_mv_bf16_f32_l4                (not supported)\n",
      "ggml_metal_init: skipping kernel_mul_mv_bf16_bf16                  (not supported)\n",
      "ggml_metal_init: skipping kernel_mul_mv_id_bf16_f32                (not supported)\n",
      "ggml_metal_init: skipping kernel_mul_mm_bf16_f32                   (not supported)\n",
      "ggml_metal_init: skipping kernel_mul_mm_id_bf16_f32                (not supported)\n",
      "ggml_metal_init: skipping kernel_flash_attn_ext_bf16_h64           (not supported)\n",
      "ggml_metal_init: skipping kernel_flash_attn_ext_bf16_h80           (not supported)\n",
      "ggml_metal_init: skipping kernel_flash_attn_ext_bf16_h96           (not supported)\n",
      "ggml_metal_init: skipping kernel_flash_attn_ext_bf16_h112          (not supported)\n",
      "ggml_metal_init: skipping kernel_flash_attn_ext_bf16_h128          (not supported)\n",
      "ggml_metal_init: skipping kernel_flash_attn_ext_bf16_h192          (not supported)\n",
      "ggml_metal_init: skipping kernel_flash_attn_ext_bf16_hk192_hv128   (not supported)\n",
      "ggml_metal_init: skipping kernel_flash_attn_ext_bf16_h256          (not supported)\n",
      "ggml_metal_init: skipping kernel_flash_attn_ext_vec_bf16_h128      (not supported)\n",
      "ggml_metal_init: skipping kernel_flash_attn_ext_vec_bf16_h192      (not supported)\n",
      "ggml_metal_init: skipping kernel_flash_attn_ext_vec_bf16_hk192_hv128 (not supported)\n",
      "ggml_metal_init: skipping kernel_flash_attn_ext_vec_bf16_h256      (not supported)\n",
      "ggml_metal_init: skipping kernel_cpy_f32_bf16                      (not supported)\n",
      "ggml_metal_init: skipping kernel_cpy_bf16_f32                      (not supported)\n",
      "ggml_metal_init: skipping kernel_cpy_bf16_bf16                     (not supported)\n",
      "whisper_backend_init: using BLAS backend\n",
      "whisper_init_state: kv self size  =   18.87 MB\n",
      "whisper_init_state: kv cross size =   56.62 MB\n",
      "whisper_init_state: kv pad  size  =    4.72 MB\n",
      "whisper_init_state: compute buffer (conv)   =   23.37 MB\n",
      "whisper_init_state: compute buffer (encode) =  128.01 MB\n",
      "whisper_init_state: compute buffer (cross)  =    6.18 MB\n",
      "whisper_init_state: compute buffer (decode) =   98.19 MB\n",
      "\n",
      "system_info: n_threads = 4 / 14 | WHISPER : COREML = 0 | OPENVINO = 0 | Metal : EMBED_LIBRARY = 1 | CPU : NEON = 1 | ARM_FMA = 1 | FP16_VA = 1 | MATMUL_INT8 = 1 | DOTPROD = 1 | SME = 1 | ACCELERATE = 1 | AARCH64_REPACK = 1 | \n",
      "\n",
      "main: processing './Sound/01/chunk_014.wav' (1919647 samples, 120.0 sec), 4 threads, 1 processors, 5 beams + best of 5, lang = ko, task = transcribe, timestamps = 1 ...\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "[00:00:00.000 --> 00:00:03.480]   스크롤을 다운해서 이쪽으로 내려오겠습니다.\n",
      "[00:00:03.480 --> 00:00:06.480]   그래서 기존에 이제 해가 걸렸던 것 다 지우고\n",
      "[00:00:06.480 --> 00:00:12.680]   소스코드를 드릴 건데 여기에 대한 간단한 설명을 먼저 드릴게요 지금.\n",
      "[00:00:12.680 --> 00:00:18.480]   보시면 길지 않아요. 길지 않고 처음에는 이제\n",
      "[00:00:18.480 --> 00:00:23.560]   이게 다 그룹이 언어라는 점을 상구를 해주시고 그룹이가 좀 C#인가?\n",
      "[00:00:23.560 --> 00:00:27.640]   그거랑 좀 비슷하다고 하는데 저는 잘 몰라. C#을 잘 몰라서 그렇겠네요.\n",
      "[00:00:27.640 --> 00:00:31.520]   리봄트라는 이제 배열에다가 명령을 다 담을 거예요.\n",
      "[00:00:31.520 --> 00:00:35.640]   명령을 다 담을 거고 이걸 정리해주는 리스트를 만드는 거죠.\n",
      "[00:00:35.640 --> 00:00:42.320]   파일선으로 치면 거기에 이제 파이프라인이라는 최상위 개념을 써서\n",
      "[00:00:42.320 --> 00:00:43.920]   이 스테이지라는 부분을 씁니다.\n",
      "[00:00:43.920 --> 00:00:46.920]   여러분도 어제 기타백선을 해보셨으니까\n",
      "[00:00:46.920 --> 00:00:50.840]   그 자 안에 이제 스텝데일로 나누는 건 좀 익숙하실 것 같아요.\n",
      "[00:00:50.840 --> 00:00:54.000]   자 안에 들어가는 게 그래서 첫 번째 스테이지에는\n",
      "[00:00:54.000 --> 00:00:58.840]   이 SSH에 대한 설정법을 배열에다가 담는 거를 할 거예요.\n",
      "[00:00:58.840 --> 00:01:03.840]   그래서 이런 문자 이런 문법 같은 거는 간단하게 보시고\n",
      "[00:01:03.840 --> 00:01:08.440]   처음에는 우리가 방금 정한 이거 있죠.\n",
      "[00:01:08.440 --> 00:01:10.080]   하나 더 넣어야 되겠네요.\n",
      "[00:01:10.080 --> 00:01:12.400]   좀 더 할게요. 타겟 서버 이름을 정해야 됩니다.\n",
      "[00:01:12.400 --> 00:01:13.640]   타겟 서버 이름.\n",
      "[00:01:13.640 --> 00:01:18.880]   그다음에 우리가 어느 서버에 접속할지에 대해서 타겟 서버 IP를 넣었죠.\n",
      "[00:01:18.880 --> 00:01:27.160]   그 다음에 여기에 접속할 유저네임을 넣기 위해서 타겟 서버 유저네임을 넣었어요.\n",
      "[00:01:27.160 --> 00:01:32.720]   그다음에 여기에 접속할 패스워드를 넣기 위해서 타겟 서버 패스워드를 넣었어요.\n",
      "[00:01:32.720 --> 00:01:38.820]   그다음에 뭐 이런 식으로 로그래벨 인포까지 설정한 다음에 여기에 대한 모든 환경 변수가\n",
      "[00:01:38.820 --> 00:01:45.760]   이 SSH 커맨드로 이 서버 IP를 가진 서버에다가 날라가는 걸 이 명령을 통째로\n",
      "[00:01:45.760 --> 00:01:53.060]   그러면 여기에 대한 정보를 가지고 여기에 있는 SSH 커맨드가 타겟 서버에다 날라간다고 생각하시면 돼요.\n",
      "[00:01:53.060 --> 00:01:59.560]   그래서 이 앞에 있는 SSH 커맨드라는 플러그인을 써서 이 PWD가 찍히는 서버는\n",
      "\n",
      "✅ chunk_014 처리 완료 → ./Text/01/chunk_014_ggml-small.json\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "output_json: saving output to './Text/01/chunk_014_ggml-small.json'\n",
      "\n",
      "whisper_print_timings:     load time =   177.49 ms\n",
      "whisper_print_timings:     fallbacks =   0 p /   0 h\n",
      "whisper_print_timings:      mel time =    28.63 ms\n",
      "whisper_print_timings:   sample time =   407.61 ms /  2808 runs (     0.15 ms per run)\n",
      "whisper_print_timings:   encode time =   571.71 ms /     5 runs (   114.34 ms per run)\n",
      "whisper_print_timings:   decode time =    38.48 ms /     9 runs (     4.28 ms per run)\n",
      "whisper_print_timings:   batchd time =  2475.83 ms /  2777 runs (     0.89 ms per run)\n",
      "whisper_print_timings:   prompt time =    92.61 ms /   810 runs (     0.11 ms per run)\n",
      "whisper_print_timings:    total time =  3857.30 ms\n",
      "ggml_metal_free: deallocating\n",
      "whisper_init_from_file_with_params_no_state: loading model from '/Users/aza1200/.cache/huggingface/hub/models--ggerganov--whisper.cpp/snapshots/5359861c739e955e79d9a303bcbc70fb988958b1/ggml-small.bin'\n",
      "whisper_init_with_params_no_state: use gpu    = 1\n",
      "whisper_init_with_params_no_state: flash attn = 0\n",
      "whisper_init_with_params_no_state: gpu_device = 0\n",
      "whisper_init_with_params_no_state: dtw        = 0\n",
      "whisper_init_with_params_no_state: devices    = 3\n",
      "whisper_init_with_params_no_state: backends   = 3\n",
      "whisper_model_load: loading model\n",
      "whisper_model_load: n_vocab       = 51865\n",
      "whisper_model_load: n_audio_ctx   = 1500\n",
      "whisper_model_load: n_audio_state = 768\n",
      "whisper_model_load: n_audio_head  = 12\n",
      "whisper_model_load: n_audio_layer = 12\n",
      "whisper_model_load: n_text_ctx    = 448\n",
      "whisper_model_load: n_text_state  = 768\n",
      "whisper_model_load: n_text_head   = 12\n",
      "whisper_model_load: n_text_layer  = 12\n",
      "whisper_model_load: n_mels        = 80\n",
      "whisper_model_load: ftype         = 1\n",
      "whisper_model_load: qntvr         = 0\n",
      "whisper_model_load: type          = 3 (small)\n",
      "whisper_model_load: adding 1608 extra tokens\n",
      "whisper_model_load: n_langs       = 99\n",
      "whisper_model_load:        Metal total size =   487.01 MB\n",
      "whisper_model_load: model size    =  487.01 MB\n",
      "whisper_backend_init_gpu: using Metal backend\n",
      "ggml_metal_init: allocating\n",
      "ggml_metal_init: found device: Apple M4 Pro\n",
      "ggml_metal_init: picking default device: Apple M4 Pro\n",
      "ggml_metal_load_library: using embedded metal library\n",
      "ggml_metal_init: GPU name:   Apple M4 Pro\n",
      "ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)\n",
      "ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)\n",
      "ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)\n",
      "ggml_metal_init: simdgroup reduction   = true\n",
      "ggml_metal_init: simdgroup matrix mul. = true\n",
      "ggml_metal_init: has residency sets    = true\n",
      "ggml_metal_init: has bfloat            = true\n",
      "ggml_metal_init: use bfloat            = false\n",
      "ggml_metal_init: hasUnifiedMemory      = true\n",
      "ggml_metal_init: recommendedMaxWorkingSetSize  = 38654.71 MB\n",
      "ggml_metal_init: skipping kernel_get_rows_bf16                     (not supported)\n",
      "ggml_metal_init: skipping kernel_mul_mv_bf16_f32                   (not supported)\n",
      "ggml_metal_init: skipping kernel_mul_mv_bf16_f32_1row              (not supported)\n",
      "ggml_metal_init: skipping kernel_mul_mv_bf16_f32_l4                (not supported)\n",
      "ggml_metal_init: skipping kernel_mul_mv_bf16_bf16                  (not supported)\n",
      "ggml_metal_init: skipping kernel_mul_mv_id_bf16_f32                (not supported)\n",
      "ggml_metal_init: skipping kernel_mul_mm_bf16_f32                   (not supported)\n",
      "ggml_metal_init: skipping kernel_mul_mm_id_bf16_f32                (not supported)\n",
      "ggml_metal_init: skipping kernel_flash_attn_ext_bf16_h64           (not supported)\n",
      "ggml_metal_init: skipping kernel_flash_attn_ext_bf16_h80           (not supported)\n",
      "ggml_metal_init: skipping kernel_flash_attn_ext_bf16_h96           (not supported)\n",
      "ggml_metal_init: skipping kernel_flash_attn_ext_bf16_h112          (not supported)\n",
      "ggml_metal_init: skipping kernel_flash_attn_ext_bf16_h128          (not supported)\n",
      "ggml_metal_init: skipping kernel_flash_attn_ext_bf16_h192          (not supported)\n",
      "ggml_metal_init: skipping kernel_flash_attn_ext_bf16_hk192_hv128   (not supported)\n",
      "ggml_metal_init: skipping kernel_flash_attn_ext_bf16_h256          (not supported)\n",
      "ggml_metal_init: skipping kernel_flash_attn_ext_vec_bf16_h128      (not supported)\n",
      "ggml_metal_init: skipping kernel_flash_attn_ext_vec_bf16_h192      (not supported)\n",
      "ggml_metal_init: skipping kernel_flash_attn_ext_vec_bf16_hk192_hv128 (not supported)\n",
      "ggml_metal_init: skipping kernel_flash_attn_ext_vec_bf16_h256      (not supported)\n",
      "ggml_metal_init: skipping kernel_cpy_f32_bf16                      (not supported)\n",
      "ggml_metal_init: skipping kernel_cpy_bf16_f32                      (not supported)\n",
      "ggml_metal_init: skipping kernel_cpy_bf16_bf16                     (not supported)\n",
      "whisper_backend_init: using BLAS backend\n",
      "whisper_init_state: kv self size  =   18.87 MB\n",
      "whisper_init_state: kv cross size =   56.62 MB\n",
      "whisper_init_state: kv pad  size  =    4.72 MB\n",
      "whisper_init_state: compute buffer (conv)   =   23.37 MB\n",
      "whisper_init_state: compute buffer (encode) =  128.01 MB\n",
      "whisper_init_state: compute buffer (cross)  =    6.18 MB\n",
      "whisper_init_state: compute buffer (decode) =   98.19 MB\n",
      "\n",
      "system_info: n_threads = 4 / 14 | WHISPER : COREML = 0 | OPENVINO = 0 | Metal : EMBED_LIBRARY = 1 | CPU : NEON = 1 | ARM_FMA = 1 | FP16_VA = 1 | MATMUL_INT8 = 1 | DOTPROD = 1 | SME = 1 | ACCELERATE = 1 | AARCH64_REPACK = 1 | \n",
      "\n",
      "main: processing './Sound/01/chunk_015.wav' (1920410 samples, 120.0 sec), 4 threads, 1 processors, 5 beams + best of 5, lang = ko, task = transcribe, timestamps = 1 ...\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "[00:00:00.000 --> 00:00:01.900]   타겟 서버가 리턴하는 건\n",
      "[00:00:01.900 --> 00:00:03.720]   왜냐하면 SSH 커맨드로\n",
      "[00:00:03.720 --> 00:00:06.200]   젠키인스가 이런 환경 분수를 가지고\n",
      "[00:00:06.200 --> 00:00:08.080]   얘한테 명령을 내린 거기 때문에\n",
      "[00:00:08.080 --> 00:00:09.600]   그걸 좀 이해하셔야 돼요\n",
      "[00:00:09.600 --> 00:00:11.840]   그러면 만약에 PWD에 대한\n",
      "[00:00:11.840 --> 00:00:13.320]   현재 디렉토리 패스 찍는\n",
      "[00:00:13.320 --> 00:00:15.640]   그런 간단한 LSE 이런 거 하셔도 되고요\n",
      "[00:00:15.640 --> 00:00:17.600]   리스트 보는 그런 거\n",
      "[00:00:17.600 --> 00:00:19.320]   그래서 만약에 반응이 잘 온다 하면은\n",
      "[00:00:19.320 --> 00:00:22.840]   그 다음에 뭘 배포할지에 대한 설정이 휴가겠죠\n",
      "[00:00:22.840 --> 00:00:27.760]   이거는 순서대로 말씀을 드리자면\n",
      "[00:00:27.760 --> 00:00:30.680]   아까 이제 제가 말씀드린 게\n",
      "[00:00:30.680 --> 00:00:31.960]   명령어가 이제 이거예요\n",
      "[00:00:31.960 --> 00:00:33.400]   어느 블록 스토리지에서\n",
      "[00:00:33.400 --> 00:00:34.920]   어떤 파일을 다운로드 받아서\n",
      "[00:00:34.920 --> 00:00:37.240]   실행해 라는 세 가지 셋이죠\n",
      "[00:00:37.240 --> 00:00:42.280]   그래서 처음에 이 애저 CLI로 시작하는 애저 명령어가\n",
      "[00:00:42.280 --> 00:00:44.120]   이 스토리지라는 애저 스토리지\n",
      "[00:00:44.120 --> 00:00:46.640]   그러니까 블록 관련된 명령어거든요\n",
      "[00:00:46.640 --> 00:00:48.120]   그러면 어느 블록에다가\n",
      "[00:00:48.120 --> 00:00:50.040]   접근하는지에 대한 인증이 필요하겠죠\n",
      "[00:00:50.040 --> 00:00:51.440]   거기에 대한 인증값이\n",
      "[00:00:51.440 --> 00:00:53.600]   뭐 아카운트키 이런 옵션이 들어가고\n",
      "[00:00:53.600 --> 00:00:56.160]   이 스토리지 아카운트에 대한 네임\n",
      "[00:00:56.160 --> 00:00:58.120]   그 다음에 이 컨테이너\n",
      "[00:00:58.120 --> 00:01:02.880]   그러니까 어제 스토리지 아카운트 보여드렸을 때\n",
      "[00:01:02.880 --> 00:01:05.600]   그 안에 데이터를 저장할 수 있는 형식으로\n",
      "[00:01:05.600 --> 00:01:09.520]   컨테이너랑 파일 세워랑 큐랑 테이블이 있었어요\n",
      "[00:01:09.520 --> 00:01:12.480]   예를 들어 이런 거다\n",
      "[00:01:12.480 --> 00:01:14.280]   데이터 스토리지에 이 컨테이너 방식 있죠\n",
      "[00:01:14.280 --> 00:01:16.800]   여기에 우리가 아까 얘기한 점 자르나 점 와르나\n",
      "[00:01:16.800 --> 00:01:18.240]   그런 파일들이 저장되어 있을 거죠\n",
      "[00:01:18.240 --> 00:01:19.800]   CLI가 잘 됐다면\n",
      "[00:01:19.800 --> 00:01:21.440]   따라서 여기에 접근해서\n",
      "[00:01:21.440 --> 00:01:23.280]   다운로드 받아라는 명령어를\n",
      "[00:01:23.280 --> 00:01:24.440]   제가 방금 보여드린 거예요\n",
      "[00:01:24.440 --> 00:01:27.280]   이해가 쉽게 공유를 드리자면\n",
      "[00:01:27.280 --> 00:01:29.960]   그리고 그게 잘 다운로드 받았으면\n",
      "[00:01:29.960 --> 00:01:32.760]   LS 찍혀서 다운로드 잘 받았는지 볼 수 있겠죠\n",
      "[00:01:32.760 --> 00:01:34.400]   또 자바 같은 경우에는\n",
      "[00:01:34.400 --> 00:01:35.920]   자바 프로세스 듀오는\n",
      "[00:01:35.920 --> 00:01:37.600]   자바 자를 이런 명령어로\n",
      "[00:01:37.600 --> 00:01:41.280]   타겟 VMI 본인의 이 배포된 이미지를\n",
      "[00:01:41.280 --> 00:01:45.920]   CL이 된 이 파일을 배포를 하게 되는 그런 케이스입니다\n",
      "[00:01:45.920 --> 00:01:49.600]   그래서 이 명령어는 지금 실행은 안 되실 거고\n",
      "[00:01:49.600 --> 00:01:50.760]   이런 식의 흐름만\n",
      "[00:01:50.760 --> 00:01:52.640]   이해를 좀 해 주시면 될 거 같아요\n",
      "[00:01:52.640 --> 00:01:55.080]   케이스 배포 같은 경우는 어렵지 않아서\n",
      "[00:01:55.080 --> 00:01:59.080]   오늘은 이런 식으로 잡을 꾸리는 것까지만\n",
      "[00:01:59.080 --> 00:01:59.560]   이제\n",
      "\n",
      "✅ chunk_015 처리 완료 → ./Text/01/chunk_015_ggml-small.json\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "output_json: saving output to './Text/01/chunk_015_ggml-small.json'\n",
      "\n",
      "whisper_print_timings:     load time =   183.02 ms\n",
      "whisper_print_timings:     fallbacks =   0 p /   0 h\n",
      "whisper_print_timings:      mel time =    28.47 ms\n",
      "whisper_print_timings:   sample time =   472.47 ms /  3284 runs (     0.14 ms per run)\n",
      "whisper_print_timings:   encode time =   570.46 ms /     5 runs (   114.09 ms per run)\n",
      "whisper_print_timings:   decode time =     4.43 ms /     1 runs (     4.43 ms per run)\n",
      "whisper_print_timings:   batchd time =  2974.68 ms /  3261 runs (     0.91 ms per run)\n",
      "whisper_print_timings:   prompt time =    95.33 ms /   830 runs (     0.11 ms per run)\n",
      "whisper_print_timings:    total time =  4396.22 ms\n",
      "ggml_metal_free: deallocating\n",
      "whisper_init_from_file_with_params_no_state: loading model from '/Users/aza1200/.cache/huggingface/hub/models--ggerganov--whisper.cpp/snapshots/5359861c739e955e79d9a303bcbc70fb988958b1/ggml-small.bin'\n",
      "whisper_init_with_params_no_state: use gpu    = 1\n",
      "whisper_init_with_params_no_state: flash attn = 0\n",
      "whisper_init_with_params_no_state: gpu_device = 0\n",
      "whisper_init_with_params_no_state: dtw        = 0\n",
      "whisper_init_with_params_no_state: devices    = 3\n",
      "whisper_init_with_params_no_state: backends   = 3\n",
      "whisper_model_load: loading model\n",
      "whisper_model_load: n_vocab       = 51865\n",
      "whisper_model_load: n_audio_ctx   = 1500\n",
      "whisper_model_load: n_audio_state = 768\n",
      "whisper_model_load: n_audio_head  = 12\n",
      "whisper_model_load: n_audio_layer = 12\n",
      "whisper_model_load: n_text_ctx    = 448\n",
      "whisper_model_load: n_text_state  = 768\n",
      "whisper_model_load: n_text_head   = 12\n",
      "whisper_model_load: n_text_layer  = 12\n",
      "whisper_model_load: n_mels        = 80\n",
      "whisper_model_load: ftype         = 1\n",
      "whisper_model_load: qntvr         = 0\n",
      "whisper_model_load: type          = 3 (small)\n",
      "whisper_model_load: adding 1608 extra tokens\n",
      "whisper_model_load: n_langs       = 99\n",
      "whisper_model_load:        Metal total size =   487.01 MB\n",
      "whisper_model_load: model size    =  487.01 MB\n",
      "whisper_backend_init_gpu: using Metal backend\n",
      "ggml_metal_init: allocating\n",
      "ggml_metal_init: found device: Apple M4 Pro\n",
      "ggml_metal_init: picking default device: Apple M4 Pro\n",
      "ggml_metal_load_library: using embedded metal library\n",
      "ggml_metal_init: GPU name:   Apple M4 Pro\n",
      "ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)\n",
      "ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)\n",
      "ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)\n",
      "ggml_metal_init: simdgroup reduction   = true\n",
      "ggml_metal_init: simdgroup matrix mul. = true\n",
      "ggml_metal_init: has residency sets    = true\n",
      "ggml_metal_init: has bfloat            = true\n",
      "ggml_metal_init: use bfloat            = false\n",
      "ggml_metal_init: hasUnifiedMemory      = true\n",
      "ggml_metal_init: recommendedMaxWorkingSetSize  = 38654.71 MB\n",
      "ggml_metal_init: skipping kernel_get_rows_bf16                     (not supported)\n",
      "ggml_metal_init: skipping kernel_mul_mv_bf16_f32                   (not supported)\n",
      "ggml_metal_init: skipping kernel_mul_mv_bf16_f32_1row              (not supported)\n",
      "ggml_metal_init: skipping kernel_mul_mv_bf16_f32_l4                (not supported)\n",
      "ggml_metal_init: skipping kernel_mul_mv_bf16_bf16                  (not supported)\n",
      "ggml_metal_init: skipping kernel_mul_mv_id_bf16_f32                (not supported)\n",
      "ggml_metal_init: skipping kernel_mul_mm_bf16_f32                   (not supported)\n",
      "ggml_metal_init: skipping kernel_mul_mm_id_bf16_f32                (not supported)\n",
      "ggml_metal_init: skipping kernel_flash_attn_ext_bf16_h64           (not supported)\n",
      "ggml_metal_init: skipping kernel_flash_attn_ext_bf16_h80           (not supported)\n",
      "ggml_metal_init: skipping kernel_flash_attn_ext_bf16_h96           (not supported)\n",
      "ggml_metal_init: skipping kernel_flash_attn_ext_bf16_h112          (not supported)\n",
      "ggml_metal_init: skipping kernel_flash_attn_ext_bf16_h128          (not supported)\n",
      "ggml_metal_init: skipping kernel_flash_attn_ext_bf16_h192          (not supported)\n",
      "ggml_metal_init: skipping kernel_flash_attn_ext_bf16_hk192_hv128   (not supported)\n",
      "ggml_metal_init: skipping kernel_flash_attn_ext_bf16_h256          (not supported)\n",
      "ggml_metal_init: skipping kernel_flash_attn_ext_vec_bf16_h128      (not supported)\n",
      "ggml_metal_init: skipping kernel_flash_attn_ext_vec_bf16_h192      (not supported)\n",
      "ggml_metal_init: skipping kernel_flash_attn_ext_vec_bf16_hk192_hv128 (not supported)\n",
      "ggml_metal_init: skipping kernel_flash_attn_ext_vec_bf16_h256      (not supported)\n",
      "ggml_metal_init: skipping kernel_cpy_f32_bf16                      (not supported)\n",
      "ggml_metal_init: skipping kernel_cpy_bf16_f32                      (not supported)\n",
      "ggml_metal_init: skipping kernel_cpy_bf16_bf16                     (not supported)\n",
      "whisper_backend_init: using BLAS backend\n",
      "whisper_init_state: kv self size  =   18.87 MB\n",
      "whisper_init_state: kv cross size =   56.62 MB\n",
      "whisper_init_state: kv pad  size  =    4.72 MB\n",
      "whisper_init_state: compute buffer (conv)   =   23.37 MB\n",
      "whisper_init_state: compute buffer (encode) =  128.01 MB\n",
      "whisper_init_state: compute buffer (cross)  =    6.18 MB\n",
      "whisper_init_state: compute buffer (decode) =   98.19 MB\n",
      "\n",
      "system_info: n_threads = 4 / 14 | WHISPER : COREML = 0 | OPENVINO = 0 | Metal : EMBED_LIBRARY = 1 | CPU : NEON = 1 | ARM_FMA = 1 | FP16_VA = 1 | MATMUL_INT8 = 1 | DOTPROD = 1 | SME = 1 | ACCELERATE = 1 | AARCH64_REPACK = 1 | \n",
      "\n",
      "main: processing './Sound/01/chunk_016.wav' (1919516 samples, 120.0 sec), 4 threads, 1 processors, 5 beams + best of 5, lang = ko, task = transcribe, timestamps = 1 ...\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "[00:00:00.000 --> 00:00:03.960]   약간 대모식으로 보여드리겠습니다.\n",
      "[00:00:03.960 --> 00:00:09.540]   그리고 10시 30분까지 쉬었다가\n",
      "[00:00:09.540 --> 00:00:13.860]   아르코시디 하기 전에 이론 조금 하고\n",
      "[00:00:13.860 --> 00:00:17.860]   오후에는 아르코시디 100% 실습을 진행하도록 하겠습니다.\n",
      "[00:00:17.860 --> 00:00:21.580]   오늘 이 젠키스 하면서 이해 안 되시는 거 있으셨는데\n",
      "[00:00:21.580 --> 00:00:32.320]   이게 뭔가 어렵다 라고 생각합니다.\n",
      "[00:00:32.320 --> 00:00:34.100]   어디서 뱉어볼까요?\n",
      "[00:00:34.100 --> 00:00:35.680]   씨하이 시기로 뱉어할게요.\n",
      "[00:00:35.680 --> 00:00:37.720]   김다 맥션으로 뱉어할게요.\n",
      "[00:00:37.720 --> 00:00:41.260]   이거는 장점 멈추려나?\n",
      "[00:00:41.260 --> 00:00:47.760]   장점 왜 젠킷을 쓰냐는 거죠?\n",
      "[00:00:47.760 --> 00:00:51.160]   우선 저희 산에 보완 정책상\n",
      "[00:00:51.160 --> 00:00:53.960]   100%를 할 때\n",
      "[00:00:53.960 --> 00:00:58.820]   지금 이 방금 말씀드린 구조가 젠키스가\n",
      "[00:00:58.820 --> 00:01:02.340]   하겟VM에다가 명령을 내리는 형태가\n",
      "[00:01:02.340 --> 00:01:06.120]   하겟VM에 저장소에서 긁어보라는 뜻이에요.\n",
      "[00:01:06.120 --> 00:01:07.960]   이걸 풀 방식이라고 하거든요.\n",
      "[00:01:07.960 --> 00:01:10.720]   우리가 아르코시디 할 때도 기돕스를 쓰면\n",
      "[00:01:10.720 --> 00:01:15.480]   기텁에 있는 100% 양식을 아르코시디가 기돕스를 긁어와서\n",
      "[00:01:15.480 --> 00:01:18.040]   ACR에 있는 이미지를 긁어와서 100%를 할 거예요.\n",
      "[00:01:18.040 --> 00:01:19.480]   이것도 풀 방식이에요.\n",
      "[00:01:19.480 --> 00:01:22.480]   KT에서는 100%를 할 때 풀 방식을 공정하고 있어요.\n",
      "[00:01:22.480 --> 00:01:23.240]   기본적으로.\n",
      "[00:01:23.240 --> 00:01:25.720]   근데 기텁 액션에서 100%를 하게 되면\n",
      "[00:01:25.720 --> 00:01:28.360]   우선 첫 번째로 기텁 엔터파이즈에서\n",
      "[00:01:28.360 --> 00:01:31.840]   저희 산에 애정 환경에 인바운드로 직접 찍을 수 없거든요.\n",
      "[00:01:31.840 --> 00:01:37.000]   애정 안에 있는 VM에다가 직접 넣게 쓸 수 있는 정의에 내린다 보니.\n",
      "[00:01:37.000 --> 00:01:38.400]   두 번째로\n",
      "[00:01:38.400 --> 00:01:41.320]   만약에 그게 가능하다 하는...\n",
      "[00:01:41.320 --> 00:01:43.200]   아니, 그게 가장 큰 일이죠.\n",
      "[00:01:43.200 --> 00:01:45.400]   첫 번째로 인바운드가 안 생기죠.\n",
      "[00:01:45.400 --> 00:01:48.560]   두 번째로는 치킨스로 따로 분리를 해뒀을 때\n",
      "[00:01:48.560 --> 00:01:52.200]   어제 이게 또 만약에 CLC를 한 번에 자동으로 해놨을 때\n",
      "[00:01:52.200 --> 00:01:54.200]   상용 100%를 하게 되었거든요.\n",
      "[00:01:54.200 --> 00:01:57.320]   검증되지 않은 어플리피해시는 100% 되면 상의가 날 수도 있어요.\n",
      "[00:01:57.320 --> 00:02:00.000]   그런 식으로 해서 이제 분리를 해둔 경우가 있어요.\n",
      "\n",
      "✅ chunk_016 처리 완료 → ./Text/01/chunk_016_ggml-small.json\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "output_json: saving output to './Text/01/chunk_016_ggml-small.json'\n",
      "\n",
      "whisper_print_timings:     load time =   185.36 ms\n",
      "whisper_print_timings:     fallbacks =   0 p /   0 h\n",
      "whisper_print_timings:      mel time =    28.19 ms\n",
      "whisper_print_timings:   sample time =   395.39 ms /  2715 runs (     0.15 ms per run)\n",
      "whisper_print_timings:   encode time =   572.92 ms /     5 runs (   114.58 ms per run)\n",
      "whisper_print_timings:   decode time =    51.04 ms /    13 runs (     3.93 ms per run)\n",
      "whisper_print_timings:   batchd time =  2411.72 ms /  2680 runs (     0.90 ms per run)\n",
      "whisper_print_timings:   prompt time =    83.38 ms /   705 runs (     0.12 ms per run)\n",
      "whisper_print_timings:    total time =  3792.17 ms\n",
      "ggml_metal_free: deallocating\n",
      "whisper_init_from_file_with_params_no_state: loading model from '/Users/aza1200/.cache/huggingface/hub/models--ggerganov--whisper.cpp/snapshots/5359861c739e955e79d9a303bcbc70fb988958b1/ggml-small.bin'\n",
      "whisper_init_with_params_no_state: use gpu    = 1\n",
      "whisper_init_with_params_no_state: flash attn = 0\n",
      "whisper_init_with_params_no_state: gpu_device = 0\n",
      "whisper_init_with_params_no_state: dtw        = 0\n",
      "whisper_init_with_params_no_state: devices    = 3\n",
      "whisper_init_with_params_no_state: backends   = 3\n",
      "whisper_model_load: loading model\n",
      "whisper_model_load: n_vocab       = 51865\n",
      "whisper_model_load: n_audio_ctx   = 1500\n",
      "whisper_model_load: n_audio_state = 768\n",
      "whisper_model_load: n_audio_head  = 12\n",
      "whisper_model_load: n_audio_layer = 12\n",
      "whisper_model_load: n_text_ctx    = 448\n",
      "whisper_model_load: n_text_state  = 768\n",
      "whisper_model_load: n_text_head   = 12\n",
      "whisper_model_load: n_text_layer  = 12\n",
      "whisper_model_load: n_mels        = 80\n",
      "whisper_model_load: ftype         = 1\n",
      "whisper_model_load: qntvr         = 0\n",
      "whisper_model_load: type          = 3 (small)\n",
      "whisper_model_load: adding 1608 extra tokens\n",
      "whisper_model_load: n_langs       = 99\n",
      "whisper_model_load:        Metal total size =   487.01 MB\n",
      "whisper_model_load: model size    =  487.01 MB\n",
      "whisper_backend_init_gpu: using Metal backend\n",
      "ggml_metal_init: allocating\n",
      "ggml_metal_init: found device: Apple M4 Pro\n",
      "ggml_metal_init: picking default device: Apple M4 Pro\n",
      "ggml_metal_load_library: using embedded metal library\n",
      "ggml_metal_init: GPU name:   Apple M4 Pro\n",
      "ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)\n",
      "ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)\n",
      "ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)\n",
      "ggml_metal_init: simdgroup reduction   = true\n",
      "ggml_metal_init: simdgroup matrix mul. = true\n",
      "ggml_metal_init: has residency sets    = true\n",
      "ggml_metal_init: has bfloat            = true\n",
      "ggml_metal_init: use bfloat            = false\n",
      "ggml_metal_init: hasUnifiedMemory      = true\n",
      "ggml_metal_init: recommendedMaxWorkingSetSize  = 38654.71 MB\n",
      "ggml_metal_init: skipping kernel_get_rows_bf16                     (not supported)\n",
      "ggml_metal_init: skipping kernel_mul_mv_bf16_f32                   (not supported)\n",
      "ggml_metal_init: skipping kernel_mul_mv_bf16_f32_1row              (not supported)\n",
      "ggml_metal_init: skipping kernel_mul_mv_bf16_f32_l4                (not supported)\n",
      "ggml_metal_init: skipping kernel_mul_mv_bf16_bf16                  (not supported)\n",
      "ggml_metal_init: skipping kernel_mul_mv_id_bf16_f32                (not supported)\n",
      "ggml_metal_init: skipping kernel_mul_mm_bf16_f32                   (not supported)\n",
      "ggml_metal_init: skipping kernel_mul_mm_id_bf16_f32                (not supported)\n",
      "ggml_metal_init: skipping kernel_flash_attn_ext_bf16_h64           (not supported)\n",
      "ggml_metal_init: skipping kernel_flash_attn_ext_bf16_h80           (not supported)\n",
      "ggml_metal_init: skipping kernel_flash_attn_ext_bf16_h96           (not supported)\n",
      "ggml_metal_init: skipping kernel_flash_attn_ext_bf16_h112          (not supported)\n",
      "ggml_metal_init: skipping kernel_flash_attn_ext_bf16_h128          (not supported)\n",
      "ggml_metal_init: skipping kernel_flash_attn_ext_bf16_h192          (not supported)\n",
      "ggml_metal_init: skipping kernel_flash_attn_ext_bf16_hk192_hv128   (not supported)\n",
      "ggml_metal_init: skipping kernel_flash_attn_ext_bf16_h256          (not supported)\n",
      "ggml_metal_init: skipping kernel_flash_attn_ext_vec_bf16_h128      (not supported)\n",
      "ggml_metal_init: skipping kernel_flash_attn_ext_vec_bf16_h192      (not supported)\n",
      "ggml_metal_init: skipping kernel_flash_attn_ext_vec_bf16_hk192_hv128 (not supported)\n",
      "ggml_metal_init: skipping kernel_flash_attn_ext_vec_bf16_h256      (not supported)\n",
      "ggml_metal_init: skipping kernel_cpy_f32_bf16                      (not supported)\n",
      "ggml_metal_init: skipping kernel_cpy_bf16_f32                      (not supported)\n",
      "ggml_metal_init: skipping kernel_cpy_bf16_bf16                     (not supported)\n",
      "whisper_backend_init: using BLAS backend\n",
      "whisper_init_state: kv self size  =   18.87 MB\n",
      "whisper_init_state: kv cross size =   56.62 MB\n",
      "whisper_init_state: kv pad  size  =    4.72 MB\n",
      "whisper_init_state: compute buffer (conv)   =   23.37 MB\n",
      "whisper_init_state: compute buffer (encode) =  128.01 MB\n",
      "whisper_init_state: compute buffer (cross)  =    6.18 MB\n",
      "whisper_init_state: compute buffer (decode) =   98.19 MB\n",
      "\n",
      "system_info: n_threads = 4 / 14 | WHISPER : COREML = 0 | OPENVINO = 0 | Metal : EMBED_LIBRARY = 1 | CPU : NEON = 1 | ARM_FMA = 1 | FP16_VA = 1 | MATMUL_INT8 = 1 | DOTPROD = 1 | SME = 1 | ACCELERATE = 1 | AARCH64_REPACK = 1 | \n",
      "\n",
      "main: processing './Sound/01/chunk_017.wav' (1919780 samples, 120.0 sec), 4 threads, 1 processors, 5 beams + best of 5, lang = ko, task = transcribe, timestamps = 1 ...\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "[00:00:00.000 --> 00:00:05.200]   아이고시리에서 보낸 건 보안으로 한 번 괜찮은 거예요.\n",
      "[00:00:05.200 --> 00:00:06.700]   아이고시리에서 보낸 건 보안으로 한 번 괜찮은 거예요.\n",
      "[00:00:06.700 --> 00:00:09.020]   아이고시리에서 보낸 건 보안으로 한 번 괜찮은 거예요.\n",
      "[00:00:09.020 --> 00:00:12.020]   아이고시리에서 보낸 건 보안으로 한 번 괜찮은 거예요.\n",
      "[00:00:12.020 --> 00:00:14.020]   아이고시리에서 보낸 건 보안으로 한 번 괜찮은 거예요.\n",
      "[00:00:14.020 --> 00:00:16.020]   아이고시리에서 보낸 건 보안으로 한 번 괜찮은 거예요.\n",
      "[00:00:16.020 --> 00:00:18.020]   아이고시리에서 보낸 건 보안으로 한 번 괜찮은 거예요.\n",
      "[00:00:18.020 --> 00:00:19.020]   아이고시리에서 보낸 건 보안으로 한 번 괜찮은 거예요.\n",
      "[00:00:19.020 --> 00:00:20.020]   아이고시리에서 보낸 건 보안으로 한 번 괜찮은 거예요.\n",
      "[00:00:20.020 --> 00:00:21.020]   아이고시리에서 보낸 건 보안으로 한 번 괜찮은 거예요.\n",
      "[00:00:21.020 --> 00:00:22.020]   아이고시리에서 보낸 건 보안으로 한 번 괜찮은 거예요.\n",
      "[00:00:30.000 --> 00:00:33.000]   액션은 실행하는 주체가\n",
      "[00:00:33.000 --> 00:00:35.000]   아이고시리에서 보낸 건 보안으로 한 번 괜찮은 거예요.\n",
      "[00:00:35.000 --> 00:00:35.980]   아이고시리에서 보낸 건 보안으로 한 번 괜찮은 거예요.\n",
      "[00:00:35.980 --> 00:00:38.000]   아이고시리에서 보낸 건 보안으로 한 번 괜찮은 거예요.\n",
      "[00:00:38.000 --> 00:00:39.000]   아이고시리에서 보낸 건 보안으로 한 번 괜찮은 거예요.\n",
      "[00:00:39.000 --> 00:00:40.000]   아이고시리에서 보낸 건 보안으로 한 번 괜찮은 거예요.\n",
      "[00:00:40.000 --> 00:00:41.000]   아이고시리에서 보낸 건 보안으로 한 번 괜찮은 거예요.\n",
      "[00:00:41.000 --> 00:00:42.000]   아이고시리에서 보낸 건 보안으로 한 번 괜찮은 거예요.\n",
      "[00:00:42.000 --> 00:00:43.000]   아이고시리에서 보낸 건 보안으로 한 번 괜찮은 거예요.\n",
      "[00:00:43.000 --> 00:00:44.000]   아이고시리에서 보낸 건 보안으로 한 번 괜찮은 거예요.\n",
      "[00:00:44.000 --> 00:00:45.000]   아이고시리에서 보낸 건 보안으로 한 번 괜찮은 거예요.\n",
      "[00:00:45.000 --> 00:01:07.000]   아이고시리에서 보낸 건 보안으로 한 번 괜찮은 거예요.\n",
      "[00:01:07.000 --> 00:01:09.000]   아이고시리에서 보낸 건 보안으로 한 번 괜찮은 거예요.\n",
      "[00:01:09.000 --> 00:01:11.000]   아이고시리에서 보낸 건 보안으로 한 번 괜찮은 거예요.\n",
      "[00:01:11.000 --> 00:01:12.000]   아이고시리에서 보낸 건 보안으로 한 번 괜찮은 거예요.\n",
      "[00:01:12.000 --> 00:01:13.000]   아이고시리에서 보낸 건 보안으로 한 번 괜찮은 거예요.\n",
      "[00:01:13.000 --> 00:01:14.000]   아이고시리에서 보낸 건 보안으로 한 번 괜찮은 거예요.\n",
      "[00:01:14.000 --> 00:01:44.000]   사과, 사과, 사과, 사과, 사과, 사과, 사과, 사과, 사과, 사과, 사과, 사과, 사과, 사과, 사과, 사과, 사과, 사과, 사과, 사과, 사과, 사과, 사과, 사과, 사과, 사과, 사과, 사과, 사과, 사과, 사과, 사과, 사과, 사과, 사과, 사과, 사과, 사과, 사과, 사과, 사과, 사과, 사과, 사과, 사과, 사과, 사과, 사과, 사과, 사과, 사과, 사과, 사과, 사과, 사과, 사과, 사과, 사과, 사과, 사과, 사과, 사과, 사과, 사과, 사과, 사과, 사과, 사과, 사과, 사과, 사과, 사과, 사과,\n",
      "[00:01:44.000 --> 00:02:00.000]   (끝나는 중)\n",
      "\n",
      "✅ chunk_017 처리 완료 → ./Text/01/chunk_017_ggml-small.json\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "output_json: saving output to './Text/01/chunk_017_ggml-small.json'\n",
      "\n",
      "whisper_print_timings:     load time =   183.94 ms\n",
      "whisper_print_timings:     fallbacks =   6 p /   7 h\n",
      "whisper_print_timings:      mel time =    28.36 ms\n",
      "whisper_print_timings:   sample time =  1580.50 ms / 10465 runs (     0.15 ms per run)\n",
      "whisper_print_timings:   encode time =   571.92 ms /     5 runs (   114.38 ms per run)\n",
      "whisper_print_timings:   decode time =   414.43 ms /   105 runs (     3.95 ms per run)\n",
      "whisper_print_timings:   batchd time =  9868.15 ms / 10331 runs (     0.96 ms per run)\n",
      "whisper_print_timings:   prompt time =   126.00 ms /  1136 runs (     0.11 ms per run)\n",
      "whisper_print_timings:    total time = 12856.39 ms\n",
      "ggml_metal_free: deallocating\n",
      "whisper_init_from_file_with_params_no_state: loading model from '/Users/aza1200/.cache/huggingface/hub/models--ggerganov--whisper.cpp/snapshots/5359861c739e955e79d9a303bcbc70fb988958b1/ggml-small.bin'\n",
      "whisper_init_with_params_no_state: use gpu    = 1\n",
      "whisper_init_with_params_no_state: flash attn = 0\n",
      "whisper_init_with_params_no_state: gpu_device = 0\n",
      "whisper_init_with_params_no_state: dtw        = 0\n",
      "whisper_init_with_params_no_state: devices    = 3\n",
      "whisper_init_with_params_no_state: backends   = 3\n",
      "whisper_model_load: loading model\n",
      "whisper_model_load: n_vocab       = 51865\n",
      "whisper_model_load: n_audio_ctx   = 1500\n",
      "whisper_model_load: n_audio_state = 768\n",
      "whisper_model_load: n_audio_head  = 12\n",
      "whisper_model_load: n_audio_layer = 12\n",
      "whisper_model_load: n_text_ctx    = 448\n",
      "whisper_model_load: n_text_state  = 768\n",
      "whisper_model_load: n_text_head   = 12\n",
      "whisper_model_load: n_text_layer  = 12\n",
      "whisper_model_load: n_mels        = 80\n",
      "whisper_model_load: ftype         = 1\n",
      "whisper_model_load: qntvr         = 0\n",
      "whisper_model_load: type          = 3 (small)\n",
      "whisper_model_load: adding 1608 extra tokens\n",
      "whisper_model_load: n_langs       = 99\n",
      "whisper_model_load:        Metal total size =   487.01 MB\n",
      "whisper_model_load: model size    =  487.01 MB\n",
      "whisper_backend_init_gpu: using Metal backend\n",
      "ggml_metal_init: allocating\n",
      "ggml_metal_init: found device: Apple M4 Pro\n",
      "ggml_metal_init: picking default device: Apple M4 Pro\n",
      "ggml_metal_load_library: using embedded metal library\n",
      "ggml_metal_init: GPU name:   Apple M4 Pro\n",
      "ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)\n",
      "ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)\n",
      "ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)\n",
      "ggml_metal_init: simdgroup reduction   = true\n",
      "ggml_metal_init: simdgroup matrix mul. = true\n",
      "ggml_metal_init: has residency sets    = true\n",
      "ggml_metal_init: has bfloat            = true\n",
      "ggml_metal_init: use bfloat            = false\n",
      "ggml_metal_init: hasUnifiedMemory      = true\n",
      "ggml_metal_init: recommendedMaxWorkingSetSize  = 38654.71 MB\n",
      "ggml_metal_init: skipping kernel_get_rows_bf16                     (not supported)\n",
      "ggml_metal_init: skipping kernel_mul_mv_bf16_f32                   (not supported)\n",
      "ggml_metal_init: skipping kernel_mul_mv_bf16_f32_1row              (not supported)\n",
      "ggml_metal_init: skipping kernel_mul_mv_bf16_f32_l4                (not supported)\n",
      "ggml_metal_init: skipping kernel_mul_mv_bf16_bf16                  (not supported)\n",
      "ggml_metal_init: skipping kernel_mul_mv_id_bf16_f32                (not supported)\n",
      "ggml_metal_init: skipping kernel_mul_mm_bf16_f32                   (not supported)\n",
      "ggml_metal_init: skipping kernel_mul_mm_id_bf16_f32                (not supported)\n",
      "ggml_metal_init: skipping kernel_flash_attn_ext_bf16_h64           (not supported)\n",
      "ggml_metal_init: skipping kernel_flash_attn_ext_bf16_h80           (not supported)\n",
      "ggml_metal_init: skipping kernel_flash_attn_ext_bf16_h96           (not supported)\n",
      "ggml_metal_init: skipping kernel_flash_attn_ext_bf16_h112          (not supported)\n",
      "ggml_metal_init: skipping kernel_flash_attn_ext_bf16_h128          (not supported)\n",
      "ggml_metal_init: skipping kernel_flash_attn_ext_bf16_h192          (not supported)\n",
      "ggml_metal_init: skipping kernel_flash_attn_ext_bf16_hk192_hv128   (not supported)\n",
      "ggml_metal_init: skipping kernel_flash_attn_ext_bf16_h256          (not supported)\n",
      "ggml_metal_init: skipping kernel_flash_attn_ext_vec_bf16_h128      (not supported)\n",
      "ggml_metal_init: skipping kernel_flash_attn_ext_vec_bf16_h192      (not supported)\n",
      "ggml_metal_init: skipping kernel_flash_attn_ext_vec_bf16_hk192_hv128 (not supported)\n",
      "ggml_metal_init: skipping kernel_flash_attn_ext_vec_bf16_h256      (not supported)\n",
      "ggml_metal_init: skipping kernel_cpy_f32_bf16                      (not supported)\n",
      "ggml_metal_init: skipping kernel_cpy_bf16_f32                      (not supported)\n",
      "ggml_metal_init: skipping kernel_cpy_bf16_bf16                     (not supported)\n",
      "whisper_backend_init: using BLAS backend\n",
      "whisper_init_state: kv self size  =   18.87 MB\n",
      "whisper_init_state: kv cross size =   56.62 MB\n",
      "whisper_init_state: kv pad  size  =    4.72 MB\n",
      "whisper_init_state: compute buffer (conv)   =   23.37 MB\n",
      "whisper_init_state: compute buffer (encode) =  128.01 MB\n",
      "whisper_init_state: compute buffer (cross)  =    6.18 MB\n",
      "whisper_init_state: compute buffer (decode) =   98.19 MB\n",
      "\n",
      "system_info: n_threads = 4 / 14 | WHISPER : COREML = 0 | OPENVINO = 0 | Metal : EMBED_LIBRARY = 1 | CPU : NEON = 1 | ARM_FMA = 1 | FP16_VA = 1 | MATMUL_INT8 = 1 | DOTPROD = 1 | SME = 1 | ACCELERATE = 1 | AARCH64_REPACK = 1 | \n",
      "\n",
      "main: processing './Sound/01/chunk_018.wav' (1919902 samples, 120.0 sec), 4 threads, 1 processors, 5 beams + best of 5, lang = ko, task = transcribe, timestamps = 1 ...\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "[00:00:00.000 --> 00:00:02.500]   저희 3번 비쥬얼이고요, 3번 갈 거예요.\n",
      "[00:00:02.500 --> 00:00:04.380]   3번 비쥬얼이고요, 3번 비쥬얼이고요.\n",
      "[00:00:04.380 --> 00:00:06.160]   - 3번 비쥬얼이고요? - 네, 3번 비쥬얼이고요.\n",
      "[00:00:06.160 --> 00:00:07.540]   - 3번 비쥬얼이고요? - 네, 3번 비쥬얼이고요.\n",
      "[00:00:07.540 --> 00:00:09.000]   - 3번 비쥬얼이고요? - 4번 비쥬얼이고요.\n",
      "[00:00:09.000 --> 00:00:11.740]   - 1번 비쥬얼이고요. - 아, 4번 비쥬얼이고요.\n",
      "[00:00:11.740 --> 00:00:13.540]   - 4번 비쥬얼이고요? - 네, 4번 비쥬얼이고요.\n",
      "[00:00:13.540 --> 00:00:14.740]   - 1번 비쥬얼이고요? - 네, 4번 비쥬얼이고요.\n",
      "[00:00:14.740 --> 00:00:15.980]   - 1번 비쥬얼이고요? - 네, 4번 비쥬얼이고요.\n",
      "[00:00:30.000 --> 00:00:32.000]   아 진짜요?\n",
      "[00:01:00.000 --> 00:01:04.000]  .\n",
      "[00:01:04.000 --> 00:01:06.000]   세면.\n",
      "[00:01:06.000 --> 00:01:08.000]   엔지.\n",
      "[00:01:08.000 --> 00:01:10.000]   등등에 있는 거 와서.\n",
      "[00:01:10.000 --> 00:01:12.000]   아니 엔지.\n",
      "[00:01:12.000 --> 00:01:14.000]   성으로 그냥 제형을 이런 줄 알고.\n",
      "[00:01:14.000 --> 00:01:16.000]   성장만 해도 되고요.\n",
      "[00:01:16.000 --> 00:01:18.000]   저 오피드 로컴퓨터 시각도.\n",
      "[00:01:18.000 --> 00:01:20.000]   이제 센스 시키기.\n",
      "[00:01:20.000 --> 00:01:22.000]   행복한 거 아니였어요.\n",
      "[00:01:22.000 --> 00:01:24.000]   그쵸?\n",
      "[00:01:24.000 --> 00:01:26.000]   세면.\n",
      "[00:01:26.000 --> 00:01:30.000]   다음날 지정봉 씨 나오니까.\n",
      "[00:01:30.000 --> 00:01:32.000]   다치는 줄 알고.\n",
      "[00:01:32.000 --> 00:01:34.000]   세면.\n",
      "[00:01:34.000 --> 00:01:36.000]   여러분들 사는 피치 있으세요?\n",
      "[00:01:36.000 --> 00:01:38.000]   엔지크렘.\n",
      "[00:01:38.000 --> 00:01:40.000]   다 있으세요?\n",
      "[00:01:40.000 --> 00:01:42.000]   아 그래요?\n",
      "[00:01:42.000 --> 00:01:44.000]   그러면은 그게 좀 필요해가지고.\n",
      "[00:01:44.000 --> 00:01:46.000]   갖고 올까요?\n",
      "[00:01:46.000 --> 00:01:48.000]   네.\n",
      "[00:01:56.000 --> 00:01:57.280]   -끝나요. -끝나요.\n",
      "\n",
      "✅ chunk_018 처리 완료 → ./Text/01/chunk_018_ggml-small.json\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "output_json: saving output to './Text/01/chunk_018_ggml-small.json'\n",
      "\n",
      "whisper_print_timings:     load time =   181.55 ms\n",
      "whisper_print_timings:     fallbacks =  11 p /   0 h\n",
      "whisper_print_timings:      mel time =    28.22 ms\n",
      "whisper_print_timings:   sample time =  1904.12 ms / 12970 runs (     0.15 ms per run)\n",
      "whisper_print_timings:   encode time =   574.34 ms /     5 runs (   114.87 ms per run)\n",
      "whisper_print_timings:   decode time =   412.21 ms /   112 runs (     3.68 ms per run)\n",
      "whisper_print_timings:   batchd time = 12764.38 ms / 12833 runs (     0.99 ms per run)\n",
      "whisper_print_timings:   prompt time =    84.09 ms /   781 runs (     0.11 ms per run)\n",
      "whisper_print_timings:    total time = 16025.38 ms\n",
      "ggml_metal_free: deallocating\n",
      "whisper_init_from_file_with_params_no_state: loading model from '/Users/aza1200/.cache/huggingface/hub/models--ggerganov--whisper.cpp/snapshots/5359861c739e955e79d9a303bcbc70fb988958b1/ggml-small.bin'\n",
      "whisper_init_with_params_no_state: use gpu    = 1\n",
      "whisper_init_with_params_no_state: flash attn = 0\n",
      "whisper_init_with_params_no_state: gpu_device = 0\n",
      "whisper_init_with_params_no_state: dtw        = 0\n",
      "whisper_init_with_params_no_state: devices    = 3\n",
      "whisper_init_with_params_no_state: backends   = 3\n",
      "whisper_model_load: loading model\n",
      "whisper_model_load: n_vocab       = 51865\n",
      "whisper_model_load: n_audio_ctx   = 1500\n",
      "whisper_model_load: n_audio_state = 768\n",
      "whisper_model_load: n_audio_head  = 12\n",
      "whisper_model_load: n_audio_layer = 12\n",
      "whisper_model_load: n_text_ctx    = 448\n",
      "whisper_model_load: n_text_state  = 768\n",
      "whisper_model_load: n_text_head   = 12\n",
      "whisper_model_load: n_text_layer  = 12\n",
      "whisper_model_load: n_mels        = 80\n",
      "whisper_model_load: ftype         = 1\n",
      "whisper_model_load: qntvr         = 0\n",
      "whisper_model_load: type          = 3 (small)\n",
      "whisper_model_load: adding 1608 extra tokens\n",
      "whisper_model_load: n_langs       = 99\n",
      "whisper_model_load:        Metal total size =   487.01 MB\n",
      "whisper_model_load: model size    =  487.01 MB\n",
      "whisper_backend_init_gpu: using Metal backend\n",
      "ggml_metal_init: allocating\n",
      "ggml_metal_init: found device: Apple M4 Pro\n",
      "ggml_metal_init: picking default device: Apple M4 Pro\n",
      "ggml_metal_load_library: using embedded metal library\n",
      "ggml_metal_init: GPU name:   Apple M4 Pro\n",
      "ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)\n",
      "ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)\n",
      "ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)\n",
      "ggml_metal_init: simdgroup reduction   = true\n",
      "ggml_metal_init: simdgroup matrix mul. = true\n",
      "ggml_metal_init: has residency sets    = true\n",
      "ggml_metal_init: has bfloat            = true\n",
      "ggml_metal_init: use bfloat            = false\n",
      "ggml_metal_init: hasUnifiedMemory      = true\n",
      "ggml_metal_init: recommendedMaxWorkingSetSize  = 38654.71 MB\n",
      "ggml_metal_init: skipping kernel_get_rows_bf16                     (not supported)\n",
      "ggml_metal_init: skipping kernel_mul_mv_bf16_f32                   (not supported)\n",
      "ggml_metal_init: skipping kernel_mul_mv_bf16_f32_1row              (not supported)\n",
      "ggml_metal_init: skipping kernel_mul_mv_bf16_f32_l4                (not supported)\n",
      "ggml_metal_init: skipping kernel_mul_mv_bf16_bf16                  (not supported)\n",
      "ggml_metal_init: skipping kernel_mul_mv_id_bf16_f32                (not supported)\n",
      "ggml_metal_init: skipping kernel_mul_mm_bf16_f32                   (not supported)\n",
      "ggml_metal_init: skipping kernel_mul_mm_id_bf16_f32                (not supported)\n",
      "ggml_metal_init: skipping kernel_flash_attn_ext_bf16_h64           (not supported)\n",
      "ggml_metal_init: skipping kernel_flash_attn_ext_bf16_h80           (not supported)\n",
      "ggml_metal_init: skipping kernel_flash_attn_ext_bf16_h96           (not supported)\n",
      "ggml_metal_init: skipping kernel_flash_attn_ext_bf16_h112          (not supported)\n",
      "ggml_metal_init: skipping kernel_flash_attn_ext_bf16_h128          (not supported)\n",
      "ggml_metal_init: skipping kernel_flash_attn_ext_bf16_h192          (not supported)\n",
      "ggml_metal_init: skipping kernel_flash_attn_ext_bf16_hk192_hv128   (not supported)\n",
      "ggml_metal_init: skipping kernel_flash_attn_ext_bf16_h256          (not supported)\n",
      "ggml_metal_init: skipping kernel_flash_attn_ext_vec_bf16_h128      (not supported)\n",
      "ggml_metal_init: skipping kernel_flash_attn_ext_vec_bf16_h192      (not supported)\n",
      "ggml_metal_init: skipping kernel_flash_attn_ext_vec_bf16_hk192_hv128 (not supported)\n",
      "ggml_metal_init: skipping kernel_flash_attn_ext_vec_bf16_h256      (not supported)\n",
      "ggml_metal_init: skipping kernel_cpy_f32_bf16                      (not supported)\n",
      "ggml_metal_init: skipping kernel_cpy_bf16_f32                      (not supported)\n",
      "ggml_metal_init: skipping kernel_cpy_bf16_bf16                     (not supported)\n",
      "whisper_backend_init: using BLAS backend\n",
      "whisper_init_state: kv self size  =   18.87 MB\n",
      "whisper_init_state: kv cross size =   56.62 MB\n",
      "whisper_init_state: kv pad  size  =    4.72 MB\n",
      "whisper_init_state: compute buffer (conv)   =   23.37 MB\n",
      "whisper_init_state: compute buffer (encode) =  128.01 MB\n",
      "whisper_init_state: compute buffer (cross)  =    6.18 MB\n",
      "whisper_init_state: compute buffer (decode) =   98.19 MB\n",
      "\n",
      "system_info: n_threads = 4 / 14 | WHISPER : COREML = 0 | OPENVINO = 0 | Metal : EMBED_LIBRARY = 1 | CPU : NEON = 1 | ARM_FMA = 1 | FP16_VA = 1 | MATMUL_INT8 = 1 | DOTPROD = 1 | SME = 1 | ACCELERATE = 1 | AARCH64_REPACK = 1 | \n",
      "\n",
      "main: processing './Sound/01/chunk_019.wav' (1920424 samples, 120.0 sec), 4 threads, 1 processors, 5 beams + best of 5, lang = ko, task = transcribe, timestamps = 1 ...\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "[00:00:00.000 --> 00:00:10.000]   [두 번째 요리는 '두 번째 요리'입니다]\n",
      "[00:00:30.000 --> 00:00:40.000]   [두 번째 요리는 '두 번째 요리'입니다]\n",
      "[00:00:40.000 --> 00:00:50.000]   [두 번째 요리는 '두 번째 요리'입니다]\n",
      "[00:00:50.000 --> 00:00:58.000]   [두 번째 요리는 '두 번째 요리'입니다]\n",
      "[00:00:58.000 --> 00:01:08.000]   [두 번째 요리는 '두 번째 요리'입니다]\n",
      "[00:01:08.000 --> 00:01:32.000]   [두 번째 요리는 '두 번째 요리'입니다]\n",
      "[00:01:32.000 --> 00:01:42.000]   [두 번째 요리는 '두 번째 요리'입니다]\n",
      "[00:01:42.000 --> 00:01:52.000]   [두 번째 요리는 '두 번째 요리'입니다]\n",
      "[00:01:52.000 --> 00:02:02.000]   [두 번째 요리는 '두 번째 요리'입니다]\n",
      "\n",
      "✅ chunk_019 처리 완료 → ./Text/01/chunk_019_ggml-small.json\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "output_json: saving output to './Text/01/chunk_019_ggml-small.json'\n",
      "\n",
      "whisper_print_timings:     load time =   182.51 ms\n",
      "whisper_print_timings:     fallbacks =   1 p /   3 h\n",
      "whisper_print_timings:      mel time =    28.21 ms\n",
      "whisper_print_timings:   sample time =   214.10 ms /  1256 runs (     0.17 ms per run)\n",
      "whisper_print_timings:   encode time =   687.59 ms /     6 runs (   114.60 ms per run)\n",
      "whisper_print_timings:   decode time =   239.71 ms /    65 runs (     3.69 ms per run)\n",
      "whisper_print_timings:   batchd time =  1584.27 ms /  1162 runs (     1.36 ms per run)\n",
      "whisper_print_timings:   prompt time =    64.34 ms /   462 runs (     0.14 ms per run)\n",
      "whisper_print_timings:    total time =  3058.80 ms\n",
      "ggml_metal_free: deallocating\n",
      "whisper_init_from_file_with_params_no_state: loading model from '/Users/aza1200/.cache/huggingface/hub/models--ggerganov--whisper.cpp/snapshots/5359861c739e955e79d9a303bcbc70fb988958b1/ggml-small.bin'\n",
      "whisper_init_with_params_no_state: use gpu    = 1\n",
      "whisper_init_with_params_no_state: flash attn = 0\n",
      "whisper_init_with_params_no_state: gpu_device = 0\n",
      "whisper_init_with_params_no_state: dtw        = 0\n",
      "whisper_init_with_params_no_state: devices    = 3\n",
      "whisper_init_with_params_no_state: backends   = 3\n",
      "whisper_model_load: loading model\n",
      "whisper_model_load: n_vocab       = 51865\n",
      "whisper_model_load: n_audio_ctx   = 1500\n",
      "whisper_model_load: n_audio_state = 768\n",
      "whisper_model_load: n_audio_head  = 12\n",
      "whisper_model_load: n_audio_layer = 12\n",
      "whisper_model_load: n_text_ctx    = 448\n",
      "whisper_model_load: n_text_state  = 768\n",
      "whisper_model_load: n_text_head   = 12\n",
      "whisper_model_load: n_text_layer  = 12\n",
      "whisper_model_load: n_mels        = 80\n",
      "whisper_model_load: ftype         = 1\n",
      "whisper_model_load: qntvr         = 0\n",
      "whisper_model_load: type          = 3 (small)\n",
      "whisper_model_load: adding 1608 extra tokens\n",
      "whisper_model_load: n_langs       = 99\n",
      "whisper_model_load:        Metal total size =   487.01 MB\n",
      "whisper_model_load: model size    =  487.01 MB\n",
      "whisper_backend_init_gpu: using Metal backend\n",
      "ggml_metal_init: allocating\n",
      "ggml_metal_init: found device: Apple M4 Pro\n",
      "ggml_metal_init: picking default device: Apple M4 Pro\n",
      "ggml_metal_load_library: using embedded metal library\n",
      "ggml_metal_init: GPU name:   Apple M4 Pro\n",
      "ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)\n",
      "ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)\n",
      "ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)\n",
      "ggml_metal_init: simdgroup reduction   = true\n",
      "ggml_metal_init: simdgroup matrix mul. = true\n",
      "ggml_metal_init: has residency sets    = true\n",
      "ggml_metal_init: has bfloat            = true\n",
      "ggml_metal_init: use bfloat            = false\n",
      "ggml_metal_init: hasUnifiedMemory      = true\n",
      "ggml_metal_init: recommendedMaxWorkingSetSize  = 38654.71 MB\n",
      "ggml_metal_init: skipping kernel_get_rows_bf16                     (not supported)\n",
      "ggml_metal_init: skipping kernel_mul_mv_bf16_f32                   (not supported)\n",
      "ggml_metal_init: skipping kernel_mul_mv_bf16_f32_1row              (not supported)\n",
      "ggml_metal_init: skipping kernel_mul_mv_bf16_f32_l4                (not supported)\n",
      "ggml_metal_init: skipping kernel_mul_mv_bf16_bf16                  (not supported)\n",
      "ggml_metal_init: skipping kernel_mul_mv_id_bf16_f32                (not supported)\n",
      "ggml_metal_init: skipping kernel_mul_mm_bf16_f32                   (not supported)\n",
      "ggml_metal_init: skipping kernel_mul_mm_id_bf16_f32                (not supported)\n",
      "ggml_metal_init: skipping kernel_flash_attn_ext_bf16_h64           (not supported)\n",
      "ggml_metal_init: skipping kernel_flash_attn_ext_bf16_h80           (not supported)\n",
      "ggml_metal_init: skipping kernel_flash_attn_ext_bf16_h96           (not supported)\n",
      "ggml_metal_init: skipping kernel_flash_attn_ext_bf16_h112          (not supported)\n",
      "ggml_metal_init: skipping kernel_flash_attn_ext_bf16_h128          (not supported)\n",
      "ggml_metal_init: skipping kernel_flash_attn_ext_bf16_h192          (not supported)\n",
      "ggml_metal_init: skipping kernel_flash_attn_ext_bf16_hk192_hv128   (not supported)\n",
      "ggml_metal_init: skipping kernel_flash_attn_ext_bf16_h256          (not supported)\n",
      "ggml_metal_init: skipping kernel_flash_attn_ext_vec_bf16_h128      (not supported)\n",
      "ggml_metal_init: skipping kernel_flash_attn_ext_vec_bf16_h192      (not supported)\n",
      "ggml_metal_init: skipping kernel_flash_attn_ext_vec_bf16_hk192_hv128 (not supported)\n",
      "ggml_metal_init: skipping kernel_flash_attn_ext_vec_bf16_h256      (not supported)\n",
      "ggml_metal_init: skipping kernel_cpy_f32_bf16                      (not supported)\n",
      "ggml_metal_init: skipping kernel_cpy_bf16_f32                      (not supported)\n",
      "ggml_metal_init: skipping kernel_cpy_bf16_bf16                     (not supported)\n",
      "whisper_backend_init: using BLAS backend\n",
      "whisper_init_state: kv self size  =   18.87 MB\n",
      "whisper_init_state: kv cross size =   56.62 MB\n",
      "whisper_init_state: kv pad  size  =    4.72 MB\n",
      "whisper_init_state: compute buffer (conv)   =   23.37 MB\n",
      "whisper_init_state: compute buffer (encode) =  128.01 MB\n",
      "whisper_init_state: compute buffer (cross)  =    6.18 MB\n",
      "whisper_init_state: compute buffer (decode) =   98.19 MB\n",
      "\n",
      "system_info: n_threads = 4 / 14 | WHISPER : COREML = 0 | OPENVINO = 0 | Metal : EMBED_LIBRARY = 1 | CPU : NEON = 1 | ARM_FMA = 1 | FP16_VA = 1 | MATMUL_INT8 = 1 | DOTPROD = 1 | SME = 1 | ACCELERATE = 1 | AARCH64_REPACK = 1 | \n",
      "\n",
      "main: processing './Sound/01/chunk_020.wav' (1919441 samples, 120.0 sec), 4 threads, 1 processors, 5 beams + best of 5, lang = ko, task = transcribe, timestamps = 1 ...\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "[00:00:00.000 --> 00:00:02.240]   - 10밖에 안 됐어요 - 근데 그거 90T만?\n",
      "[00:00:02.240 --> 00:00:04.440]   - 네, 진짜? - 네 - 망할 수 있는 거 아니야?\n",
      "[00:00:04.440 --> 00:00:07.800]   그게 또래주로 진경서 망할 수 있는 게 있는데\n",
      "[00:00:07.800 --> 00:00:10.740]   - 강화할 수 있는 거 아니야? - 네\n",
      "[00:00:10.740 --> 00:00:12.200]   - 또래주로 망할 수 있는 거 아니야? - 네\n",
      "[00:00:12.200 --> 00:00:14.420]   - 할 수 있는 거 아니야? - 문제는...\n",
      "[00:00:14.420 --> 00:00:16.500]   - 근데 우리 브래프 안 쓰니까 - 우리 브래프 안 쓰니까\n",
      "[00:00:16.500 --> 00:00:18.480]   - 다른 브래프 안 쓰니까 - 다른 브래프 안 쓰니까\n",
      "[00:00:18.480 --> 00:00:20.800]   애도 파괴되면 이런 거지\n",
      "[00:00:20.800 --> 00:00:23.420]   이게 많아, 특별히 취면은\n",
      "[00:00:23.420 --> 00:00:25.680]   - 없다고 들었는데 - 너무 아파요, 잠깐\n",
      "[00:00:25.680 --> 00:00:28.460]   - 보이긴 했는데 - 네, 근데 이거\n",
      "[00:00:28.460 --> 00:00:30.960]   저 이즈 파크 이런 게 맞지 않았어요?\n",
      "[00:00:30.960 --> 00:00:32.560]   - 네, 가자 - 네, 가자\n",
      "[00:00:32.560 --> 00:00:35.720]   - 너무 비싸구나 - 땅 빗대 얼마인데요?\n",
      "[00:00:35.720 --> 00:00:38.160]   - 근데 생각해보니까 - 응\n",
      "[00:00:38.160 --> 00:00:40.920]   가사는 3개 먹으면 미리...\n",
      "[00:00:40.920 --> 00:00:44.120]   - 얼마예요? - 1, 2, 3, 4, 5, 6...\n",
      "[00:00:44.120 --> 00:00:47.560]   그렇지, 서기랑 놀기 있던 거 아닌데, 그거 나와, 그렇죠.\n",
      "[00:00:47.560 --> 00:00:47.560]   - 석현이 형이? - 석현이 형이 형이 형이 형이 형이 형이 형이 형이 형이 형이 형이 형이 형이 형이 형이 형이 형이 형이 형이 형이 형이 형이 형이 형이 형이 형이 형이 형이 형이 형이 형이 형이 형이 형이 형이 형이 형이 형이 형이 형이 형이 형이 형이 형이 형이 형이 형이 형이 형이 형이 형이 형이 형이 형이 형이 형이 형이 형이 형이 형이 형이 형이 형이 형이 형이 형이 형이 형이 형이 형이 형이 형이 형이 형이 형이 형이 형이 형이 형이 형이 형이 형이 형이 형이 형이 형이 형이 형이 형이 형이 형이 형이 형\n",
      "[00:00:47.560 --> 00:01:14.760]   (놀기 있던)\n",
      "[00:01:17.560 --> 00:01:47.080]   (놀기 있던)\n",
      "[00:01:47.560 --> 00:01:59.880]   (놀기 있던)\n",
      "\n",
      "✅ chunk_020 처리 완료 → ./Text/01/chunk_020_ggml-small.json\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "output_json: saving output to './Text/01/chunk_020_ggml-small.json'\n",
      "\n",
      "whisper_print_timings:     load time =   180.48 ms\n",
      "whisper_print_timings:     fallbacks =  12 p /   1 h\n",
      "whisper_print_timings:      mel time =    28.20 ms\n",
      "whisper_print_timings:   sample time =  2044.14 ms / 13613 runs (     0.15 ms per run)\n",
      "whisper_print_timings:   encode time =   688.42 ms /     6 runs (   114.74 ms per run)\n",
      "whisper_print_timings:   decode time =   773.82 ms /   209 runs (     3.70 ms per run)\n",
      "whisper_print_timings:   batchd time = 12344.86 ms / 13346 runs (     0.92 ms per run)\n",
      "whisper_print_timings:   prompt time =   140.87 ms /  1038 runs (     0.14 ms per run)\n",
      "whisper_print_timings:    total time = 16285.24 ms\n",
      "ggml_metal_free: deallocating\n",
      "whisper_init_from_file_with_params_no_state: loading model from '/Users/aza1200/.cache/huggingface/hub/models--ggerganov--whisper.cpp/snapshots/5359861c739e955e79d9a303bcbc70fb988958b1/ggml-small.bin'\n",
      "whisper_init_with_params_no_state: use gpu    = 1\n",
      "whisper_init_with_params_no_state: flash attn = 0\n",
      "whisper_init_with_params_no_state: gpu_device = 0\n",
      "whisper_init_with_params_no_state: dtw        = 0\n",
      "whisper_init_with_params_no_state: devices    = 3\n",
      "whisper_init_with_params_no_state: backends   = 3\n",
      "whisper_model_load: loading model\n",
      "whisper_model_load: n_vocab       = 51865\n",
      "whisper_model_load: n_audio_ctx   = 1500\n",
      "whisper_model_load: n_audio_state = 768\n",
      "whisper_model_load: n_audio_head  = 12\n",
      "whisper_model_load: n_audio_layer = 12\n",
      "whisper_model_load: n_text_ctx    = 448\n",
      "whisper_model_load: n_text_state  = 768\n",
      "whisper_model_load: n_text_head   = 12\n",
      "whisper_model_load: n_text_layer  = 12\n",
      "whisper_model_load: n_mels        = 80\n",
      "whisper_model_load: ftype         = 1\n",
      "whisper_model_load: qntvr         = 0\n",
      "whisper_model_load: type          = 3 (small)\n",
      "whisper_model_load: adding 1608 extra tokens\n",
      "whisper_model_load: n_langs       = 99\n",
      "whisper_model_load:        Metal total size =   487.01 MB\n",
      "whisper_model_load: model size    =  487.01 MB\n",
      "whisper_backend_init_gpu: using Metal backend\n",
      "ggml_metal_init: allocating\n",
      "ggml_metal_init: found device: Apple M4 Pro\n",
      "ggml_metal_init: picking default device: Apple M4 Pro\n",
      "ggml_metal_load_library: using embedded metal library\n",
      "ggml_metal_init: GPU name:   Apple M4 Pro\n",
      "ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)\n",
      "ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)\n",
      "ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)\n",
      "ggml_metal_init: simdgroup reduction   = true\n",
      "ggml_metal_init: simdgroup matrix mul. = true\n",
      "ggml_metal_init: has residency sets    = true\n",
      "ggml_metal_init: has bfloat            = true\n",
      "ggml_metal_init: use bfloat            = false\n",
      "ggml_metal_init: hasUnifiedMemory      = true\n",
      "ggml_metal_init: recommendedMaxWorkingSetSize  = 38654.71 MB\n",
      "ggml_metal_init: skipping kernel_get_rows_bf16                     (not supported)\n",
      "ggml_metal_init: skipping kernel_mul_mv_bf16_f32                   (not supported)\n",
      "ggml_metal_init: skipping kernel_mul_mv_bf16_f32_1row              (not supported)\n",
      "ggml_metal_init: skipping kernel_mul_mv_bf16_f32_l4                (not supported)\n",
      "ggml_metal_init: skipping kernel_mul_mv_bf16_bf16                  (not supported)\n",
      "ggml_metal_init: skipping kernel_mul_mv_id_bf16_f32                (not supported)\n",
      "ggml_metal_init: skipping kernel_mul_mm_bf16_f32                   (not supported)\n",
      "ggml_metal_init: skipping kernel_mul_mm_id_bf16_f32                (not supported)\n",
      "ggml_metal_init: skipping kernel_flash_attn_ext_bf16_h64           (not supported)\n",
      "ggml_metal_init: skipping kernel_flash_attn_ext_bf16_h80           (not supported)\n",
      "ggml_metal_init: skipping kernel_flash_attn_ext_bf16_h96           (not supported)\n",
      "ggml_metal_init: skipping kernel_flash_attn_ext_bf16_h112          (not supported)\n",
      "ggml_metal_init: skipping kernel_flash_attn_ext_bf16_h128          (not supported)\n",
      "ggml_metal_init: skipping kernel_flash_attn_ext_bf16_h192          (not supported)\n",
      "ggml_metal_init: skipping kernel_flash_attn_ext_bf16_hk192_hv128   (not supported)\n",
      "ggml_metal_init: skipping kernel_flash_attn_ext_bf16_h256          (not supported)\n",
      "ggml_metal_init: skipping kernel_flash_attn_ext_vec_bf16_h128      (not supported)\n",
      "ggml_metal_init: skipping kernel_flash_attn_ext_vec_bf16_h192      (not supported)\n",
      "ggml_metal_init: skipping kernel_flash_attn_ext_vec_bf16_hk192_hv128 (not supported)\n",
      "ggml_metal_init: skipping kernel_flash_attn_ext_vec_bf16_h256      (not supported)\n",
      "ggml_metal_init: skipping kernel_cpy_f32_bf16                      (not supported)\n",
      "ggml_metal_init: skipping kernel_cpy_bf16_f32                      (not supported)\n",
      "ggml_metal_init: skipping kernel_cpy_bf16_bf16                     (not supported)\n",
      "whisper_backend_init: using BLAS backend\n",
      "whisper_init_state: kv self size  =   18.87 MB\n",
      "whisper_init_state: kv cross size =   56.62 MB\n",
      "whisper_init_state: kv pad  size  =    4.72 MB\n",
      "whisper_init_state: compute buffer (conv)   =   23.37 MB\n",
      "whisper_init_state: compute buffer (encode) =  128.01 MB\n",
      "whisper_init_state: compute buffer (cross)  =    6.18 MB\n",
      "whisper_init_state: compute buffer (decode) =   98.19 MB\n",
      "\n",
      "system_info: n_threads = 4 / 14 | WHISPER : COREML = 0 | OPENVINO = 0 | Metal : EMBED_LIBRARY = 1 | CPU : NEON = 1 | ARM_FMA = 1 | FP16_VA = 1 | MATMUL_INT8 = 1 | DOTPROD = 1 | SME = 1 | ACCELERATE = 1 | AARCH64_REPACK = 1 | \n",
      "\n",
      "main: processing './Sound/01/chunk_021.wav' (1920435 samples, 120.0 sec), 4 threads, 1 processors, 5 beams + best of 5, lang = ko, task = transcribe, timestamps = 1 ...\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "[00:00:00.000 --> 00:00:04.860]   저희에 올렸는데 왜 안 올리니 부담 때리는 방함이\n",
      "[00:00:04.860 --> 00:00:08.800]   7번 학생은 10번 학생도 해봤는데 7번 학생도 해봤다고요?\n",
      "[00:00:08.800 --> 00:00:12.000]   그쵸 그쵸 그정도는 그거 할 줄 몰라요\n",
      "[00:00:12.000 --> 00:00:12.000]   그쵸 그쵸 그쵸 그쵸 그정도는 그쵸 그쵸 그쵸 그쵸 그쵸 그쵸 그쵸 그쵸 그쵸 그쵸 그쵸 그쵸 그쵸 그쵸 그쵸 그쵸 그쵸 그쵸 그쵸 그쵸 그쵸 그쵸 그쵸 그쵸 그쵸 그쵸 그쵸 그쵸 그쵸 그쵸 그쵸 그쵸 그쵸 그쵸 그쵸 그쵸 그쵸 그쵸 그쵸 그쵸 그쵸 그쵸 그쵸 그쵸 그쵸 그쵸 그쵸 그\n",
      "[00:00:12.000 --> 00:00:36.020]   근데 7번 학생도 해봤는데 7번 학생도 해봤는데 7번 학생도 해봤다고요?\n",
      "[00:00:42.000 --> 00:01:12.000]   아 아 아 아 아 아 아 아 아 아 아 아 아 아 아 아 아 아 아 아 아 아 아 아 아 아 아 아 아 아 아 아 아 아 아 아 아 아 아 아 아 아아 아 아 아 아 아 아 아 아 아 아 아 아 아 아 아 아 아 아 아 아 아 아 아 아 아 아 아 아 아 아 아 아 아 아 아 아 아 아 아 아 아 아 아 아 아 아 아 아 아 아 아 아 아 아 아 아 아 아 아 아 아 아 아 아 아 아 아 아 아 아 아 아 아 아 아 아 아 아 아 아 아 아 아 아 아 아 아 아 아 아 아 아 아 아 아 아 아 아 아 아 아 아 아 아 아 아 아 아 아 아 아 아 아 아 아 아 아 아 아 아 아 아 아 아 아 아 아 아 아 아 아 아 아 아 아 아 아 아 아 아 아 아 아 아 아 아 아 아 아 아 아 아 아 아 아 아 아 아 아 아 아 아 아 아 아 아 아 아 아 아 아 아 아 아 아 아\n",
      "[00:01:12.000 --> 00:01:14.000]   (끝나는 소리)\n",
      "[00:01:14.000 --> 00:01:16.000]   (끝나는 소리)\n",
      "[00:01:16.000 --> 00:01:18.000]   근데 시내는...\n",
      "[00:01:18.000 --> 00:01:20.000]   (끝나는 소리)\n",
      "[00:01:20.000 --> 00:01:22.000]   (끝나는 소리)\n",
      "[00:01:22.000 --> 00:01:24.000]   (끝나는 소리)\n",
      "[00:01:24.000 --> 00:01:26.000]   (끝나는 소리)\n",
      "[00:01:26.000 --> 00:01:28.000]   (끝나는 소리)\n",
      "[00:01:28.000 --> 00:01:30.000]   (끝나는 소리)\n",
      "[00:01:30.000 --> 00:01:32.000]   (끝나는 소리)\n",
      "[00:01:32.000 --> 00:01:34.000]   (끝나는 소리)\n",
      "[00:01:34.000 --> 00:01:36.000]   (끝나는 소리)\n",
      "[00:01:36.000 --> 00:01:38.000]   (끝나는 소리)\n",
      "[00:01:38.000 --> 00:01:40.000]   (끝나는 소리)\n",
      "[00:01:40.000 --> 00:01:42.000]   (끝나는 소리)\n",
      "[00:01:42.000 --> 00:01:44.000]   (끝나는 소리)\n",
      "[00:01:44.000 --> 00:01:46.000]   (끝나는 소리)\n",
      "[00:01:46.000 --> 00:01:48.000]   (끝나는 소리)\n",
      "[00:01:48.000 --> 00:01:50.000]   (끝나는 소리)\n",
      "[00:01:50.000 --> 00:01:52.000]   (끝나는 소리)\n",
      "[00:01:52.000 --> 00:01:54.000]   (끝나는 소리)\n",
      "[00:01:54.000 --> 00:01:56.000]   (끝나는 소리)\n",
      "[00:01:56.000 --> 00:01:58.000]   (끝나는 소리)\n",
      "[00:01:58.000 --> 00:02:00.000]   (끝나는 소리)\n",
      "\n",
      "✅ chunk_021 처리 완료 → ./Text/01/chunk_021_ggml-small.json\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "output_json: saving output to './Text/01/chunk_021_ggml-small.json'\n",
      "\n",
      "whisper_print_timings:     load time =   188.32 ms\n",
      "whisper_print_timings:     fallbacks =  11 p /   6 h\n",
      "whisper_print_timings:      mel time =    28.71 ms\n",
      "whisper_print_timings:   sample time =  2236.60 ms / 15337 runs (     0.15 ms per run)\n",
      "whisper_print_timings:   encode time =   573.52 ms /     5 runs (   114.70 ms per run)\n",
      "whisper_print_timings:   decode time =   493.90 ms /   125 runs (     3.95 ms per run)\n",
      "whisper_print_timings:   batchd time = 14558.40 ms / 15165 runs (     0.96 ms per run)\n",
      "whisper_print_timings:   prompt time =    66.78 ms /   493 runs (     0.14 ms per run)\n",
      "whisper_print_timings:    total time = 18218.52 ms\n",
      "ggml_metal_free: deallocating\n",
      "whisper_init_from_file_with_params_no_state: loading model from '/Users/aza1200/.cache/huggingface/hub/models--ggerganov--whisper.cpp/snapshots/5359861c739e955e79d9a303bcbc70fb988958b1/ggml-small.bin'\n",
      "whisper_init_with_params_no_state: use gpu    = 1\n",
      "whisper_init_with_params_no_state: flash attn = 0\n",
      "whisper_init_with_params_no_state: gpu_device = 0\n",
      "whisper_init_with_params_no_state: dtw        = 0\n",
      "whisper_init_with_params_no_state: devices    = 3\n",
      "whisper_init_with_params_no_state: backends   = 3\n",
      "whisper_model_load: loading model\n",
      "whisper_model_load: n_vocab       = 51865\n",
      "whisper_model_load: n_audio_ctx   = 1500\n",
      "whisper_model_load: n_audio_state = 768\n",
      "whisper_model_load: n_audio_head  = 12\n",
      "whisper_model_load: n_audio_layer = 12\n",
      "whisper_model_load: n_text_ctx    = 448\n",
      "whisper_model_load: n_text_state  = 768\n",
      "whisper_model_load: n_text_head   = 12\n",
      "whisper_model_load: n_text_layer  = 12\n",
      "whisper_model_load: n_mels        = 80\n",
      "whisper_model_load: ftype         = 1\n",
      "whisper_model_load: qntvr         = 0\n",
      "whisper_model_load: type          = 3 (small)\n",
      "whisper_model_load: adding 1608 extra tokens\n",
      "whisper_model_load: n_langs       = 99\n",
      "whisper_model_load:        Metal total size =   487.01 MB\n",
      "whisper_model_load: model size    =  487.01 MB\n",
      "whisper_backend_init_gpu: using Metal backend\n",
      "ggml_metal_init: allocating\n",
      "ggml_metal_init: found device: Apple M4 Pro\n",
      "ggml_metal_init: picking default device: Apple M4 Pro\n",
      "ggml_metal_load_library: using embedded metal library\n",
      "ggml_metal_init: GPU name:   Apple M4 Pro\n",
      "ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)\n",
      "ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)\n",
      "ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)\n",
      "ggml_metal_init: simdgroup reduction   = true\n",
      "ggml_metal_init: simdgroup matrix mul. = true\n",
      "ggml_metal_init: has residency sets    = true\n",
      "ggml_metal_init: has bfloat            = true\n",
      "ggml_metal_init: use bfloat            = false\n",
      "ggml_metal_init: hasUnifiedMemory      = true\n",
      "ggml_metal_init: recommendedMaxWorkingSetSize  = 38654.71 MB\n",
      "ggml_metal_init: skipping kernel_get_rows_bf16                     (not supported)\n",
      "ggml_metal_init: skipping kernel_mul_mv_bf16_f32                   (not supported)\n",
      "ggml_metal_init: skipping kernel_mul_mv_bf16_f32_1row              (not supported)\n",
      "ggml_metal_init: skipping kernel_mul_mv_bf16_f32_l4                (not supported)\n",
      "ggml_metal_init: skipping kernel_mul_mv_bf16_bf16                  (not supported)\n",
      "ggml_metal_init: skipping kernel_mul_mv_id_bf16_f32                (not supported)\n",
      "ggml_metal_init: skipping kernel_mul_mm_bf16_f32                   (not supported)\n",
      "ggml_metal_init: skipping kernel_mul_mm_id_bf16_f32                (not supported)\n",
      "ggml_metal_init: skipping kernel_flash_attn_ext_bf16_h64           (not supported)\n",
      "ggml_metal_init: skipping kernel_flash_attn_ext_bf16_h80           (not supported)\n",
      "ggml_metal_init: skipping kernel_flash_attn_ext_bf16_h96           (not supported)\n",
      "ggml_metal_init: skipping kernel_flash_attn_ext_bf16_h112          (not supported)\n",
      "ggml_metal_init: skipping kernel_flash_attn_ext_bf16_h128          (not supported)\n",
      "ggml_metal_init: skipping kernel_flash_attn_ext_bf16_h192          (not supported)\n",
      "ggml_metal_init: skipping kernel_flash_attn_ext_bf16_hk192_hv128   (not supported)\n",
      "ggml_metal_init: skipping kernel_flash_attn_ext_bf16_h256          (not supported)\n",
      "ggml_metal_init: skipping kernel_flash_attn_ext_vec_bf16_h128      (not supported)\n",
      "ggml_metal_init: skipping kernel_flash_attn_ext_vec_bf16_h192      (not supported)\n",
      "ggml_metal_init: skipping kernel_flash_attn_ext_vec_bf16_hk192_hv128 (not supported)\n",
      "ggml_metal_init: skipping kernel_flash_attn_ext_vec_bf16_h256      (not supported)\n",
      "ggml_metal_init: skipping kernel_cpy_f32_bf16                      (not supported)\n",
      "ggml_metal_init: skipping kernel_cpy_bf16_f32                      (not supported)\n",
      "ggml_metal_init: skipping kernel_cpy_bf16_bf16                     (not supported)\n",
      "whisper_backend_init: using BLAS backend\n",
      "whisper_init_state: kv self size  =   18.87 MB\n",
      "whisper_init_state: kv cross size =   56.62 MB\n",
      "whisper_init_state: kv pad  size  =    4.72 MB\n",
      "whisper_init_state: compute buffer (conv)   =   23.37 MB\n",
      "whisper_init_state: compute buffer (encode) =  128.01 MB\n",
      "whisper_init_state: compute buffer (cross)  =    6.18 MB\n",
      "whisper_init_state: compute buffer (decode) =   98.19 MB\n",
      "\n",
      "system_info: n_threads = 4 / 14 | WHISPER : COREML = 0 | OPENVINO = 0 | Metal : EMBED_LIBRARY = 1 | CPU : NEON = 1 | ARM_FMA = 1 | FP16_VA = 1 | MATMUL_INT8 = 1 | DOTPROD = 1 | SME = 1 | ACCELERATE = 1 | AARCH64_REPACK = 1 | \n",
      "\n",
      "main: processing './Sound/01/chunk_022.wav' (1919874 samples, 120.0 sec), 4 threads, 1 processors, 5 beams + best of 5, lang = ko, task = transcribe, timestamps = 1 ...\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "[00:00:00.000 --> 00:00:02.000]   수고하셨습니다.\n",
      "[00:00:02.000 --> 00:00:04.000]   수고하셨습니다.\n",
      "[00:00:04.000 --> 00:00:06.000]   수고하셨습니다.\n",
      "[00:00:06.000 --> 00:00:08.000]   수고하셨습니다.\n",
      "[00:00:08.000 --> 00:00:10.000]   수고하셨습니다.\n",
      "[00:00:10.000 --> 00:00:12.000]   수고하셨습니다.\n",
      "[00:00:12.000 --> 00:00:14.000]   수고하셨습니다.\n",
      "[00:00:14.000 --> 00:00:16.000]   수고하셨습니다.\n",
      "[00:00:16.000 --> 00:00:18.000]   수고하셨습니다.\n",
      "[00:00:18.000 --> 00:00:20.000]   수고하셨습니다.\n",
      "[00:00:20.000 --> 00:00:22.000]   수고하셨습니다.\n",
      "[00:00:22.000 --> 00:00:24.000]   수고하셨습니다.\n",
      "[00:00:24.000 --> 00:00:26.000]   수고합니다.\n",
      "[00:00:26.000 --> 00:00:28.000]   수고하셨습니다.\n",
      "[00:00:28.000 --> 00:00:30.000]   시작할게요.\n",
      "[00:00:30.000 --> 00:00:32.000]   제가 사내PC를 가져오러 간 이유는\n",
      "[00:00:32.000 --> 00:00:34.000]   Argo CD를\n",
      "[00:00:34.000 --> 00:00:36.000]   Jenkins도 그렇고, Argo CD도 그렇고\n",
      "[00:00:36.000 --> 00:00:38.000]   저희가 사내환경에서 배포를 할 때는\n",
      "[00:00:38.000 --> 00:00:40.000]   사내PC로\n",
      "[00:00:40.000 --> 00:00:42.000]   접속을 방압역을 뚫어서 하고 있어요.\n",
      "[00:00:42.000 --> 00:00:44.000]   그래서\n",
      "[00:00:44.000 --> 00:00:46.000]   지금 간단하게 방압역에 대한 정책 얘기를 하고서\n",
      "[00:00:46.000 --> 00:00:48.000]   그다음에 CD 들어가 보도록 할게요.\n",
      "[00:00:48.000 --> 00:00:50.000]   우리가 CIC를 하기 위해서는\n",
      "[00:00:50.000 --> 00:00:52.000]   오늘 이제 여러분들이\n",
      "[00:00:52.000 --> 00:00:54.000]   실습을 하고\n",
      "[00:00:54.000 --> 00:00:56.000]   자체가 미리 방압역을 뚫어둔 부분도 있어서\n",
      "[00:00:56.000 --> 00:00:58.000]   그것까지 좀 인지를 하시면 좋을 것 같아요.\n",
      "[00:00:58.000 --> 00:01:00.000]   첫 번째로\n",
      "[00:01:00.000 --> 00:01:02.000]   우리가 CIC를 하려면\n",
      "[00:01:02.000 --> 00:01:04.000]   기톱 액션을 쓰기 위한\n",
      "[00:01:04.000 --> 00:01:06.000]   방압역 순정이 필요합니다.\n",
      "[00:01:06.000 --> 00:01:08.000]   그래서\n",
      "[00:01:08.000 --> 00:01:10.000]   우리가 VNBef를 할 때\n",
      "[00:01:10.000 --> 00:01:12.000]   VNBef의 경우에는\n",
      "[00:01:12.000 --> 00:01:14.000]   우리가 CI를 하면\n",
      "[00:01:14.000 --> 00:01:16.000]   지금껏 계속 얘기했던 정적 파일들이 있죠.\n",
      "[00:01:16.000 --> 00:01:18.000]   점, 자르, 점, 와르\n",
      "[00:01:18.000 --> 00:01:20.000]   아니면 파이선, 점, 위일 같은 파일들을\n",
      "[00:01:20.000 --> 00:01:22.000]   블록 스토리지에 올릴 거예요.\n",
      "[00:01:22.000 --> 00:01:24.000]   블록 스토리지에다가 업로드하는 기체는\n",
      "[00:01:24.000 --> 00:01:26.000]   서비스 팀마다 구성한\n",
      "[00:01:26.000 --> 00:01:28.000]   블록 스토리지에 대해서\n",
      "[00:01:28.000 --> 00:01:30.000]   방압역 신청을 직접 하셔야 됩니다.\n",
      "[00:01:30.000 --> 00:01:32.000]   이거는 저희 팀 해드리는 건 아니고\n",
      "[00:01:32.000 --> 00:01:34.000]   러너 걸리는 저희가 하지만\n",
      "[00:01:34.000 --> 00:01:36.000]   방압역은 직접 해야 된다.\n",
      "[00:01:36.000 --> 00:01:38.000]   따라서 VNBef의 경우\n",
      "[00:01:38.000 --> 00:01:40.000]   우리 러너 서버의 로드 밸런스 IP를\n",
      "[00:01:40.000 --> 00:01:42.000]   여기에 구성해놨어요.\n",
      "[00:01:42.000 --> 00:01:44.000]   거기에서\n",
      "[00:01:44.000 --> 00:01:46.000]   서비스 팀마다 꾸릴\n",
      "[00:01:46.000 --> 00:01:48.000]   스토리지 아카운트에\n",
      "[00:01:48.000 --> 00:01:50.000]   프라이빗 엔드 포인트 들어보셨나요?\n",
      "[00:01:50.000 --> 00:01:52.000]   프라이빗 엔드 포인트에\n",
      "[00:01:52.000 --> 00:01:54.000]   FQDN URL로\n",
      "[00:01:54.000 --> 00:01:56.000]   443를 하시면 되는데\n",
      "[00:01:56.000 --> 00:01:58.000]   이걸 직접 보여드리면\n",
      "[00:01:58.000 --> 00:02:00.000]   어떻게 보자면 이제\n",
      "\n",
      "✅ chunk_022 처리 완료 → ./Text/01/chunk_022_ggml-small.json\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "output_json: saving output to './Text/01/chunk_022_ggml-small.json'\n",
      "\n",
      "whisper_print_timings:     load time =   184.30 ms\n",
      "whisper_print_timings:     fallbacks =   5 p /   9 h\n",
      "whisper_print_timings:      mel time =    28.15 ms\n",
      "whisper_print_timings:   sample time =  1213.12 ms /  8420 runs (     0.14 ms per run)\n",
      "whisper_print_timings:   encode time =   573.78 ms /     5 runs (   114.76 ms per run)\n",
      "whisper_print_timings:   decode time =    68.21 ms /    16 runs (     4.26 ms per run)\n",
      "whisper_print_timings:   batchd time =  7664.80 ms /  8373 runs (     0.92 ms per run)\n",
      "whisper_print_timings:   prompt time =    91.75 ms /   799 runs (     0.11 ms per run)\n",
      "whisper_print_timings:    total time =  9895.34 ms\n",
      "ggml_metal_free: deallocating\n",
      "whisper_init_from_file_with_params_no_state: loading model from '/Users/aza1200/.cache/huggingface/hub/models--ggerganov--whisper.cpp/snapshots/5359861c739e955e79d9a303bcbc70fb988958b1/ggml-small.bin'\n",
      "whisper_init_with_params_no_state: use gpu    = 1\n",
      "whisper_init_with_params_no_state: flash attn = 0\n",
      "whisper_init_with_params_no_state: gpu_device = 0\n",
      "whisper_init_with_params_no_state: dtw        = 0\n",
      "whisper_init_with_params_no_state: devices    = 3\n",
      "whisper_init_with_params_no_state: backends   = 3\n",
      "whisper_model_load: loading model\n",
      "whisper_model_load: n_vocab       = 51865\n",
      "whisper_model_load: n_audio_ctx   = 1500\n",
      "whisper_model_load: n_audio_state = 768\n",
      "whisper_model_load: n_audio_head  = 12\n",
      "whisper_model_load: n_audio_layer = 12\n",
      "whisper_model_load: n_text_ctx    = 448\n",
      "whisper_model_load: n_text_state  = 768\n",
      "whisper_model_load: n_text_head   = 12\n",
      "whisper_model_load: n_text_layer  = 12\n",
      "whisper_model_load: n_mels        = 80\n",
      "whisper_model_load: ftype         = 1\n",
      "whisper_model_load: qntvr         = 0\n",
      "whisper_model_load: type          = 3 (small)\n",
      "whisper_model_load: adding 1608 extra tokens\n",
      "whisper_model_load: n_langs       = 99\n",
      "whisper_model_load:        Metal total size =   487.01 MB\n",
      "whisper_model_load: model size    =  487.01 MB\n",
      "whisper_backend_init_gpu: using Metal backend\n",
      "ggml_metal_init: allocating\n",
      "ggml_metal_init: found device: Apple M4 Pro\n",
      "ggml_metal_init: picking default device: Apple M4 Pro\n",
      "ggml_metal_load_library: using embedded metal library\n",
      "ggml_metal_init: GPU name:   Apple M4 Pro\n",
      "ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)\n",
      "ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)\n",
      "ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)\n",
      "ggml_metal_init: simdgroup reduction   = true\n",
      "ggml_metal_init: simdgroup matrix mul. = true\n",
      "ggml_metal_init: has residency sets    = true\n",
      "ggml_metal_init: has bfloat            = true\n",
      "ggml_metal_init: use bfloat            = false\n",
      "ggml_metal_init: hasUnifiedMemory      = true\n",
      "ggml_metal_init: recommendedMaxWorkingSetSize  = 38654.71 MB\n",
      "ggml_metal_init: skipping kernel_get_rows_bf16                     (not supported)\n",
      "ggml_metal_init: skipping kernel_mul_mv_bf16_f32                   (not supported)\n",
      "ggml_metal_init: skipping kernel_mul_mv_bf16_f32_1row              (not supported)\n",
      "ggml_metal_init: skipping kernel_mul_mv_bf16_f32_l4                (not supported)\n",
      "ggml_metal_init: skipping kernel_mul_mv_bf16_bf16                  (not supported)\n",
      "ggml_metal_init: skipping kernel_mul_mv_id_bf16_f32                (not supported)\n",
      "ggml_metal_init: skipping kernel_mul_mm_bf16_f32                   (not supported)\n",
      "ggml_metal_init: skipping kernel_mul_mm_id_bf16_f32                (not supported)\n",
      "ggml_metal_init: skipping kernel_flash_attn_ext_bf16_h64           (not supported)\n",
      "ggml_metal_init: skipping kernel_flash_attn_ext_bf16_h80           (not supported)\n",
      "ggml_metal_init: skipping kernel_flash_attn_ext_bf16_h96           (not supported)\n",
      "ggml_metal_init: skipping kernel_flash_attn_ext_bf16_h112          (not supported)\n",
      "ggml_metal_init: skipping kernel_flash_attn_ext_bf16_h128          (not supported)\n",
      "ggml_metal_init: skipping kernel_flash_attn_ext_bf16_h192          (not supported)\n",
      "ggml_metal_init: skipping kernel_flash_attn_ext_bf16_hk192_hv128   (not supported)\n",
      "ggml_metal_init: skipping kernel_flash_attn_ext_bf16_h256          (not supported)\n",
      "ggml_metal_init: skipping kernel_flash_attn_ext_vec_bf16_h128      (not supported)\n",
      "ggml_metal_init: skipping kernel_flash_attn_ext_vec_bf16_h192      (not supported)\n",
      "ggml_metal_init: skipping kernel_flash_attn_ext_vec_bf16_hk192_hv128 (not supported)\n",
      "ggml_metal_init: skipping kernel_flash_attn_ext_vec_bf16_h256      (not supported)\n",
      "ggml_metal_init: skipping kernel_cpy_f32_bf16                      (not supported)\n",
      "ggml_metal_init: skipping kernel_cpy_bf16_f32                      (not supported)\n",
      "ggml_metal_init: skipping kernel_cpy_bf16_bf16                     (not supported)\n",
      "whisper_backend_init: using BLAS backend\n",
      "whisper_init_state: kv self size  =   18.87 MB\n",
      "whisper_init_state: kv cross size =   56.62 MB\n",
      "whisper_init_state: kv pad  size  =    4.72 MB\n",
      "whisper_init_state: compute buffer (conv)   =   23.37 MB\n",
      "whisper_init_state: compute buffer (encode) =  128.01 MB\n",
      "whisper_init_state: compute buffer (cross)  =    6.18 MB\n",
      "whisper_init_state: compute buffer (decode) =   98.19 MB\n",
      "\n",
      "system_info: n_threads = 4 / 14 | WHISPER : COREML = 0 | OPENVINO = 0 | Metal : EMBED_LIBRARY = 1 | CPU : NEON = 1 | ARM_FMA = 1 | FP16_VA = 1 | MATMUL_INT8 = 1 | DOTPROD = 1 | SME = 1 | ACCELERATE = 1 | AARCH64_REPACK = 1 | \n",
      "\n",
      "main: processing './Sound/01/chunk_023.wav' (1920561 samples, 120.0 sec), 4 threads, 1 processors, 5 beams + best of 5, lang = ko, task = transcribe, timestamps = 1 ...\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "[00:00:00.000 --> 00:00:07.500]   제가 어디에 요청할지에 대한 블록스토리즈, 스토리즈 아카운트의 진입점이라고 보시면 돼요.\n",
      "[00:00:07.500 --> 00:00:12.180]   그래서 저의 FQDN을 어디서 확인하는지는\n",
      "[00:00:12.180 --> 00:00:15.660]   여기에 우리 구독을 보여드리자면\n",
      "[00:00:15.660 --> 00:00:19.500]   여러가지에다가 이 프라이벳 앤드 포인트 이 부분이거든요.\n",
      "[00:00:19.500 --> 00:00:23.340]   이게 이제 블록스토리즈마다 하나씩 생성이 될 거예요.\n",
      "[00:00:23.340 --> 00:00:24.840]   이거 블록스토리즈 찍어낼 때.\n",
      "[00:00:24.840 --> 00:00:31.340]   프라이벳 앤드 포인트의 설정에 DNS 구성에 FQDN 값이라는 게 있어요.\n",
      "[00:00:31.340 --> 00:00:37.220]   이게 저희가 통합 DNS라고 해서 애정 안에서 쓰는 DNS 정보들을 다 한 데서 모으고 있기 때문에\n",
      "[00:00:37.220 --> 00:00:41.520]   FQDN 정보인 이 URL로 방압역 443을 요청하시면\n",
      "[00:00:41.520 --> 00:00:44.700]   우리 키터프에서 CI 할 때\n",
      "[00:00:44.700 --> 00:00:50.780]   러너에서 서비스 구도마다의 블록스토리즈에 파일 업로드할 수 있는 경로를 뚫어주게 되는 거예요.\n",
      "[00:00:50.780 --> 00:00:53.960]   우리는 이런 블록스토리즈 쓰는 CI를 안 했죠.\n",
      "[00:00:53.960 --> 00:00:59.000]   그러면 우리 acr에다가 올렸는데 acr도 이 프라이벳 앤드 포인트가 있습니다.\n",
      "[00:00:59.000 --> 00:01:06.780]   그래서 컨테이너 100포의 경우도 acr의 프라이벳 앤드 포인트에 FQDN에다가 수시를 하시면 되는데\n",
      "[00:01:06.780 --> 00:01:08.700]   그거는 어디에 있느냐?\n",
      "[00:01:08.700 --> 00:01:13.060]   또 동일하게 저희 구독 기준으로 말씀 드릴게요.\n",
      "[00:01:13.060 --> 00:01:17.640]   이게 이제 여러분들이 팀에 가서 쓰셔야 할 acr이라고 한다면\n",
      "[00:01:17.640 --> 00:01:24.500]   여기 네트워크 들어와서 프라이빗 액세스에 이 프라이빗 앤드 포인트를 기점으로 들어가는 거예요.\n",
      "[00:01:24.500 --> 00:01:29.600]   근데 acr 같은 경우에는 FQDN 정보가 두 개거든요.\n",
      "[00:01:29.600 --> 00:01:32.640]   하나는 이 리켓스를 받을 곳\n",
      "[00:01:32.640 --> 00:01:35.980]   하나는 여기에 대한 실질적을 저장을 하고 있는 저장소에 대한 정보예요.\n",
      "[00:01:35.980 --> 00:01:43.960]   그래서 우리가 acr에다가 푸시를 할 때는 러너에서 FQDN 두 개의 정보에다가\n",
      "[00:01:43.960 --> 00:01:46.380]   이제 443 방압역 신청을 하셔야 된다.\n",
      "[00:01:46.380 --> 00:01:49.440]   방압역 신청은 어디서 하시는지 아시나요?\n",
      "[00:01:49.440 --> 00:01:53.840]   보시는 저희는 방압역 득컴이라는 거를 쓰고 있어요.\n",
      "[00:01:53.840 --> 00:01:56.880]   마이오월드 케이펫 좀 먹어.\n",
      "[00:01:56.880 --> 00:02:00.100]   여기 들어가서 그냥 KT4번 비밀밀밀밀밀밀밀밀\n",
      "\n",
      "✅ chunk_023 처리 완료 → ./Text/01/chunk_023_ggml-small.json\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "output_json: saving output to './Text/01/chunk_023_ggml-small.json'\n",
      "\n",
      "whisper_print_timings:     load time =   180.26 ms\n",
      "whisper_print_timings:     fallbacks =   0 p /   1 h\n",
      "whisper_print_timings:      mel time =    28.26 ms\n",
      "whisper_print_timings:   sample time =   495.07 ms /  3119 runs (     0.16 ms per run)\n",
      "whisper_print_timings:   encode time =   573.21 ms /     5 runs (   114.64 ms per run)\n",
      "whisper_print_timings:   decode time =   658.60 ms /   160 runs (     4.12 ms per run)\n",
      "whisper_print_timings:   batchd time =  2652.67 ms /  2937 runs (     0.90 ms per run)\n",
      "whisper_print_timings:   prompt time =    92.42 ms /   809 runs (     0.11 ms per run)\n",
      "whisper_print_timings:    total time =  4750.82 ms\n",
      "ggml_metal_free: deallocating\n",
      "whisper_init_from_file_with_params_no_state: loading model from '/Users/aza1200/.cache/huggingface/hub/models--ggerganov--whisper.cpp/snapshots/5359861c739e955e79d9a303bcbc70fb988958b1/ggml-small.bin'\n",
      "whisper_init_with_params_no_state: use gpu    = 1\n",
      "whisper_init_with_params_no_state: flash attn = 0\n",
      "whisper_init_with_params_no_state: gpu_device = 0\n",
      "whisper_init_with_params_no_state: dtw        = 0\n",
      "whisper_init_with_params_no_state: devices    = 3\n",
      "whisper_init_with_params_no_state: backends   = 3\n",
      "whisper_model_load: loading model\n",
      "whisper_model_load: n_vocab       = 51865\n",
      "whisper_model_load: n_audio_ctx   = 1500\n",
      "whisper_model_load: n_audio_state = 768\n",
      "whisper_model_load: n_audio_head  = 12\n",
      "whisper_model_load: n_audio_layer = 12\n",
      "whisper_model_load: n_text_ctx    = 448\n",
      "whisper_model_load: n_text_state  = 768\n",
      "whisper_model_load: n_text_head   = 12\n",
      "whisper_model_load: n_text_layer  = 12\n",
      "whisper_model_load: n_mels        = 80\n",
      "whisper_model_load: ftype         = 1\n",
      "whisper_model_load: qntvr         = 0\n",
      "whisper_model_load: type          = 3 (small)\n",
      "whisper_model_load: adding 1608 extra tokens\n",
      "whisper_model_load: n_langs       = 99\n",
      "whisper_model_load:        Metal total size =   487.01 MB\n",
      "whisper_model_load: model size    =  487.01 MB\n",
      "whisper_backend_init_gpu: using Metal backend\n",
      "ggml_metal_init: allocating\n",
      "ggml_metal_init: found device: Apple M4 Pro\n",
      "ggml_metal_init: picking default device: Apple M4 Pro\n",
      "ggml_metal_load_library: using embedded metal library\n",
      "ggml_metal_init: GPU name:   Apple M4 Pro\n",
      "ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)\n",
      "ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)\n",
      "ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)\n",
      "ggml_metal_init: simdgroup reduction   = true\n",
      "ggml_metal_init: simdgroup matrix mul. = true\n",
      "ggml_metal_init: has residency sets    = true\n",
      "ggml_metal_init: has bfloat            = true\n",
      "ggml_metal_init: use bfloat            = false\n",
      "ggml_metal_init: hasUnifiedMemory      = true\n",
      "ggml_metal_init: recommendedMaxWorkingSetSize  = 38654.71 MB\n",
      "ggml_metal_init: skipping kernel_get_rows_bf16                     (not supported)\n",
      "ggml_metal_init: skipping kernel_mul_mv_bf16_f32                   (not supported)\n",
      "ggml_metal_init: skipping kernel_mul_mv_bf16_f32_1row              (not supported)\n",
      "ggml_metal_init: skipping kernel_mul_mv_bf16_f32_l4                (not supported)\n",
      "ggml_metal_init: skipping kernel_mul_mv_bf16_bf16                  (not supported)\n",
      "ggml_metal_init: skipping kernel_mul_mv_id_bf16_f32                (not supported)\n",
      "ggml_metal_init: skipping kernel_mul_mm_bf16_f32                   (not supported)\n",
      "ggml_metal_init: skipping kernel_mul_mm_id_bf16_f32                (not supported)\n",
      "ggml_metal_init: skipping kernel_flash_attn_ext_bf16_h64           (not supported)\n",
      "ggml_metal_init: skipping kernel_flash_attn_ext_bf16_h80           (not supported)\n",
      "ggml_metal_init: skipping kernel_flash_attn_ext_bf16_h96           (not supported)\n",
      "ggml_metal_init: skipping kernel_flash_attn_ext_bf16_h112          (not supported)\n",
      "ggml_metal_init: skipping kernel_flash_attn_ext_bf16_h128          (not supported)\n",
      "ggml_metal_init: skipping kernel_flash_attn_ext_bf16_h192          (not supported)\n",
      "ggml_metal_init: skipping kernel_flash_attn_ext_bf16_hk192_hv128   (not supported)\n",
      "ggml_metal_init: skipping kernel_flash_attn_ext_bf16_h256          (not supported)\n",
      "ggml_metal_init: skipping kernel_flash_attn_ext_vec_bf16_h128      (not supported)\n",
      "ggml_metal_init: skipping kernel_flash_attn_ext_vec_bf16_h192      (not supported)\n",
      "ggml_metal_init: skipping kernel_flash_attn_ext_vec_bf16_hk192_hv128 (not supported)\n",
      "ggml_metal_init: skipping kernel_flash_attn_ext_vec_bf16_h256      (not supported)\n",
      "ggml_metal_init: skipping kernel_cpy_f32_bf16                      (not supported)\n",
      "ggml_metal_init: skipping kernel_cpy_bf16_f32                      (not supported)\n",
      "ggml_metal_init: skipping kernel_cpy_bf16_bf16                     (not supported)\n",
      "whisper_backend_init: using BLAS backend\n",
      "whisper_init_state: kv self size  =   18.87 MB\n",
      "whisper_init_state: kv cross size =   56.62 MB\n",
      "whisper_init_state: kv pad  size  =    4.72 MB\n",
      "whisper_init_state: compute buffer (conv)   =   23.37 MB\n",
      "whisper_init_state: compute buffer (encode) =  128.01 MB\n",
      "whisper_init_state: compute buffer (cross)  =    6.18 MB\n",
      "whisper_init_state: compute buffer (decode) =   98.19 MB\n",
      "\n",
      "system_info: n_threads = 4 / 14 | WHISPER : COREML = 0 | OPENVINO = 0 | Metal : EMBED_LIBRARY = 1 | CPU : NEON = 1 | ARM_FMA = 1 | FP16_VA = 1 | MATMUL_INT8 = 1 | DOTPROD = 1 | SME = 1 | ACCELERATE = 1 | AARCH64_REPACK = 1 | \n",
      "\n",
      "main: processing './Sound/01/chunk_024.wav' (1919395 samples, 120.0 sec), 4 threads, 1 processors, 5 beams + best of 5, lang = ko, task = transcribe, timestamps = 1 ...\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "[00:00:00.000 --> 00:00:08.240]   1번을 넣으시고 정책 이런 걸 한번 보여드릴게요 그러면 방압 신청을 어떻게 하는지\n",
      "[00:00:08.240 --> 00:00:15.240]   정책 신규 신청 들어가셔서 그냥 간단하게 참고만 이게 이제 방압 신청은 양식이거든요\n",
      "[00:00:15.240 --> 00:00:22.440]   여기서 요청 구분에 우리는 이렇게 운영을 하고 있고 러너는 이제 운영의 개념이고요\n",
      "[00:00:22.440 --> 00:00:27.880]   여러분들이 만약에 그 개발 구독을 받았다면 운영에서 개발 서버를 넣으시면 되고요\n",
      "[00:00:27.880 --> 00:00:39.960]   여기는 내 숙소, IDKIT, 내부공부, 신까지, 담당자, 본인이 신청자 이름 넣으시면 됩니다\n",
      "[00:00:39.960 --> 00:00:46.360]   단위 서비스 코드는 KT에서 IT 프로젝트를 관리하기 위한 고요한 값이거든요\n",
      "[00:00:46.360 --> 00:00:55.040]   고요한 ID가 얘를 이제 직접 넣으시면 되고 대부분 앞에 알파펫 2개랑 숫자 한 4개에서 6개 정도로 이루어져 있어요\n",
      "[00:00:55.040 --> 00:01:01.520]   뭐 이런 단위 서비도 넣으시고 시스템명은 여기 가이드가 있죠\n",
      "[00:01:01.520 --> 00:01:07.640]   시스템명은 여기 있네요 이거 넣으시면 됩니다. 러너에서 신청할 때 우리가 CI 할 때 신청할 때\n",
      "[00:01:07.640 --> 00:01:13.560]   그래서 이렇게 출발지 넣으시고 이거 2개 넣으시고 하나 더 있었어요\n",
      "[00:01:13.560 --> 00:01:25.000]   이거 10점 1, 2개가 들어가죠 그러면 이 데스티네이션에 서비스 구독의 acr 에다가 이미지를 올릴 거잖아요\n",
      "[00:01:25.000 --> 00:01:31.440]   그러면 여기에는 본인팀 넣으시고 본인소서\n",
      "[00:01:31.440 --> 00:01:41.360]   본인 단위 서비스 코드는 acr 로 받으실 거잖아요 acr acr\n",
      "[00:01:41.360 --> 00:01:46.480]   여기 IP정부에 아까 말씀드린 fqdn 값이 2개가 들어가면 되는데\n",
      "[00:01:46.480 --> 00:01:51.520]   첫 번째로는 데이터 acr acr IO를 넣어볼게요\n",
      "[00:01:51.520 --> 00:01:56.920]   이게 다 actps 요청이라서 포트정보연에 4살사로 들어가고\n",
      "[00:01:56.920 --> 00:01:59.920]   tct 들어가는데\n",
      "\n",
      "✅ chunk_024 처리 완료 → ./Text/01/chunk_024_ggml-small.json\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "output_json: saving output to './Text/01/chunk_024_ggml-small.json'\n",
      "\n",
      "whisper_print_timings:     load time =   182.99 ms\n",
      "whisper_print_timings:     fallbacks =   0 p /   0 h\n",
      "whisper_print_timings:      mel time =    28.43 ms\n",
      "whisper_print_timings:   sample time =   372.60 ms /  2288 runs (     0.16 ms per run)\n",
      "whisper_print_timings:   encode time =   573.62 ms /     5 runs (   114.72 ms per run)\n",
      "whisper_print_timings:   decode time =   573.05 ms /   140 runs (     4.09 ms per run)\n",
      "whisper_print_timings:   batchd time =  1953.07 ms /  2126 runs (     0.92 ms per run)\n",
      "whisper_print_timings:   prompt time =    92.64 ms /   792 runs (     0.12 ms per run)\n",
      "whisper_print_timings:    total time =  3842.40 ms\n",
      "ggml_metal_free: deallocating\n",
      "whisper_init_from_file_with_params_no_state: loading model from '/Users/aza1200/.cache/huggingface/hub/models--ggerganov--whisper.cpp/snapshots/5359861c739e955e79d9a303bcbc70fb988958b1/ggml-small.bin'\n",
      "whisper_init_with_params_no_state: use gpu    = 1\n",
      "whisper_init_with_params_no_state: flash attn = 0\n",
      "whisper_init_with_params_no_state: gpu_device = 0\n",
      "whisper_init_with_params_no_state: dtw        = 0\n",
      "whisper_init_with_params_no_state: devices    = 3\n",
      "whisper_init_with_params_no_state: backends   = 3\n",
      "whisper_model_load: loading model\n",
      "whisper_model_load: n_vocab       = 51865\n",
      "whisper_model_load: n_audio_ctx   = 1500\n",
      "whisper_model_load: n_audio_state = 768\n",
      "whisper_model_load: n_audio_head  = 12\n",
      "whisper_model_load: n_audio_layer = 12\n",
      "whisper_model_load: n_text_ctx    = 448\n",
      "whisper_model_load: n_text_state  = 768\n",
      "whisper_model_load: n_text_head   = 12\n",
      "whisper_model_load: n_text_layer  = 12\n",
      "whisper_model_load: n_mels        = 80\n",
      "whisper_model_load: ftype         = 1\n",
      "whisper_model_load: qntvr         = 0\n",
      "whisper_model_load: type          = 3 (small)\n",
      "whisper_model_load: adding 1608 extra tokens\n",
      "whisper_model_load: n_langs       = 99\n",
      "whisper_model_load:        Metal total size =   487.01 MB\n",
      "whisper_model_load: model size    =  487.01 MB\n",
      "whisper_backend_init_gpu: using Metal backend\n",
      "ggml_metal_init: allocating\n",
      "ggml_metal_init: found device: Apple M4 Pro\n",
      "ggml_metal_init: picking default device: Apple M4 Pro\n",
      "ggml_metal_load_library: using embedded metal library\n",
      "ggml_metal_init: GPU name:   Apple M4 Pro\n",
      "ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)\n",
      "ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)\n",
      "ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)\n",
      "ggml_metal_init: simdgroup reduction   = true\n",
      "ggml_metal_init: simdgroup matrix mul. = true\n",
      "ggml_metal_init: has residency sets    = true\n",
      "ggml_metal_init: has bfloat            = true\n",
      "ggml_metal_init: use bfloat            = false\n",
      "ggml_metal_init: hasUnifiedMemory      = true\n",
      "ggml_metal_init: recommendedMaxWorkingSetSize  = 38654.71 MB\n",
      "ggml_metal_init: skipping kernel_get_rows_bf16                     (not supported)\n",
      "ggml_metal_init: skipping kernel_mul_mv_bf16_f32                   (not supported)\n",
      "ggml_metal_init: skipping kernel_mul_mv_bf16_f32_1row              (not supported)\n",
      "ggml_metal_init: skipping kernel_mul_mv_bf16_f32_l4                (not supported)\n",
      "ggml_metal_init: skipping kernel_mul_mv_bf16_bf16                  (not supported)\n",
      "ggml_metal_init: skipping kernel_mul_mv_id_bf16_f32                (not supported)\n",
      "ggml_metal_init: skipping kernel_mul_mm_bf16_f32                   (not supported)\n",
      "ggml_metal_init: skipping kernel_mul_mm_id_bf16_f32                (not supported)\n",
      "ggml_metal_init: skipping kernel_flash_attn_ext_bf16_h64           (not supported)\n",
      "ggml_metal_init: skipping kernel_flash_attn_ext_bf16_h80           (not supported)\n",
      "ggml_metal_init: skipping kernel_flash_attn_ext_bf16_h96           (not supported)\n",
      "ggml_metal_init: skipping kernel_flash_attn_ext_bf16_h112          (not supported)\n",
      "ggml_metal_init: skipping kernel_flash_attn_ext_bf16_h128          (not supported)\n",
      "ggml_metal_init: skipping kernel_flash_attn_ext_bf16_h192          (not supported)\n",
      "ggml_metal_init: skipping kernel_flash_attn_ext_bf16_hk192_hv128   (not supported)\n",
      "ggml_metal_init: skipping kernel_flash_attn_ext_bf16_h256          (not supported)\n",
      "ggml_metal_init: skipping kernel_flash_attn_ext_vec_bf16_h128      (not supported)\n",
      "ggml_metal_init: skipping kernel_flash_attn_ext_vec_bf16_h192      (not supported)\n",
      "ggml_metal_init: skipping kernel_flash_attn_ext_vec_bf16_hk192_hv128 (not supported)\n",
      "ggml_metal_init: skipping kernel_flash_attn_ext_vec_bf16_h256      (not supported)\n",
      "ggml_metal_init: skipping kernel_cpy_f32_bf16                      (not supported)\n",
      "ggml_metal_init: skipping kernel_cpy_bf16_f32                      (not supported)\n",
      "ggml_metal_init: skipping kernel_cpy_bf16_bf16                     (not supported)\n",
      "whisper_backend_init: using BLAS backend\n",
      "whisper_init_state: kv self size  =   18.87 MB\n",
      "whisper_init_state: kv cross size =   56.62 MB\n",
      "whisper_init_state: kv pad  size  =    4.72 MB\n",
      "whisper_init_state: compute buffer (conv)   =   23.37 MB\n",
      "whisper_init_state: compute buffer (encode) =  128.01 MB\n",
      "whisper_init_state: compute buffer (cross)  =    6.18 MB\n",
      "whisper_init_state: compute buffer (decode) =   98.19 MB\n",
      "\n",
      "system_info: n_threads = 4 / 14 | WHISPER : COREML = 0 | OPENVINO = 0 | Metal : EMBED_LIBRARY = 1 | CPU : NEON = 1 | ARM_FMA = 1 | FP16_VA = 1 | MATMUL_INT8 = 1 | DOTPROD = 1 | SME = 1 | ACCELERATE = 1 | AARCH64_REPACK = 1 | \n",
      "\n",
      "main: processing './Sound/01/chunk_025.wav' (1920724 samples, 120.0 sec), 4 threads, 1 processors, 5 beams + best of 5, lang = ko, task = transcribe, timestamps = 1 ...\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "[00:00:00.000 --> 00:00:07.860]   그리고 만료일은 오늘 기준으로 1년 빼기 하루 하면은 오늘 25년 4월 16일이죠\n",
      "[00:00:07.860 --> 00:00:15.880]   그러면은 26년 4월 15일까지 이렇게 요청사유는 뭐\n",
      "[00:00:15.880 --> 00:00:22.000]   애저아에서 CICD 사용을 위약 한 이 정도로 할게요\n",
      "[00:00:22.000 --> 00:00:26.720]   그리고 이게 출발지 IP가 두 개고 목적지 IP가 두 개라서 총 4개의 열이 생겨\n",
      "[00:00:26.720 --> 00:00:34.540]   네 개 행이 생겨야 되거든요 따라서 각각 이 IP 정보들을 보시고\n",
      "[00:00:34.540 --> 00:00:37.740]   우리가 출발지가 이렇게 두 개였거든요\n",
      "[00:00:37.740 --> 00:00:42.940]   2개고 FQDN 값이 두 개잖아요 그러니까 208.2에서 4개의 행을 만드셔야 되고\n",
      "[00:00:42.940 --> 00:00:46.920]   각 출발지마다 FQDN 2개씩 넣으면 되니까\n",
      "[00:00:46.920 --> 00:00:55.540]   이렇게 넣으시면 됩니다 그러면은 1607에서 이 코리아 데이터로 나가고\n",
      "[00:00:55.540 --> 00:01:02.020]   64에서 이렇게 약간 고개 얼음 어쨌든 이 두 개마다 이렇게 하나씩 교차해서 넣으시면 돼요\n",
      "[00:01:02.020 --> 00:01:07.060]   이렇게 해서 방역 신청을 하시면 러너에서 서비스 고독의 ACR에다가 이제 푸시할 수 있게 되는 거죠\n",
      "[00:01:07.060 --> 00:01:12.300]   어제 그 빌드한 그 있잖아요 이미지 그래서 그렇게 방역 신청을 하시면 되고\n",
      "[00:01:12.300 --> 00:01:18.340]   CI 단계 장급까지만 되고 우리가 이제 아까는 젠킨스를 개발맥에서 간단하게\n",
      "[00:01:18.340 --> 00:01:21.760]   그냥 푸시 받아가지고 로커로스트로 띄었어요\n",
      "[00:01:21.760 --> 00:01:26.500]   근데 젠킨스를 쓰기 위한 VM들을 하나씩 가드실 거예요 VM 배포의 경우\n",
      "[00:01:26.500 --> 00:01:31.840]   그때 이 산에 PC에서 그 젠킨스에 붙기 위한 방역 신청이 필요합니다\n",
      "[00:01:31.840 --> 00:01:38.420]   그래서 이때는 우리 이 산에 LG그램 기준 산에 PCIP에서 도착지로 젠킨스 VM IP를 넣고\n",
      "[00:01:38.420 --> 00:01:43.200]   아까 서비스 포트 8080 했죠 그러니까 젠킨스 포트를 여러 둘 포트로 신청하신데요\n",
      "[00:01:43.200 --> 00:01:47.200]   이거는 기본 값이라서 젠킨스에 서비스 기본 값이라서 넣으신 거고\n",
      "[00:01:47.200 --> 00:01:53.240]   정도는 사용자가 젠킨스에 요하에 접근할 수 하기 위합니다 그래서 이런 옵션도 다 있습니다\n",
      "[00:01:53.240 --> 00:01:59.680]   산에 PCIP 방압 신청할 때 위의 번 이제 무시해 주시고\n",
      "\n",
      "✅ chunk_025 처리 완료 → ./Text/01/chunk_025_ggml-small.json\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "output_json: saving output to './Text/01/chunk_025_ggml-small.json'\n",
      "\n",
      "whisper_print_timings:     load time =   189.58 ms\n",
      "whisper_print_timings:     fallbacks =   0 p /   0 h\n",
      "whisper_print_timings:      mel time =    28.44 ms\n",
      "whisper_print_timings:   sample time =   374.21 ms /  2501 runs (     0.15 ms per run)\n",
      "whisper_print_timings:   encode time =   572.58 ms /     5 runs (   114.52 ms per run)\n",
      "whisper_print_timings:   decode time =     8.36 ms /     2 runs (     4.18 ms per run)\n",
      "whisper_print_timings:   batchd time =  2298.50 ms /  2477 runs (     0.93 ms per run)\n",
      "whisper_print_timings:   prompt time =    87.36 ms /   754 runs (     0.12 ms per run)\n",
      "whisper_print_timings:    total time =  3625.43 ms\n",
      "ggml_metal_free: deallocating\n",
      "whisper_init_from_file_with_params_no_state: loading model from '/Users/aza1200/.cache/huggingface/hub/models--ggerganov--whisper.cpp/snapshots/5359861c739e955e79d9a303bcbc70fb988958b1/ggml-small.bin'\n",
      "whisper_init_with_params_no_state: use gpu    = 1\n",
      "whisper_init_with_params_no_state: flash attn = 0\n",
      "whisper_init_with_params_no_state: gpu_device = 0\n",
      "whisper_init_with_params_no_state: dtw        = 0\n",
      "whisper_init_with_params_no_state: devices    = 3\n",
      "whisper_init_with_params_no_state: backends   = 3\n",
      "whisper_model_load: loading model\n",
      "whisper_model_load: n_vocab       = 51865\n",
      "whisper_model_load: n_audio_ctx   = 1500\n",
      "whisper_model_load: n_audio_state = 768\n",
      "whisper_model_load: n_audio_head  = 12\n",
      "whisper_model_load: n_audio_layer = 12\n",
      "whisper_model_load: n_text_ctx    = 448\n",
      "whisper_model_load: n_text_state  = 768\n",
      "whisper_model_load: n_text_head   = 12\n",
      "whisper_model_load: n_text_layer  = 12\n",
      "whisper_model_load: n_mels        = 80\n",
      "whisper_model_load: ftype         = 1\n",
      "whisper_model_load: qntvr         = 0\n",
      "whisper_model_load: type          = 3 (small)\n",
      "whisper_model_load: adding 1608 extra tokens\n",
      "whisper_model_load: n_langs       = 99\n",
      "whisper_model_load:        Metal total size =   487.01 MB\n",
      "whisper_model_load: model size    =  487.01 MB\n",
      "whisper_backend_init_gpu: using Metal backend\n",
      "ggml_metal_init: allocating\n",
      "ggml_metal_init: found device: Apple M4 Pro\n",
      "ggml_metal_init: picking default device: Apple M4 Pro\n",
      "ggml_metal_load_library: using embedded metal library\n",
      "ggml_metal_init: GPU name:   Apple M4 Pro\n",
      "ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)\n",
      "ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)\n",
      "ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)\n",
      "ggml_metal_init: simdgroup reduction   = true\n",
      "ggml_metal_init: simdgroup matrix mul. = true\n",
      "ggml_metal_init: has residency sets    = true\n",
      "ggml_metal_init: has bfloat            = true\n",
      "ggml_metal_init: use bfloat            = false\n",
      "ggml_metal_init: hasUnifiedMemory      = true\n",
      "ggml_metal_init: recommendedMaxWorkingSetSize  = 38654.71 MB\n",
      "ggml_metal_init: skipping kernel_get_rows_bf16                     (not supported)\n",
      "ggml_metal_init: skipping kernel_mul_mv_bf16_f32                   (not supported)\n",
      "ggml_metal_init: skipping kernel_mul_mv_bf16_f32_1row              (not supported)\n",
      "ggml_metal_init: skipping kernel_mul_mv_bf16_f32_l4                (not supported)\n",
      "ggml_metal_init: skipping kernel_mul_mv_bf16_bf16                  (not supported)\n",
      "ggml_metal_init: skipping kernel_mul_mv_id_bf16_f32                (not supported)\n",
      "ggml_metal_init: skipping kernel_mul_mm_bf16_f32                   (not supported)\n",
      "ggml_metal_init: skipping kernel_mul_mm_id_bf16_f32                (not supported)\n",
      "ggml_metal_init: skipping kernel_flash_attn_ext_bf16_h64           (not supported)\n",
      "ggml_metal_init: skipping kernel_flash_attn_ext_bf16_h80           (not supported)\n",
      "ggml_metal_init: skipping kernel_flash_attn_ext_bf16_h96           (not supported)\n",
      "ggml_metal_init: skipping kernel_flash_attn_ext_bf16_h112          (not supported)\n",
      "ggml_metal_init: skipping kernel_flash_attn_ext_bf16_h128          (not supported)\n",
      "ggml_metal_init: skipping kernel_flash_attn_ext_bf16_h192          (not supported)\n",
      "ggml_metal_init: skipping kernel_flash_attn_ext_bf16_hk192_hv128   (not supported)\n",
      "ggml_metal_init: skipping kernel_flash_attn_ext_bf16_h256          (not supported)\n",
      "ggml_metal_init: skipping kernel_flash_attn_ext_vec_bf16_h128      (not supported)\n",
      "ggml_metal_init: skipping kernel_flash_attn_ext_vec_bf16_h192      (not supported)\n",
      "ggml_metal_init: skipping kernel_flash_attn_ext_vec_bf16_hk192_hv128 (not supported)\n",
      "ggml_metal_init: skipping kernel_flash_attn_ext_vec_bf16_h256      (not supported)\n",
      "ggml_metal_init: skipping kernel_cpy_f32_bf16                      (not supported)\n",
      "ggml_metal_init: skipping kernel_cpy_bf16_f32                      (not supported)\n",
      "ggml_metal_init: skipping kernel_cpy_bf16_bf16                     (not supported)\n",
      "whisper_backend_init: using BLAS backend\n",
      "whisper_init_state: kv self size  =   18.87 MB\n",
      "whisper_init_state: kv cross size =   56.62 MB\n",
      "whisper_init_state: kv pad  size  =    4.72 MB\n",
      "whisper_init_state: compute buffer (conv)   =   23.37 MB\n",
      "whisper_init_state: compute buffer (encode) =  128.01 MB\n",
      "whisper_init_state: compute buffer (cross)  =    6.18 MB\n",
      "whisper_init_state: compute buffer (decode) =   98.19 MB\n",
      "\n",
      "system_info: n_threads = 4 / 14 | WHISPER : COREML = 0 | OPENVINO = 0 | Metal : EMBED_LIBRARY = 1 | CPU : NEON = 1 | ARM_FMA = 1 | FP16_VA = 1 | MATMUL_INT8 = 1 | DOTPROD = 1 | SME = 1 | ACCELERATE = 1 | AARCH64_REPACK = 1 | \n",
      "\n",
      "main: processing './Sound/01/chunk_026.wav' (1919505 samples, 120.0 sec), 4 threads, 1 processors, 5 beams + best of 5, lang = ko, task = transcribe, timestamps = 1 ...\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "[00:00:00.000 --> 00:00:07.000]   여기 드롭다운 하시면 사용자가 이제 개발 사내 PCIP 기준이거든요.\n",
      "[00:00:07.000 --> 00:00:10.000]   사용자에서 개발 서버 젠킨스에다가 신청하십니까.\n",
      "[00:00:10.000 --> 00:00:15.000]   여기에 이 IP에다가 사내 PCIP가 들어가면 되겠어.\n",
      "[00:00:15.000 --> 00:00:22.000]   다음 도착지에는 여기에 젠킨스, dm, ip 이렇게.\n",
      "[00:00:22.000 --> 00:00:29.000]   그리고 포트 정보는 아까 8080 pcp 사유는 비슷하게 100% 하기 위함.\n",
      "[00:00:29.000 --> 00:00:31.000]   이렇게 쓰시면 돼요. 간단하죠.\n",
      "[00:00:31.000 --> 00:00:36.000]   이렇게 해서 젠킨스를 쓰시면 되고, 마지막으로 아로고 시디에도 접속을 하셔야 돼요.\n",
      "[00:00:36.000 --> 00:00:44.000]   아로고 시디도 똑같이 사용자에서 사내 PC로 붙을 거라서 동일한데 이 목적지가 달라집니다.\n",
      "[00:00:44.000 --> 00:00:50.000]   목적지는 아로고 시디는 이제 커버네티스에다가 저희가 설치를 해드려요.\n",
      "[00:00:50.000 --> 00:00:56.000]   IAC라고 해서 테라콤으로 아로고 시디를 설치하는 과정을 개발한 게 있는데\n",
      "[00:00:56.000 --> 00:00:59.000]   그 전까지는 저희 팀이 이거를 직접 설치를 해드리기 때문에\n",
      "[00:00:59.000 --> 00:01:06.000]   저는 설치를 받으시면 여기에 아로고 시디의 부합은산장치 커버네티스 인터넷 프롬트헨터 IP라고 써 있어요.\n",
      "[00:01:06.000 --> 00:01:11.000]   그러니까 말 그대로 아로고 시디가 설치된 AKS의 로드 밸런스 IP라고 생각하시면 돼요.\n",
      "[00:01:11.000 --> 00:01:13.000]   그럼 이거는 어디에 있느냐.\n",
      "[00:01:13.000 --> 00:01:20.000]   이제 우리는 AKS 배포를 할 거야 라는 팀은 이 서비스 구독에\n",
      "[00:01:20.000 --> 00:01:25.000]   아니죠. 죄송합니다. 그 로드 밸런서로 들어가야 돼요. 부합은산장치 여기 있죠.\n",
      "[00:01:25.000 --> 00:01:30.000]   부합은산장치에 내 구독에 이 커버네티스 인터넷이라고 있어요.\n",
      "[00:01:30.000 --> 00:01:36.000]   이제 아까 저희 팀 구독은 AKS가 3개이기 때문에 이 부합은산장치가 3개인 거고\n",
      "[00:01:36.000 --> 00:01:39.000]   다 딱 여기 물려있는 리소스 그룹을 볼 수 있거든요.\n",
      "[00:01:39.000 --> 00:01:45.000]   그래서 아까 1번이었으니까 여기에 들어가서 설정에 이 프롬트헨터 IP 있죠.\n",
      "[00:01:45.000 --> 00:01:47.000]   이쪽으로 접속을 하게 되는 거예요.\n",
      "[00:01:47.000 --> 00:01:56.000]   그래서 이 IP 주소를 복사해서 속은 인그레스로 이제 애저 웹앱 라우팅이라는 걸 쓰는데\n",
      "[00:01:56.000 --> 00:02:00.000]   80을 기본으로 받고 만약에 인증서가 추가가 됩니다.\n",
      "\n",
      "✅ chunk_026 처리 완료 → ./Text/01/chunk_026_ggml-small.json\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "output_json: saving output to './Text/01/chunk_026_ggml-small.json'\n",
      "\n",
      "whisper_print_timings:     load time =   177.43 ms\n",
      "whisper_print_timings:     fallbacks =   0 p /   0 h\n",
      "whisper_print_timings:      mel time =    28.44 ms\n",
      "whisper_print_timings:   sample time =   441.26 ms /  2656 runs (     0.17 ms per run)\n",
      "whisper_print_timings:   encode time =   607.50 ms /     5 runs (   121.50 ms per run)\n",
      "whisper_print_timings:   decode time =     8.79 ms /     2 runs (     4.39 ms per run)\n",
      "whisper_print_timings:   batchd time =  2556.97 ms /  2632 runs (     0.97 ms per run)\n",
      "whisper_print_timings:   prompt time =    97.07 ms /   795 runs (     0.12 ms per run)\n",
      "whisper_print_timings:    total time =  3990.72 ms\n",
      "ggml_metal_free: deallocating\n",
      "whisper_init_from_file_with_params_no_state: loading model from '/Users/aza1200/.cache/huggingface/hub/models--ggerganov--whisper.cpp/snapshots/5359861c739e955e79d9a303bcbc70fb988958b1/ggml-small.bin'\n",
      "whisper_init_with_params_no_state: use gpu    = 1\n",
      "whisper_init_with_params_no_state: flash attn = 0\n",
      "whisper_init_with_params_no_state: gpu_device = 0\n",
      "whisper_init_with_params_no_state: dtw        = 0\n",
      "whisper_init_with_params_no_state: devices    = 3\n",
      "whisper_init_with_params_no_state: backends   = 3\n",
      "whisper_model_load: loading model\n",
      "whisper_model_load: n_vocab       = 51865\n",
      "whisper_model_load: n_audio_ctx   = 1500\n",
      "whisper_model_load: n_audio_state = 768\n",
      "whisper_model_load: n_audio_head  = 12\n",
      "whisper_model_load: n_audio_layer = 12\n",
      "whisper_model_load: n_text_ctx    = 448\n",
      "whisper_model_load: n_text_state  = 768\n",
      "whisper_model_load: n_text_head   = 12\n",
      "whisper_model_load: n_text_layer  = 12\n",
      "whisper_model_load: n_mels        = 80\n",
      "whisper_model_load: ftype         = 1\n",
      "whisper_model_load: qntvr         = 0\n",
      "whisper_model_load: type          = 3 (small)\n",
      "whisper_model_load: adding 1608 extra tokens\n",
      "whisper_model_load: n_langs       = 99\n",
      "whisper_model_load:        Metal total size =   487.01 MB\n",
      "whisper_model_load: model size    =  487.01 MB\n",
      "whisper_backend_init_gpu: using Metal backend\n",
      "ggml_metal_init: allocating\n",
      "ggml_metal_init: found device: Apple M4 Pro\n",
      "ggml_metal_init: picking default device: Apple M4 Pro\n",
      "ggml_metal_load_library: using embedded metal library\n",
      "ggml_metal_init: GPU name:   Apple M4 Pro\n",
      "ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)\n",
      "ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)\n",
      "ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)\n",
      "ggml_metal_init: simdgroup reduction   = true\n",
      "ggml_metal_init: simdgroup matrix mul. = true\n",
      "ggml_metal_init: has residency sets    = true\n",
      "ggml_metal_init: has bfloat            = true\n",
      "ggml_metal_init: use bfloat            = false\n",
      "ggml_metal_init: hasUnifiedMemory      = true\n",
      "ggml_metal_init: recommendedMaxWorkingSetSize  = 38654.71 MB\n",
      "ggml_metal_init: skipping kernel_get_rows_bf16                     (not supported)\n",
      "ggml_metal_init: skipping kernel_mul_mv_bf16_f32                   (not supported)\n",
      "ggml_metal_init: skipping kernel_mul_mv_bf16_f32_1row              (not supported)\n",
      "ggml_metal_init: skipping kernel_mul_mv_bf16_f32_l4                (not supported)\n",
      "ggml_metal_init: skipping kernel_mul_mv_bf16_bf16                  (not supported)\n",
      "ggml_metal_init: skipping kernel_mul_mv_id_bf16_f32                (not supported)\n",
      "ggml_metal_init: skipping kernel_mul_mm_bf16_f32                   (not supported)\n",
      "ggml_metal_init: skipping kernel_mul_mm_id_bf16_f32                (not supported)\n",
      "ggml_metal_init: skipping kernel_flash_attn_ext_bf16_h64           (not supported)\n",
      "ggml_metal_init: skipping kernel_flash_attn_ext_bf16_h80           (not supported)\n",
      "ggml_metal_init: skipping kernel_flash_attn_ext_bf16_h96           (not supported)\n",
      "ggml_metal_init: skipping kernel_flash_attn_ext_bf16_h112          (not supported)\n",
      "ggml_metal_init: skipping kernel_flash_attn_ext_bf16_h128          (not supported)\n",
      "ggml_metal_init: skipping kernel_flash_attn_ext_bf16_h192          (not supported)\n",
      "ggml_metal_init: skipping kernel_flash_attn_ext_bf16_hk192_hv128   (not supported)\n",
      "ggml_metal_init: skipping kernel_flash_attn_ext_bf16_h256          (not supported)\n",
      "ggml_metal_init: skipping kernel_flash_attn_ext_vec_bf16_h128      (not supported)\n",
      "ggml_metal_init: skipping kernel_flash_attn_ext_vec_bf16_h192      (not supported)\n",
      "ggml_metal_init: skipping kernel_flash_attn_ext_vec_bf16_hk192_hv128 (not supported)\n",
      "ggml_metal_init: skipping kernel_flash_attn_ext_vec_bf16_h256      (not supported)\n",
      "ggml_metal_init: skipping kernel_cpy_f32_bf16                      (not supported)\n",
      "ggml_metal_init: skipping kernel_cpy_bf16_f32                      (not supported)\n",
      "ggml_metal_init: skipping kernel_cpy_bf16_bf16                     (not supported)\n",
      "whisper_backend_init: using BLAS backend\n",
      "whisper_init_state: kv self size  =   18.87 MB\n",
      "whisper_init_state: kv cross size =   56.62 MB\n",
      "whisper_init_state: kv pad  size  =    4.72 MB\n",
      "whisper_init_state: compute buffer (conv)   =   23.37 MB\n",
      "whisper_init_state: compute buffer (encode) =  128.01 MB\n",
      "whisper_init_state: compute buffer (cross)  =    6.18 MB\n",
      "whisper_init_state: compute buffer (decode) =   98.19 MB\n",
      "\n",
      "system_info: n_threads = 4 / 14 | WHISPER : COREML = 0 | OPENVINO = 0 | Metal : EMBED_LIBRARY = 1 | CPU : NEON = 1 | ARM_FMA = 1 | FP16_VA = 1 | MATMUL_INT8 = 1 | DOTPROD = 1 | SME = 1 | ACCELERATE = 1 | AARCH64_REPACK = 1 | \n",
      "\n",
      "main: processing './Sound/01/chunk_027.wav' (1920065 samples, 120.0 sec), 4 threads, 1 processors, 5 beams + best of 5, lang = ko, task = transcribe, timestamps = 1 ...\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "[00:00:00.000 --> 00:00:04.600]   그러면은 443으로 들어가야 해서 2개를 신청하시면 됩니다.\n",
      "[00:00:04.600 --> 00:00:07.800]   이런 식으로 해서 처음 이 4개 열은\n",
      "[00:00:07.800 --> 00:00:11.800]   acr에다가 ci를 하는 경우에 대한 바가 벽 세팅이고\n",
      "[00:00:11.800 --> 00:00:15.000]   두 번째 이 하나의 행이죠.\n",
      "[00:00:15.000 --> 00:00:18.080]   이건 이제 이 사내빛에서 생스틱 스틱을 접속할 때는\n",
      "[00:00:18.080 --> 00:00:20.200]   그다음에 이 아래의 마지막 2개는\n",
      "[00:00:20.200 --> 00:00:23.600]   이 스펙 스틱에서 rgcd에 접속할 때 이 바가 벽이 필요하다.\n",
      "[00:00:23.600 --> 00:00:27.000]   그리고 이거는 표정 정책이기 때문에 꼭 지켜주셔야 됩니다.\n",
      "[00:00:27.000 --> 00:00:29.000]   안 그러면 접속을 못해요.\n",
      "[00:00:29.000 --> 00:00:32.200]   이런 식으로 해서 방압 벽까지 말씀드렸고\n",
      "[00:00:32.200 --> 00:00:36.300]   이거를 그냥 구조도록 잠깐 다시 보여드리자면\n",
      "[00:00:36.300 --> 00:00:40.800]   아까 처음에 4개의 행은 이 러너에서 여기다가 찌르는 거예요.\n",
      "[00:00:40.800 --> 00:00:43.800]   단순하게 여기서 러너에서 업로드를 해야겠죠.\n",
      "[00:00:43.800 --> 00:00:46.700]   그래서 이렇게 방압 벽이 필요한 부분.\n",
      "[00:00:46.700 --> 00:00:49.400]   그 다음에 젠키스 접속할 때는\n",
      "[00:00:49.400 --> 00:00:53.600]   사내 이 사용자가 여기에 접속하기 위한 방압 벽을 신청하는 거예요.\n",
      "[00:00:53.600 --> 00:00:55.200]   그래서 사내의 어차피 이미 들어가 있겠죠.\n",
      "[00:00:55.200 --> 00:01:00.000]   사내빛이니까 IP죠. IP가 사내망에서 여기에다 접속하기 위한 방압 벽.\n",
      "[00:01:00.000 --> 00:01:03.400]   그다음에 마지막으로 이 rgcd에다가 접속하기 위한\n",
      "[00:01:03.400 --> 00:01:05.800]   프론텐드 ip로 방압 벽 접속이 필요하다.\n",
      "[00:01:05.800 --> 00:01:07.500]   해제가 필요하다.\n",
      "[00:01:07.500 --> 00:01:11.800]   그래서 그 과정을 제가 교육을 위해서는 다 미리 해놨기 때문에\n",
      "[00:01:11.800 --> 00:01:14.800]   여러분들이 알고 계셔야 되어서 말씀을 드렸고\n",
      "[00:01:14.800 --> 00:01:18.300]   이제 rgcd를 접속을 할 거예요.\n",
      "[00:01:18.300 --> 00:01:21.400]   접속을 할 때 저희가 아까 말씀드린 것처럼\n",
      "[00:01:21.400 --> 00:01:23.100]   DNS 고성을 안 해놨어요.\n",
      "[00:01:23.100 --> 00:01:24.900]   DNS 고성을 안 해놨기 때문에\n",
      "[00:01:24.900 --> 00:01:27.700]   사내 pc를 다 켜서 가지고\n",
      "[00:01:27.700 --> 00:01:30.900]   호스트 등록하는 방법 아시나요?\n",
      "[00:01:30.900 --> 00:01:34.000]   호스트 등록하는 방법 제가 설명해 드리겠습니다.\n",
      "[00:01:34.000 --> 00:01:35.700]   혹시 배터리 없어요?\n",
      "[00:01:35.700 --> 00:01:37.400]   충전 다 되어 있어요.\n",
      "[00:01:37.400 --> 00:01:39.900]   아 그래? 그럼 충전 좀 해야 되겠네.\n",
      "[00:01:55.900 --> 00:01:57.900]   감사합니다.\n",
      "\n",
      "✅ chunk_027 처리 완료 → ./Text/01/chunk_027_ggml-small.json\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "output_json: saving output to './Text/01/chunk_027_ggml-small.json'\n",
      "\n",
      "whisper_print_timings:     load time =   190.31 ms\n",
      "whisper_print_timings:     fallbacks =   1 p /   0 h\n",
      "whisper_print_timings:      mel time =    28.66 ms\n",
      "whisper_print_timings:   sample time =   415.36 ms /  2669 runs (     0.16 ms per run)\n",
      "whisper_print_timings:   encode time =   598.30 ms /     5 runs (   119.66 ms per run)\n",
      "whisper_print_timings:   decode time =     4.00 ms /     1 runs (     4.00 ms per run)\n",
      "whisper_print_timings:   batchd time =  2424.27 ms /  2641 runs (     0.92 ms per run)\n",
      "whisper_print_timings:   prompt time =   124.47 ms /  1065 runs (     0.12 ms per run)\n",
      "whisper_print_timings:    total time =  3866.00 ms\n",
      "ggml_metal_free: deallocating\n",
      "whisper_init_from_file_with_params_no_state: loading model from '/Users/aza1200/.cache/huggingface/hub/models--ggerganov--whisper.cpp/snapshots/5359861c739e955e79d9a303bcbc70fb988958b1/ggml-small.bin'\n",
      "whisper_init_with_params_no_state: use gpu    = 1\n",
      "whisper_init_with_params_no_state: flash attn = 0\n",
      "whisper_init_with_params_no_state: gpu_device = 0\n",
      "whisper_init_with_params_no_state: dtw        = 0\n",
      "whisper_init_with_params_no_state: devices    = 3\n",
      "whisper_init_with_params_no_state: backends   = 3\n",
      "whisper_model_load: loading model\n",
      "whisper_model_load: n_vocab       = 51865\n",
      "whisper_model_load: n_audio_ctx   = 1500\n",
      "whisper_model_load: n_audio_state = 768\n",
      "whisper_model_load: n_audio_head  = 12\n",
      "whisper_model_load: n_audio_layer = 12\n",
      "whisper_model_load: n_text_ctx    = 448\n",
      "whisper_model_load: n_text_state  = 768\n",
      "whisper_model_load: n_text_head   = 12\n",
      "whisper_model_load: n_text_layer  = 12\n",
      "whisper_model_load: n_mels        = 80\n",
      "whisper_model_load: ftype         = 1\n",
      "whisper_model_load: qntvr         = 0\n",
      "whisper_model_load: type          = 3 (small)\n",
      "whisper_model_load: adding 1608 extra tokens\n",
      "whisper_model_load: n_langs       = 99\n",
      "whisper_model_load:        Metal total size =   487.01 MB\n",
      "whisper_model_load: model size    =  487.01 MB\n",
      "whisper_backend_init_gpu: using Metal backend\n",
      "ggml_metal_init: allocating\n",
      "ggml_metal_init: found device: Apple M4 Pro\n",
      "ggml_metal_init: picking default device: Apple M4 Pro\n",
      "ggml_metal_load_library: using embedded metal library\n",
      "ggml_metal_init: GPU name:   Apple M4 Pro\n",
      "ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)\n",
      "ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)\n",
      "ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)\n",
      "ggml_metal_init: simdgroup reduction   = true\n",
      "ggml_metal_init: simdgroup matrix mul. = true\n",
      "ggml_metal_init: has residency sets    = true\n",
      "ggml_metal_init: has bfloat            = true\n",
      "ggml_metal_init: use bfloat            = false\n",
      "ggml_metal_init: hasUnifiedMemory      = true\n",
      "ggml_metal_init: recommendedMaxWorkingSetSize  = 38654.71 MB\n",
      "ggml_metal_init: skipping kernel_get_rows_bf16                     (not supported)\n",
      "ggml_metal_init: skipping kernel_mul_mv_bf16_f32                   (not supported)\n",
      "ggml_metal_init: skipping kernel_mul_mv_bf16_f32_1row              (not supported)\n",
      "ggml_metal_init: skipping kernel_mul_mv_bf16_f32_l4                (not supported)\n",
      "ggml_metal_init: skipping kernel_mul_mv_bf16_bf16                  (not supported)\n",
      "ggml_metal_init: skipping kernel_mul_mv_id_bf16_f32                (not supported)\n",
      "ggml_metal_init: skipping kernel_mul_mm_bf16_f32                   (not supported)\n",
      "ggml_metal_init: skipping kernel_mul_mm_id_bf16_f32                (not supported)\n",
      "ggml_metal_init: skipping kernel_flash_attn_ext_bf16_h64           (not supported)\n",
      "ggml_metal_init: skipping kernel_flash_attn_ext_bf16_h80           (not supported)\n",
      "ggml_metal_init: skipping kernel_flash_attn_ext_bf16_h96           (not supported)\n",
      "ggml_metal_init: skipping kernel_flash_attn_ext_bf16_h112          (not supported)\n",
      "ggml_metal_init: skipping kernel_flash_attn_ext_bf16_h128          (not supported)\n",
      "ggml_metal_init: skipping kernel_flash_attn_ext_bf16_h192          (not supported)\n",
      "ggml_metal_init: skipping kernel_flash_attn_ext_bf16_hk192_hv128   (not supported)\n",
      "ggml_metal_init: skipping kernel_flash_attn_ext_bf16_h256          (not supported)\n",
      "ggml_metal_init: skipping kernel_flash_attn_ext_vec_bf16_h128      (not supported)\n",
      "ggml_metal_init: skipping kernel_flash_attn_ext_vec_bf16_h192      (not supported)\n",
      "ggml_metal_init: skipping kernel_flash_attn_ext_vec_bf16_hk192_hv128 (not supported)\n",
      "ggml_metal_init: skipping kernel_flash_attn_ext_vec_bf16_h256      (not supported)\n",
      "ggml_metal_init: skipping kernel_cpy_f32_bf16                      (not supported)\n",
      "ggml_metal_init: skipping kernel_cpy_bf16_f32                      (not supported)\n",
      "ggml_metal_init: skipping kernel_cpy_bf16_bf16                     (not supported)\n",
      "whisper_backend_init: using BLAS backend\n",
      "whisper_init_state: kv self size  =   18.87 MB\n",
      "whisper_init_state: kv cross size =   56.62 MB\n",
      "whisper_init_state: kv pad  size  =    4.72 MB\n",
      "whisper_init_state: compute buffer (conv)   =   23.37 MB\n",
      "whisper_init_state: compute buffer (encode) =  128.01 MB\n",
      "whisper_init_state: compute buffer (cross)  =    6.18 MB\n",
      "whisper_init_state: compute buffer (decode) =   98.19 MB\n",
      "\n",
      "system_info: n_threads = 4 / 14 | WHISPER : COREML = 0 | OPENVINO = 0 | Metal : EMBED_LIBRARY = 1 | CPU : NEON = 1 | ARM_FMA = 1 | FP16_VA = 1 | MATMUL_INT8 = 1 | DOTPROD = 1 | SME = 1 | ACCELERATE = 1 | AARCH64_REPACK = 1 | \n",
      "\n",
      "main: processing './Sound/01/chunk_028.wav' (1920181 samples, 120.0 sec), 4 threads, 1 processors, 5 beams + best of 5, lang = ko, task = transcribe, timestamps = 1 ...\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "[00:00:00.000 --> 00:00:06.000]   [두 번째는 뷔의 뷔의 뷔]\n",
      "[00:00:31.000 --> 00:00:45.000]   [뷔의 뷔의 뷔의 뷔]\n",
      "[00:00:45.000 --> 00:01:03.000]   [뷔의 뷔의 뷔의 뷔]\n",
      "[00:01:03.000 --> 00:01:25.000]   [뷔의 뷔의 뷔의 뷔의 뷔]\n",
      "[00:01:25.000 --> 00:01:36.000]   [뷔의 뷔의 뷔의 뷔의 뷔의 뷔]\n",
      "[00:01:36.000 --> 00:02:00.000]   [뷔의 뷔의 뷔의 뷔의 뷔의 뷔의 뷔]\n",
      "\n",
      "✅ chunk_028 처리 완료 → ./Text/01/chunk_028_ggml-small.json\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "output_json: saving output to './Text/01/chunk_028_ggml-small.json'\n",
      "\n",
      "whisper_print_timings:     load time =   183.78 ms\n",
      "whisper_print_timings:     fallbacks =   3 p /   0 h\n",
      "whisper_print_timings:      mel time =    30.00 ms\n",
      "whisper_print_timings:   sample time =   790.85 ms /  3208 runs (     0.25 ms per run)\n",
      "whisper_print_timings:   encode time =   728.30 ms /     6 runs (   121.38 ms per run)\n",
      "whisper_print_timings:   decode time =  3830.59 ms /   994 runs (     3.85 ms per run)\n",
      "whisper_print_timings:   batchd time =  2417.18 ms /  2175 runs (     1.11 ms per run)\n",
      "whisper_print_timings:   prompt time =    86.10 ms /   561 runs (     0.15 ms per run)\n",
      "whisper_print_timings:    total time =  8149.30 ms\n",
      "ggml_metal_free: deallocating\n",
      "whisper_init_from_file_with_params_no_state: loading model from '/Users/aza1200/.cache/huggingface/hub/models--ggerganov--whisper.cpp/snapshots/5359861c739e955e79d9a303bcbc70fb988958b1/ggml-small.bin'\n",
      "whisper_init_with_params_no_state: use gpu    = 1\n",
      "whisper_init_with_params_no_state: flash attn = 0\n",
      "whisper_init_with_params_no_state: gpu_device = 0\n",
      "whisper_init_with_params_no_state: dtw        = 0\n",
      "whisper_init_with_params_no_state: devices    = 3\n",
      "whisper_init_with_params_no_state: backends   = 3\n",
      "whisper_model_load: loading model\n",
      "whisper_model_load: n_vocab       = 51865\n",
      "whisper_model_load: n_audio_ctx   = 1500\n",
      "whisper_model_load: n_audio_state = 768\n",
      "whisper_model_load: n_audio_head  = 12\n",
      "whisper_model_load: n_audio_layer = 12\n",
      "whisper_model_load: n_text_ctx    = 448\n",
      "whisper_model_load: n_text_state  = 768\n",
      "whisper_model_load: n_text_head   = 12\n",
      "whisper_model_load: n_text_layer  = 12\n",
      "whisper_model_load: n_mels        = 80\n",
      "whisper_model_load: ftype         = 1\n",
      "whisper_model_load: qntvr         = 0\n",
      "whisper_model_load: type          = 3 (small)\n",
      "whisper_model_load: adding 1608 extra tokens\n",
      "whisper_model_load: n_langs       = 99\n",
      "whisper_model_load:        Metal total size =   487.01 MB\n",
      "whisper_model_load: model size    =  487.01 MB\n",
      "whisper_backend_init_gpu: using Metal backend\n",
      "ggml_metal_init: allocating\n",
      "ggml_metal_init: found device: Apple M4 Pro\n",
      "ggml_metal_init: picking default device: Apple M4 Pro\n",
      "ggml_metal_load_library: using embedded metal library\n",
      "ggml_metal_init: GPU name:   Apple M4 Pro\n",
      "ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)\n",
      "ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)\n",
      "ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)\n",
      "ggml_metal_init: simdgroup reduction   = true\n",
      "ggml_metal_init: simdgroup matrix mul. = true\n",
      "ggml_metal_init: has residency sets    = true\n",
      "ggml_metal_init: has bfloat            = true\n",
      "ggml_metal_init: use bfloat            = false\n",
      "ggml_metal_init: hasUnifiedMemory      = true\n",
      "ggml_metal_init: recommendedMaxWorkingSetSize  = 38654.71 MB\n",
      "ggml_metal_init: skipping kernel_get_rows_bf16                     (not supported)\n",
      "ggml_metal_init: skipping kernel_mul_mv_bf16_f32                   (not supported)\n",
      "ggml_metal_init: skipping kernel_mul_mv_bf16_f32_1row              (not supported)\n",
      "ggml_metal_init: skipping kernel_mul_mv_bf16_f32_l4                (not supported)\n",
      "ggml_metal_init: skipping kernel_mul_mv_bf16_bf16                  (not supported)\n",
      "ggml_metal_init: skipping kernel_mul_mv_id_bf16_f32                (not supported)\n",
      "ggml_metal_init: skipping kernel_mul_mm_bf16_f32                   (not supported)\n",
      "ggml_metal_init: skipping kernel_mul_mm_id_bf16_f32                (not supported)\n",
      "ggml_metal_init: skipping kernel_flash_attn_ext_bf16_h64           (not supported)\n",
      "ggml_metal_init: skipping kernel_flash_attn_ext_bf16_h80           (not supported)\n",
      "ggml_metal_init: skipping kernel_flash_attn_ext_bf16_h96           (not supported)\n",
      "ggml_metal_init: skipping kernel_flash_attn_ext_bf16_h112          (not supported)\n",
      "ggml_metal_init: skipping kernel_flash_attn_ext_bf16_h128          (not supported)\n",
      "ggml_metal_init: skipping kernel_flash_attn_ext_bf16_h192          (not supported)\n",
      "ggml_metal_init: skipping kernel_flash_attn_ext_bf16_hk192_hv128   (not supported)\n",
      "ggml_metal_init: skipping kernel_flash_attn_ext_bf16_h256          (not supported)\n",
      "ggml_metal_init: skipping kernel_flash_attn_ext_vec_bf16_h128      (not supported)\n",
      "ggml_metal_init: skipping kernel_flash_attn_ext_vec_bf16_h192      (not supported)\n",
      "ggml_metal_init: skipping kernel_flash_attn_ext_vec_bf16_hk192_hv128 (not supported)\n",
      "ggml_metal_init: skipping kernel_flash_attn_ext_vec_bf16_h256      (not supported)\n",
      "ggml_metal_init: skipping kernel_cpy_f32_bf16                      (not supported)\n",
      "ggml_metal_init: skipping kernel_cpy_bf16_f32                      (not supported)\n",
      "ggml_metal_init: skipping kernel_cpy_bf16_bf16                     (not supported)\n",
      "whisper_backend_init: using BLAS backend\n",
      "whisper_init_state: kv self size  =   18.87 MB\n",
      "whisper_init_state: kv cross size =   56.62 MB\n",
      "whisper_init_state: kv pad  size  =    4.72 MB\n",
      "whisper_init_state: compute buffer (conv)   =   23.37 MB\n",
      "whisper_init_state: compute buffer (encode) =  128.01 MB\n",
      "whisper_init_state: compute buffer (cross)  =    6.18 MB\n",
      "whisper_init_state: compute buffer (decode) =   98.19 MB\n",
      "\n",
      "system_info: n_threads = 4 / 14 | WHISPER : COREML = 0 | OPENVINO = 0 | Metal : EMBED_LIBRARY = 1 | CPU : NEON = 1 | ARM_FMA = 1 | FP16_VA = 1 | MATMUL_INT8 = 1 | DOTPROD = 1 | SME = 1 | ACCELERATE = 1 | AARCH64_REPACK = 1 | \n",
      "\n",
      "main: processing './Sound/01/chunk_029.wav' (1919960 samples, 120.0 sec), 4 threads, 1 processors, 5 beams + best of 5, lang = ko, task = transcribe, timestamps = 1 ...\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "[00:00:00.000 --> 00:00:08.840]   그럼 배포라는 게 이제 그냥 뭔지에 대해서 간단하게 얘기해 드릴게요\n",
      "[00:00:08.840 --> 00:00:11.440]   그거에 형태는 굉장히 다양하구요\n",
      "[00:00:11.440 --> 00:00:18.800]   그냥 저번에 보니까 슬랩 보니까 개인 썩어 꼬리셔가지고 공유해주신 분들 계시고\n",
      "[00:00:18.800 --> 00:00:20.400]   제 이름은 잘 기억 안 나지만\n",
      "[00:00:20.400 --> 00:00:23.240]   그런 식으로 이제 온 프레임으로 배폴할 수도 있고요\n",
      "[00:00:23.240 --> 00:00:27.600]   데이터센터에 있는 썩어를 그대로 그냥 하드웨어 형태로 쓰는 걸 온 프레임이라고 하고\n",
      "[00:00:27.600 --> 00:00:33.960]   클라우드 형태에서 가장 머신 쓰는 VM, 아니면 컨테이너, 프로세스로 뛰어내는 컨테이너 배포\n",
      "[00:00:33.960 --> 00:00:38.520]   그다음에 코버네티스라고 하는 그런 컨테이너들을\n",
      "[00:00:38.520 --> 00:00:44.360]   파드 형태로 전체적인 관리를 해줄 수 있게 해주는 이걸 정확히는\n",
      "[00:00:44.360 --> 00:00:46.760]   오케스트레이션 툴이라고 하거든요 코버네티스를\n",
      "[00:00:46.760 --> 00:00:52.880]   그래서 거기에 대해서 가까이 배포를 할 수 있는 환경이에요 요즘에는\n",
      "[00:00:52.880 --> 00:00:58.640]   근데 요즘 MSA라든가 대규모 서비스를 뭐 이런 거 있죠\n",
      "[00:00:58.640 --> 00:01:04.360]   정말 서비스가 너무 다양해서 각각의 기능을 MSA로 구현하는 방식에는\n",
      "[00:01:04.360 --> 00:01:08.440]   모든 프로세스나 서비스를 수동으로 관리하기 어렵기 때문에\n",
      "[00:01:08.440 --> 00:01:11.680]   코버네티스에 많이 뛰고 있고 따라서\n",
      "[00:01:11.680 --> 00:01:14.840]   그것 때문에 이제 서비스 간의 의전성도 좀 높아지고\n",
      "[00:01:14.840 --> 00:01:17.560]   여러 가지 문제가 있지만 어쨌든 코버네티스는 좋은 툴입니다\n",
      "[00:01:17.560 --> 00:01:20.480]   요즘 워낙 몇 년 전부터 트렌디이기도 하고\n",
      "[00:01:20.480 --> 00:01:26.160]   여러분들도 잘 아셔서 힘이 가셔서 AKS를 다루실 가능성이 높기 때문에\n",
      "[00:01:26.160 --> 00:01:30.080]   잘 아시면 좋을 것 같고 각각의 장단점 이런 거 이제\n",
      "[00:01:30.080 --> 00:01:34.440]   그냥 간단하게 보시면 돼요 코버네티스 배포 뭐 매우 빠르다\n",
      "[00:01:34.440 --> 00:01:37.360]   왜냐하면은 그냥 파드 이런 퀘스트레이션 자체를\n",
      "[00:01:37.360 --> 00:01:41.080]   코버네티스가 다 잘 해주고 뭐 기도스 같은 툴도 잘 되어 있고\n",
      "[00:01:41.080 --> 00:01:44.160]   사람들은 그냥 야물 정의해서 아까 말씀 주신 것처럼\n",
      "[00:01:44.160 --> 00:01:48.800]   기탑 연동해서 그냥 버튼 딱강 누르면 배포 되는 수준이라서\n",
      "[00:01:48.800 --> 00:01:50.480]   정말 쉽죠\n",
      "[00:01:50.480 --> 00:01:52.000]   운영 난이도가 어려운 이유는\n",
      "[00:01:52.000 --> 00:01:54.300]   코버네티스라는 거 자체가 좀 이해하기 어려운\n",
      "[00:01:54.300 --> 00:01:56.640]   런닝커브가 있는 오픈 소스에요\n",
      "[00:01:56.640 --> 00:02:00.000]   그래서 그렇긴 한데 저희는 기본적으로\n",
      "\n",
      "✅ chunk_029 처리 완료 → ./Text/01/chunk_029_ggml-small.json\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "output_json: saving output to './Text/01/chunk_029_ggml-small.json'\n",
      "\n",
      "whisper_print_timings:     load time =   189.64 ms\n",
      "whisper_print_timings:     fallbacks =   0 p /   0 h\n",
      "whisper_print_timings:      mel time =    28.40 ms\n",
      "whisper_print_timings:   sample time =   473.56 ms /  2928 runs (     0.16 ms per run)\n",
      "whisper_print_timings:   encode time =   594.67 ms /     5 runs (   118.93 ms per run)\n",
      "whisper_print_timings:   decode time =    17.63 ms /     4 runs (     4.41 ms per run)\n",
      "whisper_print_timings:   batchd time =  2641.19 ms /  2902 runs (     0.91 ms per run)\n",
      "whisper_print_timings:   prompt time =    95.98 ms /   806 runs (     0.12 ms per run)\n",
      "whisper_print_timings:    total time =  4113.28 ms\n",
      "ggml_metal_free: deallocating\n",
      "whisper_init_from_file_with_params_no_state: loading model from '/Users/aza1200/.cache/huggingface/hub/models--ggerganov--whisper.cpp/snapshots/5359861c739e955e79d9a303bcbc70fb988958b1/ggml-small.bin'\n",
      "whisper_init_with_params_no_state: use gpu    = 1\n",
      "whisper_init_with_params_no_state: flash attn = 0\n",
      "whisper_init_with_params_no_state: gpu_device = 0\n",
      "whisper_init_with_params_no_state: dtw        = 0\n",
      "whisper_init_with_params_no_state: devices    = 3\n",
      "whisper_init_with_params_no_state: backends   = 3\n",
      "whisper_model_load: loading model\n",
      "whisper_model_load: n_vocab       = 51865\n",
      "whisper_model_load: n_audio_ctx   = 1500\n",
      "whisper_model_load: n_audio_state = 768\n",
      "whisper_model_load: n_audio_head  = 12\n",
      "whisper_model_load: n_audio_layer = 12\n",
      "whisper_model_load: n_text_ctx    = 448\n",
      "whisper_model_load: n_text_state  = 768\n",
      "whisper_model_load: n_text_head   = 12\n",
      "whisper_model_load: n_text_layer  = 12\n",
      "whisper_model_load: n_mels        = 80\n",
      "whisper_model_load: ftype         = 1\n",
      "whisper_model_load: qntvr         = 0\n",
      "whisper_model_load: type          = 3 (small)\n",
      "whisper_model_load: adding 1608 extra tokens\n",
      "whisper_model_load: n_langs       = 99\n",
      "whisper_model_load:        Metal total size =   487.01 MB\n",
      "whisper_model_load: model size    =  487.01 MB\n",
      "whisper_backend_init_gpu: using Metal backend\n",
      "ggml_metal_init: allocating\n",
      "ggml_metal_init: found device: Apple M4 Pro\n",
      "ggml_metal_init: picking default device: Apple M4 Pro\n",
      "ggml_metal_load_library: using embedded metal library\n",
      "ggml_metal_init: GPU name:   Apple M4 Pro\n",
      "ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)\n",
      "ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)\n",
      "ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)\n",
      "ggml_metal_init: simdgroup reduction   = true\n",
      "ggml_metal_init: simdgroup matrix mul. = true\n",
      "ggml_metal_init: has residency sets    = true\n",
      "ggml_metal_init: has bfloat            = true\n",
      "ggml_metal_init: use bfloat            = false\n",
      "ggml_metal_init: hasUnifiedMemory      = true\n",
      "ggml_metal_init: recommendedMaxWorkingSetSize  = 38654.71 MB\n",
      "ggml_metal_init: skipping kernel_get_rows_bf16                     (not supported)\n",
      "ggml_metal_init: skipping kernel_mul_mv_bf16_f32                   (not supported)\n",
      "ggml_metal_init: skipping kernel_mul_mv_bf16_f32_1row              (not supported)\n",
      "ggml_metal_init: skipping kernel_mul_mv_bf16_f32_l4                (not supported)\n",
      "ggml_metal_init: skipping kernel_mul_mv_bf16_bf16                  (not supported)\n",
      "ggml_metal_init: skipping kernel_mul_mv_id_bf16_f32                (not supported)\n",
      "ggml_metal_init: skipping kernel_mul_mm_bf16_f32                   (not supported)\n",
      "ggml_metal_init: skipping kernel_mul_mm_id_bf16_f32                (not supported)\n",
      "ggml_metal_init: skipping kernel_flash_attn_ext_bf16_h64           (not supported)\n",
      "ggml_metal_init: skipping kernel_flash_attn_ext_bf16_h80           (not supported)\n",
      "ggml_metal_init: skipping kernel_flash_attn_ext_bf16_h96           (not supported)\n",
      "ggml_metal_init: skipping kernel_flash_attn_ext_bf16_h112          (not supported)\n",
      "ggml_metal_init: skipping kernel_flash_attn_ext_bf16_h128          (not supported)\n",
      "ggml_metal_init: skipping kernel_flash_attn_ext_bf16_h192          (not supported)\n",
      "ggml_metal_init: skipping kernel_flash_attn_ext_bf16_hk192_hv128   (not supported)\n",
      "ggml_metal_init: skipping kernel_flash_attn_ext_bf16_h256          (not supported)\n",
      "ggml_metal_init: skipping kernel_flash_attn_ext_vec_bf16_h128      (not supported)\n",
      "ggml_metal_init: skipping kernel_flash_attn_ext_vec_bf16_h192      (not supported)\n",
      "ggml_metal_init: skipping kernel_flash_attn_ext_vec_bf16_hk192_hv128 (not supported)\n",
      "ggml_metal_init: skipping kernel_flash_attn_ext_vec_bf16_h256      (not supported)\n",
      "ggml_metal_init: skipping kernel_cpy_f32_bf16                      (not supported)\n",
      "ggml_metal_init: skipping kernel_cpy_bf16_f32                      (not supported)\n",
      "ggml_metal_init: skipping kernel_cpy_bf16_bf16                     (not supported)\n",
      "whisper_backend_init: using BLAS backend\n",
      "whisper_init_state: kv self size  =   18.87 MB\n",
      "whisper_init_state: kv cross size =   56.62 MB\n",
      "whisper_init_state: kv pad  size  =    4.72 MB\n",
      "whisper_init_state: compute buffer (conv)   =   23.37 MB\n",
      "whisper_init_state: compute buffer (encode) =  128.01 MB\n",
      "whisper_init_state: compute buffer (cross)  =    6.18 MB\n",
      "whisper_init_state: compute buffer (decode) =   98.19 MB\n",
      "\n",
      "system_info: n_threads = 4 / 14 | WHISPER : COREML = 0 | OPENVINO = 0 | Metal : EMBED_LIBRARY = 1 | CPU : NEON = 1 | ARM_FMA = 1 | FP16_VA = 1 | MATMUL_INT8 = 1 | DOTPROD = 1 | SME = 1 | ACCELERATE = 1 | AARCH64_REPACK = 1 | \n",
      "\n",
      "main: processing './Sound/01/chunk_030.wav' (1919943 samples, 120.0 sec), 4 threads, 1 processors, 5 beams + best of 5, lang = ko, task = transcribe, timestamps = 1 ...\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "[00:00:00.000 --> 00:00:06.200]   또 산회에서는 배포할 때 디플로이먼트랑 서비스랑 그 정도만 알고 계시면 됩니다\n",
      "[00:00:06.200 --> 00:00:14.900]   인그레스 해서 이 사용자들이 어떻게 엔트리 포인트로 들어와서 이 안에 이거를 분산시켜서 꾸려줄지 정도만 이해하시면 돼요\n",
      "[00:00:14.900 --> 00:00:17.940]   그 외에 여러 가지 그 오버젝트들이 있죠\n",
      "[00:00:17.940 --> 00:00:26.400]   뭐 데몬셋이라든가 스테이크 풀셋 뭐 이렇게 커버네티스의 파드를 다른 형태로 뛰어주는 것들은 쓰실 수는 있긴 하지만\n",
      "[00:00:26.400 --> 00:00:35.000]   사실 잘 쓰고 있는 팁이 있겠죠. 저는 잘 모릅니다. 그래서 뭐 기본적인 것만 이제 알고 계시면 됩니다\n",
      "[00:00:35.000 --> 00:00:42.600]   그리고 뭐 꾸버리티스 같은 경우에는 대규모 트래픽에 대응하기 좋다 롤백 이런거 간단하게 잘 되어 있고\n",
      "[00:00:42.600 --> 00:00:47.560]   이런 서비스들이나 파드에 대한 상태 자체를 꾸버리티스가 관리를 잘 해주기 때문에\n",
      "[00:00:47.560 --> 00:00:57.020]   저 좀 이렇게 운영하는데 좀 미련직스러운 것도 있어. 그리고 cicd 이미 기념성공주기 필요한 기업이나 대규모 서비스 대상으로 커버네티스를 많이 쓴다\n",
      "[00:00:57.020 --> 00:01:05.220]   뭐 이렇게 있다고 합니다. 컨테이너를 어제 그 질문 주신 분이 이제 vm에다가 그냥 컨테이너로 서비스 띄우면 안되냐 라고 하셨어요\n",
      "[00:01:05.220 --> 00:01:12.320]   근데 그것도 가능은 하죠. 가능은 한데 이 만약에 서비스 규모가 커져서 컨테이너의 개수가 여러 개가 됐다 했을 때\n",
      "[00:01:12.320 --> 00:01:19.920]   그렇게 되면은 vm 관리도 해야 되고 컨테이너 프로세스까지 관리를 해줘야 돼요. 그리고 컨테이너 프로세스 같은 경우에는\n",
      "[00:01:19.920 --> 00:01:27.920]   기본적으로 이제 휘발성 데이터가 휘발되는 이제 그런 특성이 있죠. 따라서 거기에 마음대로 되는 볼륨 이런 것도 다 관리를 해줘야 돼서\n",
      "[00:01:27.920 --> 00:01:36.320]   그런 경우에는 그냥 커버네티스를 쓴게 났습니다. 뭐 근데 이런 여러 가지 선택지가 장난점은 있지만 뭐가 좋고 나쁘다 라기보다는\n",
      "[00:01:36.320 --> 00:01:46.560]   본인의 서비스에 맞춰서 이제 사용하시면 될 것 같아요. 그리고 이제 저희가 기본적으로 지금까지 오늘 오전에 좀 vm 배포도구로 간단하게 젠킷을 썼죠.\n",
      "[00:01:46.560 --> 00:01:56.000]   젠킷으로 이제 대충 뭐가 교정되어 있는지까지 봤는데 이렇게 배포 자동화도구로 vm에다 배포하는 경우에 앤서블이라는 걸 많이 쓰고 있어요. 앤서블은\n",
      "[00:01:56.000 --> 00:02:00.080]   플레이북이라는 단위로 약간 이런거죠. 앤서블 하나\n",
      "\n",
      "✅ chunk_030 처리 완료 → ./Text/01/chunk_030_ggml-small.json\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "output_json: saving output to './Text/01/chunk_030_ggml-small.json'\n",
      "\n",
      "whisper_print_timings:     load time =   184.07 ms\n",
      "whisper_print_timings:     fallbacks =   0 p /   0 h\n",
      "whisper_print_timings:      mel time =    29.38 ms\n",
      "whisper_print_timings:   sample time =   491.82 ms /  3114 runs (     0.16 ms per run)\n",
      "whisper_print_timings:   encode time =   591.93 ms /     5 runs (   118.39 ms per run)\n",
      "whisper_print_timings:   decode time =     0.00 ms /     1 runs (     0.00 ms per run)\n",
      "whisper_print_timings:   batchd time =  2823.56 ms /  3092 runs (     0.91 ms per run)\n",
      "whisper_print_timings:   prompt time =    96.27 ms /   823 runs (     0.12 ms per run)\n",
      "whisper_print_timings:    total time =  4293.93 ms\n",
      "ggml_metal_free: deallocating\n",
      "whisper_init_from_file_with_params_no_state: loading model from '/Users/aza1200/.cache/huggingface/hub/models--ggerganov--whisper.cpp/snapshots/5359861c739e955e79d9a303bcbc70fb988958b1/ggml-small.bin'\n",
      "whisper_init_with_params_no_state: use gpu    = 1\n",
      "whisper_init_with_params_no_state: flash attn = 0\n",
      "whisper_init_with_params_no_state: gpu_device = 0\n",
      "whisper_init_with_params_no_state: dtw        = 0\n",
      "whisper_init_with_params_no_state: devices    = 3\n",
      "whisper_init_with_params_no_state: backends   = 3\n",
      "whisper_model_load: loading model\n",
      "whisper_model_load: n_vocab       = 51865\n",
      "whisper_model_load: n_audio_ctx   = 1500\n",
      "whisper_model_load: n_audio_state = 768\n",
      "whisper_model_load: n_audio_head  = 12\n",
      "whisper_model_load: n_audio_layer = 12\n",
      "whisper_model_load: n_text_ctx    = 448\n",
      "whisper_model_load: n_text_state  = 768\n",
      "whisper_model_load: n_text_head   = 12\n",
      "whisper_model_load: n_text_layer  = 12\n",
      "whisper_model_load: n_mels        = 80\n",
      "whisper_model_load: ftype         = 1\n",
      "whisper_model_load: qntvr         = 0\n",
      "whisper_model_load: type          = 3 (small)\n",
      "whisper_model_load: adding 1608 extra tokens\n",
      "whisper_model_load: n_langs       = 99\n",
      "whisper_model_load:        Metal total size =   487.01 MB\n",
      "whisper_model_load: model size    =  487.01 MB\n",
      "whisper_backend_init_gpu: using Metal backend\n",
      "ggml_metal_init: allocating\n",
      "ggml_metal_init: found device: Apple M4 Pro\n",
      "ggml_metal_init: picking default device: Apple M4 Pro\n",
      "ggml_metal_load_library: using embedded metal library\n",
      "ggml_metal_init: GPU name:   Apple M4 Pro\n",
      "ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)\n",
      "ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)\n",
      "ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)\n",
      "ggml_metal_init: simdgroup reduction   = true\n",
      "ggml_metal_init: simdgroup matrix mul. = true\n",
      "ggml_metal_init: has residency sets    = true\n",
      "ggml_metal_init: has bfloat            = true\n",
      "ggml_metal_init: use bfloat            = false\n",
      "ggml_metal_init: hasUnifiedMemory      = true\n",
      "ggml_metal_init: recommendedMaxWorkingSetSize  = 38654.71 MB\n",
      "ggml_metal_init: skipping kernel_get_rows_bf16                     (not supported)\n",
      "ggml_metal_init: skipping kernel_mul_mv_bf16_f32                   (not supported)\n",
      "ggml_metal_init: skipping kernel_mul_mv_bf16_f32_1row              (not supported)\n",
      "ggml_metal_init: skipping kernel_mul_mv_bf16_f32_l4                (not supported)\n",
      "ggml_metal_init: skipping kernel_mul_mv_bf16_bf16                  (not supported)\n",
      "ggml_metal_init: skipping kernel_mul_mv_id_bf16_f32                (not supported)\n",
      "ggml_metal_init: skipping kernel_mul_mm_bf16_f32                   (not supported)\n",
      "ggml_metal_init: skipping kernel_mul_mm_id_bf16_f32                (not supported)\n",
      "ggml_metal_init: skipping kernel_flash_attn_ext_bf16_h64           (not supported)\n",
      "ggml_metal_init: skipping kernel_flash_attn_ext_bf16_h80           (not supported)\n",
      "ggml_metal_init: skipping kernel_flash_attn_ext_bf16_h96           (not supported)\n",
      "ggml_metal_init: skipping kernel_flash_attn_ext_bf16_h112          (not supported)\n",
      "ggml_metal_init: skipping kernel_flash_attn_ext_bf16_h128          (not supported)\n",
      "ggml_metal_init: skipping kernel_flash_attn_ext_bf16_h192          (not supported)\n",
      "ggml_metal_init: skipping kernel_flash_attn_ext_bf16_hk192_hv128   (not supported)\n",
      "ggml_metal_init: skipping kernel_flash_attn_ext_bf16_h256          (not supported)\n",
      "ggml_metal_init: skipping kernel_flash_attn_ext_vec_bf16_h128      (not supported)\n",
      "ggml_metal_init: skipping kernel_flash_attn_ext_vec_bf16_h192      (not supported)\n",
      "ggml_metal_init: skipping kernel_flash_attn_ext_vec_bf16_hk192_hv128 (not supported)\n",
      "ggml_metal_init: skipping kernel_flash_attn_ext_vec_bf16_h256      (not supported)\n",
      "ggml_metal_init: skipping kernel_cpy_f32_bf16                      (not supported)\n",
      "ggml_metal_init: skipping kernel_cpy_bf16_f32                      (not supported)\n",
      "ggml_metal_init: skipping kernel_cpy_bf16_bf16                     (not supported)\n",
      "whisper_backend_init: using BLAS backend\n",
      "whisper_init_state: kv self size  =   18.87 MB\n",
      "whisper_init_state: kv cross size =   56.62 MB\n",
      "whisper_init_state: kv pad  size  =    4.72 MB\n",
      "whisper_init_state: compute buffer (conv)   =   23.37 MB\n",
      "whisper_init_state: compute buffer (encode) =  128.01 MB\n",
      "whisper_init_state: compute buffer (cross)  =    6.18 MB\n",
      "whisper_init_state: compute buffer (decode) =   98.19 MB\n",
      "\n",
      "system_info: n_threads = 4 / 14 | WHISPER : COREML = 0 | OPENVINO = 0 | Metal : EMBED_LIBRARY = 1 | CPU : NEON = 1 | ARM_FMA = 1 | FP16_VA = 1 | MATMUL_INT8 = 1 | DOTPROD = 1 | SME = 1 | ACCELERATE = 1 | AARCH64_REPACK = 1 | \n",
      "\n",
      "main: processing './Sound/01/chunk_031.wav' (1920154 samples, 120.0 sec), 4 threads, 1 processors, 5 beams + best of 5, lang = ko, task = transcribe, timestamps = 1 ...\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "[00:00:00.000 --> 00:00:06.800]   자다가 연동되어 있는 모든 VM에다가 한 번의 명령어를 때려서 안에 있는 OS 단 부터 해서\n",
      "[00:00:06.800 --> 00:00:15.360]   그 안에 있는 서버의 구성이라든가 그런 거를 한 번에 취할 수 있는 중앙화된 SSH 기반의 트리거든요.\n",
      "[00:00:15.360 --> 00:00:19.600]   이런 것도 있고, 오늘 다뤘던 진키스는 굉장히 유해가 잘 되어 있기 때문에\n",
      "[00:00:19.600 --> 00:00:23.360]   타입 프라인을 잘 짜면 앤선분처럼 잘 유용하게 쓸 수 있고요.\n",
      "[00:00:23.360 --> 00:00:28.560]   그 다음에 KT 앤섭프라이즈 환경에서는 저희가 기톱 액션으로 배포하지 않지만\n",
      "[00:00:28.560 --> 00:00:34.800]   그냥 이제 퍼블릿 환경에서는 기톱 액션으로도 배포할 수 있습니다. 이런 게 있고, 이해하시면 되고\n",
      "[00:00:34.800 --> 00:00:38.240]   컨테이너 같은 경우는 진키스로도 충분히 배폴할 수 있어요.\n",
      "[00:00:38.240 --> 00:00:46.080]   그래서 여러분들이 입사하시기 전에 제가 예전에 KT 내부의 개발자 플랫폼이라고 해서\n",
      "[00:00:46.080 --> 00:00:51.280]   데브스페이스라는 거 운영을 했는데 그때는 이제 여러분들의 애저에 배포하시겠지만\n",
      "[00:00:51.280 --> 00:00:56.080]   KT 클라우드라는 그룹사가 있죠. KT 클라우드는 데이터센터로 운영하고 있고\n",
      "[00:00:56.080 --> 00:01:03.200]   거기에 산회 KT 전사를 위한 IPC라고 해서 인터넷 커블릿 클라우드라고 해서\n",
      "[00:01:03.200 --> 00:01:08.240]   산회를 대상으로 하는 서비스가 있어요. 거기도 온크래미나 VM이 있고 커버렉티스가 있죠.\n",
      "[00:01:08.240 --> 00:01:15.840]   거기에다가 배포하는 툴로 댕킹세로 썼습니다. 그때 이게 사실 굉장히 조확한 방법인데\n",
      "[00:01:15.840 --> 00:01:19.840]   이때는 아르보 시즈를 도입하기 전이었고, 댕킹스로도 어쨌든 배포는 가능하다.\n",
      "[00:01:19.840 --> 00:01:24.000]   근데 사실 효율적이지 않습니다. 그렇게 이해하시면 될 것 같고\n",
      "[00:01:24.000 --> 00:01:27.760]   키터맥션으로도 배포 가능하고 오늘 아르보 시디 할 거죠.\n",
      "[00:01:27.760 --> 00:01:32.000]   그 다음에 택톤이라는 오픈소스도 사실 좀 많이 요즘 많이 쓰고 유용하기 때문에\n",
      "[00:01:32.000 --> 00:01:35.040]   이거 만에 관심 있으시면 굉장히 좋은 추리예요.\n",
      "[00:01:35.040 --> 00:01:38.560]   그래서 택톤도 자동화가 잘 되어 있기 때문에 이런 거 한번 생각해보시고\n",
      "[00:01:38.560 --> 00:01:42.480]   헬륨이나 커스텀 아이스도로도 배포가 가능합니다.\n",
      "[00:01:42.480 --> 00:01:47.200]   이제 근데 저희가 커버렉티스에다가 배포를 할 때 좀\n",
      "[00:01:47.200 --> 00:01:52.160]   특히 아르보 시디를 쓸 때 저희가 가져가는 좀 큰 장점이\n",
      "[00:01:52.160 --> 00:01:56.560]   대규모 서비스를 배포할 때 서비스가 다운되면 안 되잖아요.\n",
      "[00:01:56.560 --> 00:01:59.040]   그러니까 말 그대로 장애가 발생하면 안 되잖아요.\n",
      "[00:01:59.040 --> 00:02:02.400]   이때 컨테이너 무섭죠.\n",
      "\n",
      "✅ chunk_031 처리 완료 → ./Text/01/chunk_031_ggml-small.json\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "output_json: saving output to './Text/01/chunk_031_ggml-small.json'\n",
      "\n",
      "whisper_print_timings:     load time =   166.86 ms\n",
      "whisper_print_timings:     fallbacks =   0 p /   0 h\n",
      "whisper_print_timings:      mel time =    28.54 ms\n",
      "whisper_print_timings:   sample time =   496.64 ms /  3173 runs (     0.16 ms per run)\n",
      "whisper_print_timings:   encode time =   584.74 ms /     5 runs (   116.95 ms per run)\n",
      "whisper_print_timings:   decode time =    12.54 ms /     3 runs (     4.18 ms per run)\n",
      "whisper_print_timings:   batchd time =  2895.70 ms /  3148 runs (     0.92 ms per run)\n",
      "whisper_print_timings:   prompt time =    97.75 ms /   831 runs (     0.12 ms per run)\n",
      "whisper_print_timings:    total time =  4355.31 ms\n",
      "ggml_metal_free: deallocating\n",
      "whisper_init_from_file_with_params_no_state: loading model from '/Users/aza1200/.cache/huggingface/hub/models--ggerganov--whisper.cpp/snapshots/5359861c739e955e79d9a303bcbc70fb988958b1/ggml-small.bin'\n",
      "whisper_init_with_params_no_state: use gpu    = 1\n",
      "whisper_init_with_params_no_state: flash attn = 0\n",
      "whisper_init_with_params_no_state: gpu_device = 0\n",
      "whisper_init_with_params_no_state: dtw        = 0\n",
      "whisper_init_with_params_no_state: devices    = 3\n",
      "whisper_init_with_params_no_state: backends   = 3\n",
      "whisper_model_load: loading model\n",
      "whisper_model_load: n_vocab       = 51865\n",
      "whisper_model_load: n_audio_ctx   = 1500\n",
      "whisper_model_load: n_audio_state = 768\n",
      "whisper_model_load: n_audio_head  = 12\n",
      "whisper_model_load: n_audio_layer = 12\n",
      "whisper_model_load: n_text_ctx    = 448\n",
      "whisper_model_load: n_text_state  = 768\n",
      "whisper_model_load: n_text_head   = 12\n",
      "whisper_model_load: n_text_layer  = 12\n",
      "whisper_model_load: n_mels        = 80\n",
      "whisper_model_load: ftype         = 1\n",
      "whisper_model_load: qntvr         = 0\n",
      "whisper_model_load: type          = 3 (small)\n",
      "whisper_model_load: adding 1608 extra tokens\n",
      "whisper_model_load: n_langs       = 99\n",
      "whisper_model_load:        Metal total size =   487.01 MB\n",
      "whisper_model_load: model size    =  487.01 MB\n",
      "whisper_backend_init_gpu: using Metal backend\n",
      "ggml_metal_init: allocating\n",
      "ggml_metal_init: found device: Apple M4 Pro\n",
      "ggml_metal_init: picking default device: Apple M4 Pro\n",
      "ggml_metal_load_library: using embedded metal library\n",
      "ggml_metal_init: GPU name:   Apple M4 Pro\n",
      "ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)\n",
      "ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)\n",
      "ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)\n",
      "ggml_metal_init: simdgroup reduction   = true\n",
      "ggml_metal_init: simdgroup matrix mul. = true\n",
      "ggml_metal_init: has residency sets    = true\n",
      "ggml_metal_init: has bfloat            = true\n",
      "ggml_metal_init: use bfloat            = false\n",
      "ggml_metal_init: hasUnifiedMemory      = true\n",
      "ggml_metal_init: recommendedMaxWorkingSetSize  = 38654.71 MB\n",
      "ggml_metal_init: skipping kernel_get_rows_bf16                     (not supported)\n",
      "ggml_metal_init: skipping kernel_mul_mv_bf16_f32                   (not supported)\n",
      "ggml_metal_init: skipping kernel_mul_mv_bf16_f32_1row              (not supported)\n",
      "ggml_metal_init: skipping kernel_mul_mv_bf16_f32_l4                (not supported)\n",
      "ggml_metal_init: skipping kernel_mul_mv_bf16_bf16                  (not supported)\n",
      "ggml_metal_init: skipping kernel_mul_mv_id_bf16_f32                (not supported)\n",
      "ggml_metal_init: skipping kernel_mul_mm_bf16_f32                   (not supported)\n",
      "ggml_metal_init: skipping kernel_mul_mm_id_bf16_f32                (not supported)\n",
      "ggml_metal_init: skipping kernel_flash_attn_ext_bf16_h64           (not supported)\n",
      "ggml_metal_init: skipping kernel_flash_attn_ext_bf16_h80           (not supported)\n",
      "ggml_metal_init: skipping kernel_flash_attn_ext_bf16_h96           (not supported)\n",
      "ggml_metal_init: skipping kernel_flash_attn_ext_bf16_h112          (not supported)\n",
      "ggml_metal_init: skipping kernel_flash_attn_ext_bf16_h128          (not supported)\n",
      "ggml_metal_init: skipping kernel_flash_attn_ext_bf16_h192          (not supported)\n",
      "ggml_metal_init: skipping kernel_flash_attn_ext_bf16_hk192_hv128   (not supported)\n",
      "ggml_metal_init: skipping kernel_flash_attn_ext_bf16_h256          (not supported)\n",
      "ggml_metal_init: skipping kernel_flash_attn_ext_vec_bf16_h128      (not supported)\n",
      "ggml_metal_init: skipping kernel_flash_attn_ext_vec_bf16_h192      (not supported)\n",
      "ggml_metal_init: skipping kernel_flash_attn_ext_vec_bf16_hk192_hv128 (not supported)\n",
      "ggml_metal_init: skipping kernel_flash_attn_ext_vec_bf16_h256      (not supported)\n",
      "ggml_metal_init: skipping kernel_cpy_f32_bf16                      (not supported)\n",
      "ggml_metal_init: skipping kernel_cpy_bf16_f32                      (not supported)\n",
      "ggml_metal_init: skipping kernel_cpy_bf16_bf16                     (not supported)\n",
      "whisper_backend_init: using BLAS backend\n",
      "whisper_init_state: kv self size  =   18.87 MB\n",
      "whisper_init_state: kv cross size =   56.62 MB\n",
      "whisper_init_state: kv pad  size  =    4.72 MB\n",
      "whisper_init_state: compute buffer (conv)   =   23.37 MB\n",
      "whisper_init_state: compute buffer (encode) =  128.01 MB\n",
      "whisper_init_state: compute buffer (cross)  =    6.18 MB\n",
      "whisper_init_state: compute buffer (decode) =   98.19 MB\n",
      "\n",
      "system_info: n_threads = 4 / 14 | WHISPER : COREML = 0 | OPENVINO = 0 | Metal : EMBED_LIBRARY = 1 | CPU : NEON = 1 | ARM_FMA = 1 | FP16_VA = 1 | MATMUL_INT8 = 1 | DOTPROD = 1 | SME = 1 | ACCELERATE = 1 | AARCH64_REPACK = 1 | \n",
      "\n",
      "main: processing './Sound/01/chunk_032.wav' (1920127 samples, 120.0 sec), 4 threads, 1 processors, 5 beams + best of 5, lang = ko, task = transcribe, timestamps = 1 ...\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "[00:00:00.000 --> 00:00:02.480]   무중단 배포를 위해서 기독스를 많이 쓰는 경우가 있어요.\n",
      "[00:00:02.480 --> 00:00:06.180]   아르고시디를 저런 자동화된 초를 쓰는데\n",
      "[00:00:06.180 --> 00:00:09.820]   이 무중단 배포를 하는 옵션 중에 여러분이\n",
      "[00:00:09.820 --> 00:00:11.920]   이론적으로 알고 있겠지만 크게 세 가지 옵션이 있습니다.\n",
      "[00:00:11.920 --> 00:00:15.620]   첫 번째로 아르고시디를 롤링 업데이트를 기본적으로 가지고 있어요.\n",
      "[00:00:15.620 --> 00:00:20.240]   롤링 업데이트라고 하면 배포가 이미 되어 있는 버전1이랑\n",
      "[00:00:20.240 --> 00:00:22.280]   새롭게 TU를 버전2가 있죠.\n",
      "[00:00:22.280 --> 00:00:28.460]   그러면 버전2로 이제 모든 배포로 이제 다 업데이트를 해야 되는데\n",
      "[00:00:28.460 --> 00:00:34.720]   버전2로 업데이트하기 전에 V2를 먼저 뛰어넣고 V1을 죽이고\n",
      "[00:00:34.720 --> 00:00:37.360]   그 다음에 파드가 만약에 3개라고 한다면\n",
      "[00:00:37.360 --> 00:00:40.680]   그 다음에 V2를 뛰어넣고 V1을 죽이고\n",
      "[00:00:40.680 --> 00:00:44.200]   이런 식으로 하나씩 교체하는 배포 방식이에요.\n",
      "[00:00:44.200 --> 00:00:49.600]   이렇게 되면 기본적으로 배포하는데\n",
      "[00:00:49.600 --> 00:00:52.880]   첫 번째로 리소스가 크게 차이는지 않겠죠.\n",
      "[00:00:52.880 --> 00:00:59.840]   그러니까 만약에 AKS 안에 '노드풀'이라고 하는 서버라고 할게요.\n",
      "[00:00:59.840 --> 00:01:02.980]   서버가 10개가 있고 거기에 파드가 100개가 떠 있다.\n",
      "[00:01:02.980 --> 00:01:08.460]   근데 이 버전1의 파드 100개를 다 버전초로 바꾸려고 하면\n",
      "[00:01:08.460 --> 00:01:13.540]   블러블링 같은 경우에는 똑같이 노드 10개의 파드 100개\n",
      "[00:01:13.540 --> 00:01:15.420]   또 노드 10개의 파드 100개를 뛰어서\n",
      "[00:01:15.420 --> 00:01:17.700]   이거를 로드 밸런싱으로 바꿔주는 형식이거든요.\n",
      "[00:01:17.700 --> 00:01:19.620]   그렇게 되면 리소스가 2배가 필요하죠.\n",
      "[00:01:19.620 --> 00:01:21.700]   왜냐하면 V2까지 준비해놔야 되니까.\n",
      "[00:01:21.700 --> 00:01:25.340]   롤링 업데이트는 그러지 않고 하나씩 교체하는 방식으로\n",
      "[00:01:25.340 --> 00:01:29.120]   하나의 리소스에서도 사용 가능하다.\n",
      "[00:01:29.120 --> 00:01:32.220]   그런 비용적인 장점이 있고요.\n",
      "[00:01:32.220 --> 00:01:34.340]   다만 단점으로는 업데이트 과정 중에\n",
      "[00:01:34.340 --> 00:01:37.120]   파드의 일부가 새로운 버전으로 운영돼서\n",
      "[00:01:37.120 --> 00:01:40.520]   버전과 호환성 문제가 발생 가능하다는 문제가 있습니다.\n",
      "[00:01:40.520 --> 00:01:43.440]   따라서 MSA 같이 복잡한 서비스에서는\n",
      "[00:01:43.440 --> 00:01:46.760]   롤링 업데이트를 쓸 때\n",
      "[00:01:46.760 --> 00:01:50.440]   서비스 간의 호환성 문제를 좀 생각을 하셔야 돼요.\n",
      "[00:01:50.440 --> 00:01:55.020]   특히 예를 들어 만약에 쇼핑 서비스를 개발한다\n",
      "[00:01:55.020 --> 00:01:57.500]   예를 들어 네이버 쇼핑 같은 거 있죠.\n",
      "[00:01:57.500 --> 00:01:59.780]   그러면 사용자가 로그인하는 진입점부터\n",
      "\n",
      "✅ chunk_032 처리 완료 → ./Text/01/chunk_032_ggml-small.json\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "output_json: saving output to './Text/01/chunk_032_ggml-small.json'\n",
      "\n",
      "whisper_print_timings:     load time =   180.08 ms\n",
      "whisper_print_timings:     fallbacks =   0 p /   0 h\n",
      "whisper_print_timings:      mel time =    28.99 ms\n",
      "whisper_print_timings:   sample time =   446.88 ms /  2942 runs (     0.15 ms per run)\n",
      "whisper_print_timings:   encode time =   585.62 ms /     5 runs (   117.12 ms per run)\n",
      "whisper_print_timings:   decode time =     8.81 ms /     2 runs (     4.40 ms per run)\n",
      "whisper_print_timings:   batchd time =  2637.02 ms /  2918 runs (     0.90 ms per run)\n",
      "whisper_print_timings:   prompt time =    99.04 ms /   843 runs (     0.12 ms per run)\n",
      "whisper_print_timings:    total time =  4062.87 ms\n",
      "ggml_metal_free: deallocating\n",
      "whisper_init_from_file_with_params_no_state: loading model from '/Users/aza1200/.cache/huggingface/hub/models--ggerganov--whisper.cpp/snapshots/5359861c739e955e79d9a303bcbc70fb988958b1/ggml-small.bin'\n",
      "whisper_init_with_params_no_state: use gpu    = 1\n",
      "whisper_init_with_params_no_state: flash attn = 0\n",
      "whisper_init_with_params_no_state: gpu_device = 0\n",
      "whisper_init_with_params_no_state: dtw        = 0\n",
      "whisper_init_with_params_no_state: devices    = 3\n",
      "whisper_init_with_params_no_state: backends   = 3\n",
      "whisper_model_load: loading model\n",
      "whisper_model_load: n_vocab       = 51865\n",
      "whisper_model_load: n_audio_ctx   = 1500\n",
      "whisper_model_load: n_audio_state = 768\n",
      "whisper_model_load: n_audio_head  = 12\n",
      "whisper_model_load: n_audio_layer = 12\n",
      "whisper_model_load: n_text_ctx    = 448\n",
      "whisper_model_load: n_text_state  = 768\n",
      "whisper_model_load: n_text_head   = 12\n",
      "whisper_model_load: n_text_layer  = 12\n",
      "whisper_model_load: n_mels        = 80\n",
      "whisper_model_load: ftype         = 1\n",
      "whisper_model_load: qntvr         = 0\n",
      "whisper_model_load: type          = 3 (small)\n",
      "whisper_model_load: adding 1608 extra tokens\n",
      "whisper_model_load: n_langs       = 99\n",
      "whisper_model_load:        Metal total size =   487.01 MB\n",
      "whisper_model_load: model size    =  487.01 MB\n",
      "whisper_backend_init_gpu: using Metal backend\n",
      "ggml_metal_init: allocating\n",
      "ggml_metal_init: found device: Apple M4 Pro\n",
      "ggml_metal_init: picking default device: Apple M4 Pro\n",
      "ggml_metal_load_library: using embedded metal library\n",
      "ggml_metal_init: GPU name:   Apple M4 Pro\n",
      "ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)\n",
      "ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)\n",
      "ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)\n",
      "ggml_metal_init: simdgroup reduction   = true\n",
      "ggml_metal_init: simdgroup matrix mul. = true\n",
      "ggml_metal_init: has residency sets    = true\n",
      "ggml_metal_init: has bfloat            = true\n",
      "ggml_metal_init: use bfloat            = false\n",
      "ggml_metal_init: hasUnifiedMemory      = true\n",
      "ggml_metal_init: recommendedMaxWorkingSetSize  = 38654.71 MB\n",
      "ggml_metal_init: skipping kernel_get_rows_bf16                     (not supported)\n",
      "ggml_metal_init: skipping kernel_mul_mv_bf16_f32                   (not supported)\n",
      "ggml_metal_init: skipping kernel_mul_mv_bf16_f32_1row              (not supported)\n",
      "ggml_metal_init: skipping kernel_mul_mv_bf16_f32_l4                (not supported)\n",
      "ggml_metal_init: skipping kernel_mul_mv_bf16_bf16                  (not supported)\n",
      "ggml_metal_init: skipping kernel_mul_mv_id_bf16_f32                (not supported)\n",
      "ggml_metal_init: skipping kernel_mul_mm_bf16_f32                   (not supported)\n",
      "ggml_metal_init: skipping kernel_mul_mm_id_bf16_f32                (not supported)\n",
      "ggml_metal_init: skipping kernel_flash_attn_ext_bf16_h64           (not supported)\n",
      "ggml_metal_init: skipping kernel_flash_attn_ext_bf16_h80           (not supported)\n",
      "ggml_metal_init: skipping kernel_flash_attn_ext_bf16_h96           (not supported)\n",
      "ggml_metal_init: skipping kernel_flash_attn_ext_bf16_h112          (not supported)\n",
      "ggml_metal_init: skipping kernel_flash_attn_ext_bf16_h128          (not supported)\n",
      "ggml_metal_init: skipping kernel_flash_attn_ext_bf16_h192          (not supported)\n",
      "ggml_metal_init: skipping kernel_flash_attn_ext_bf16_hk192_hv128   (not supported)\n",
      "ggml_metal_init: skipping kernel_flash_attn_ext_bf16_h256          (not supported)\n",
      "ggml_metal_init: skipping kernel_flash_attn_ext_vec_bf16_h128      (not supported)\n",
      "ggml_metal_init: skipping kernel_flash_attn_ext_vec_bf16_h192      (not supported)\n",
      "ggml_metal_init: skipping kernel_flash_attn_ext_vec_bf16_hk192_hv128 (not supported)\n",
      "ggml_metal_init: skipping kernel_flash_attn_ext_vec_bf16_h256      (not supported)\n",
      "ggml_metal_init: skipping kernel_cpy_f32_bf16                      (not supported)\n",
      "ggml_metal_init: skipping kernel_cpy_bf16_f32                      (not supported)\n",
      "ggml_metal_init: skipping kernel_cpy_bf16_bf16                     (not supported)\n",
      "whisper_backend_init: using BLAS backend\n",
      "whisper_init_state: kv self size  =   18.87 MB\n",
      "whisper_init_state: kv cross size =   56.62 MB\n",
      "whisper_init_state: kv pad  size  =    4.72 MB\n",
      "whisper_init_state: compute buffer (conv)   =   23.37 MB\n",
      "whisper_init_state: compute buffer (encode) =  128.01 MB\n",
      "whisper_init_state: compute buffer (cross)  =    6.18 MB\n",
      "whisper_init_state: compute buffer (decode) =   98.19 MB\n",
      "\n",
      "system_info: n_threads = 4 / 14 | WHISPER : COREML = 0 | OPENVINO = 0 | Metal : EMBED_LIBRARY = 1 | CPU : NEON = 1 | ARM_FMA = 1 | FP16_VA = 1 | MATMUL_INT8 = 1 | DOTPROD = 1 | SME = 1 | ACCELERATE = 1 | AARCH64_REPACK = 1 | \n",
      "\n",
      "main: processing './Sound/01/chunk_033.wav' (1919988 samples, 120.0 sec), 4 threads, 1 processors, 5 beams + best of 5, lang = ko, task = transcribe, timestamps = 1 ...\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "[00:00:00.000 --> 00:00:06.980]   단순하게 로그인 서비스, 상품을 고르는 서비스,\n",
      "[00:00:06.980 --> 00:00:11.280]   예를 저장하는 서비스, 결제하는 서비스, 배송하는 서비스,\n",
      "[00:00:11.280 --> 00:00:15.120]   반품하는 서비스, 배송 완료하는 서비스, 이렇게 다양할 수 있잖아요.\n",
      "[00:00:15.120 --> 00:00:20.720]   그러면 모든 파드들이 다 트레이싱처럼 연결되어 있을 건데\n",
      "[00:00:20.720 --> 00:00:23.900]   만약에 중간에 있는 파드부터 업데이트가 됐다,\n",
      "[00:00:23.900 --> 00:00:27.780]   그러면 그 앞 파드들과 그 뒷 파드들이 호환이 안 될 수도 있잖아요.\n",
      "[00:00:27.780 --> 00:00:35.060]   따라서 이런 서비스별로 이렇게 업데이트하는 그런 순서도 좀 고민을 해봐야 되는 포인트입니다.\n",
      "[00:00:35.060 --> 00:00:41.080]   어쨌든 뭐 그런 복자성이 있지만 어쨌든 아라보 시기는 기본적으로 롤링 업데이트를 지원한다.\n",
      "[00:00:41.080 --> 00:00:48.920]   그 다음에 블루고리는 아까 말씀드린 것처럼 버전1과 버전2를 한 번에 로드 밸런싱을 해서 스위치를 해주는 형태예요.\n",
      "[00:00:48.920 --> 00:00:57.420]   아까 말씀드린 것처럼 단점은 버전2로 바꿔주기 위한 인플 하나 여러 가지 이해를 받쳐줄 리소스들이 이미 준비가 되어 있어야 합니다.\n",
      "[00:00:57.420 --> 00:01:02.860]   다만 장점은 버전1처럼 이미 다 업데이트된 애들을 한 번에 바꿔주기 때문에\n",
      "[00:01:02.860 --> 00:01:10.860]   사용자가 이 서비스를 쓸 때도 어떤 사용자는 버전1을 계속 경험하고 또 어떤 사용자는 버전2를 경험하고 이렇게\n",
      "[00:01:10.860 --> 00:01:18.300]   사용자 경험상에서의 차이가 없겠죠. 한 번에 버전2를 쓰게 되니까 이런 장점이 있으나 리소스가 두 메인 다점이 있다.\n",
      "[00:01:18.300 --> 00:01:29.800]   바다리 같은 경우는 이제 전체 트래픽의 일부만 새로운 버전으로 라우팅해서 그 일부들이 검증이 됐을 때 전체적으로 이제 바꾸는 점진적으로 바꾸는 경우인데요.\n",
      "[00:01:29.800 --> 00:01:37.520]   이것도 이제 사실 블루고리인과 롤링 업데이트 약간 좀 롤링 업데이트에 가까운 방식이지만\n",
      "[00:01:37.520 --> 00:01:45.600]   저는 오히려 이게 조금 더 조심스럽게 배포하기에는 좋은 방식이라 생각하고 다만 단점으로는 만약에 비율 설정을 잘못했을 때\n",
      "[00:01:45.600 --> 00:01:55.600]   새로운 버전의 문제를 충분히 검증하지 못해서 전체 서비스에 문제가 될 수 있다. 그러니까 보수적인 비율들이 충분히 검증이 되었을 때 전체의 배포하는 방식이 좋겠다.\n",
      "[00:01:55.600 --> 00:02:01.000]   이런 식으로 서비스 특히 AKS나 커버넘 키스에 올라가는 서비스\n",
      "\n",
      "✅ chunk_033 처리 완료 → ./Text/01/chunk_033_ggml-small.json\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "output_json: saving output to './Text/01/chunk_033_ggml-small.json'\n",
      "\n",
      "whisper_print_timings:     load time =   179.68 ms\n",
      "whisper_print_timings:     fallbacks =   0 p /   0 h\n",
      "whisper_print_timings:      mel time =    28.79 ms\n",
      "whisper_print_timings:   sample time =   459.85 ms /  2951 runs (     0.16 ms per run)\n",
      "whisper_print_timings:   encode time =   592.51 ms /     5 runs (   118.50 ms per run)\n",
      "whisper_print_timings:   decode time =     8.62 ms /     2 runs (     4.31 ms per run)\n",
      "whisper_print_timings:   batchd time =  2658.00 ms /  2927 runs (     0.91 ms per run)\n",
      "whisper_print_timings:   prompt time =    96.19 ms /   821 runs (     0.12 ms per run)\n",
      "whisper_print_timings:    total time =  4099.02 ms\n",
      "ggml_metal_free: deallocating\n",
      "whisper_init_from_file_with_params_no_state: loading model from '/Users/aza1200/.cache/huggingface/hub/models--ggerganov--whisper.cpp/snapshots/5359861c739e955e79d9a303bcbc70fb988958b1/ggml-small.bin'\n",
      "whisper_init_with_params_no_state: use gpu    = 1\n",
      "whisper_init_with_params_no_state: flash attn = 0\n",
      "whisper_init_with_params_no_state: gpu_device = 0\n",
      "whisper_init_with_params_no_state: dtw        = 0\n",
      "whisper_init_with_params_no_state: devices    = 3\n",
      "whisper_init_with_params_no_state: backends   = 3\n",
      "whisper_model_load: loading model\n",
      "whisper_model_load: n_vocab       = 51865\n",
      "whisper_model_load: n_audio_ctx   = 1500\n",
      "whisper_model_load: n_audio_state = 768\n",
      "whisper_model_load: n_audio_head  = 12\n",
      "whisper_model_load: n_audio_layer = 12\n",
      "whisper_model_load: n_text_ctx    = 448\n",
      "whisper_model_load: n_text_state  = 768\n",
      "whisper_model_load: n_text_head   = 12\n",
      "whisper_model_load: n_text_layer  = 12\n",
      "whisper_model_load: n_mels        = 80\n",
      "whisper_model_load: ftype         = 1\n",
      "whisper_model_load: qntvr         = 0\n",
      "whisper_model_load: type          = 3 (small)\n",
      "whisper_model_load: adding 1608 extra tokens\n",
      "whisper_model_load: n_langs       = 99\n",
      "whisper_model_load:        Metal total size =   487.01 MB\n",
      "whisper_model_load: model size    =  487.01 MB\n",
      "whisper_backend_init_gpu: using Metal backend\n",
      "ggml_metal_init: allocating\n",
      "ggml_metal_init: found device: Apple M4 Pro\n",
      "ggml_metal_init: picking default device: Apple M4 Pro\n",
      "ggml_metal_load_library: using embedded metal library\n",
      "ggml_metal_init: GPU name:   Apple M4 Pro\n",
      "ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)\n",
      "ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)\n",
      "ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)\n",
      "ggml_metal_init: simdgroup reduction   = true\n",
      "ggml_metal_init: simdgroup matrix mul. = true\n",
      "ggml_metal_init: has residency sets    = true\n",
      "ggml_metal_init: has bfloat            = true\n",
      "ggml_metal_init: use bfloat            = false\n",
      "ggml_metal_init: hasUnifiedMemory      = true\n",
      "ggml_metal_init: recommendedMaxWorkingSetSize  = 38654.71 MB\n",
      "ggml_metal_init: skipping kernel_get_rows_bf16                     (not supported)\n",
      "ggml_metal_init: skipping kernel_mul_mv_bf16_f32                   (not supported)\n",
      "ggml_metal_init: skipping kernel_mul_mv_bf16_f32_1row              (not supported)\n",
      "ggml_metal_init: skipping kernel_mul_mv_bf16_f32_l4                (not supported)\n",
      "ggml_metal_init: skipping kernel_mul_mv_bf16_bf16                  (not supported)\n",
      "ggml_metal_init: skipping kernel_mul_mv_id_bf16_f32                (not supported)\n",
      "ggml_metal_init: skipping kernel_mul_mm_bf16_f32                   (not supported)\n",
      "ggml_metal_init: skipping kernel_mul_mm_id_bf16_f32                (not supported)\n",
      "ggml_metal_init: skipping kernel_flash_attn_ext_bf16_h64           (not supported)\n",
      "ggml_metal_init: skipping kernel_flash_attn_ext_bf16_h80           (not supported)\n",
      "ggml_metal_init: skipping kernel_flash_attn_ext_bf16_h96           (not supported)\n",
      "ggml_metal_init: skipping kernel_flash_attn_ext_bf16_h112          (not supported)\n",
      "ggml_metal_init: skipping kernel_flash_attn_ext_bf16_h128          (not supported)\n",
      "ggml_metal_init: skipping kernel_flash_attn_ext_bf16_h192          (not supported)\n",
      "ggml_metal_init: skipping kernel_flash_attn_ext_bf16_hk192_hv128   (not supported)\n",
      "ggml_metal_init: skipping kernel_flash_attn_ext_bf16_h256          (not supported)\n",
      "ggml_metal_init: skipping kernel_flash_attn_ext_vec_bf16_h128      (not supported)\n",
      "ggml_metal_init: skipping kernel_flash_attn_ext_vec_bf16_h192      (not supported)\n",
      "ggml_metal_init: skipping kernel_flash_attn_ext_vec_bf16_hk192_hv128 (not supported)\n",
      "ggml_metal_init: skipping kernel_flash_attn_ext_vec_bf16_h256      (not supported)\n",
      "ggml_metal_init: skipping kernel_cpy_f32_bf16                      (not supported)\n",
      "ggml_metal_init: skipping kernel_cpy_bf16_f32                      (not supported)\n",
      "ggml_metal_init: skipping kernel_cpy_bf16_bf16                     (not supported)\n",
      "whisper_backend_init: using BLAS backend\n",
      "whisper_init_state: kv self size  =   18.87 MB\n",
      "whisper_init_state: kv cross size =   56.62 MB\n",
      "whisper_init_state: kv pad  size  =    4.72 MB\n",
      "whisper_init_state: compute buffer (conv)   =   23.37 MB\n",
      "whisper_init_state: compute buffer (encode) =  128.01 MB\n",
      "whisper_init_state: compute buffer (cross)  =    6.18 MB\n",
      "whisper_init_state: compute buffer (decode) =   98.19 MB\n",
      "\n",
      "system_info: n_threads = 4 / 14 | WHISPER : COREML = 0 | OPENVINO = 0 | Metal : EMBED_LIBRARY = 1 | CPU : NEON = 1 | ARM_FMA = 1 | FP16_VA = 1 | MATMUL_INT8 = 1 | DOTPROD = 1 | SME = 1 | ACCELERATE = 1 | AARCH64_REPACK = 1 | \n",
      "\n",
      "main: processing './Sound/01/chunk_034.wav' (1919157 samples, 119.9 sec), 4 threads, 1 processors, 5 beams + best of 5, lang = ko, task = transcribe, timestamps = 1 ...\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "[00:00:00.000 --> 00:00:09.920]   기토브의 대상으로 100포를 할 때 각각의 100포 전략이 이제 장점이 있으기 때문에 열심히 고민을 해보셔서 고르신 다음에\n",
      "[00:00:09.920 --> 00:00:15.400]   그러시면 좋을 것 같습니다. 이런 이론적인 문제가 있어요. 이론적인 것들이 있어요.\n",
      "[00:00:15.400 --> 00:00:25.200]   그 다음에 이제 기독스인데 여러분들이 아까 얘기한 ROCD를 기토브에 연결해서 헬런 100포 하는 것 자체가 기독스에요.\n",
      "[00:00:25.200 --> 00:00:34.200]   그러니까 정확히는 개발자가 저희가 어제 기톱 엔터파이지를 각각 레포스토리 만들어서 소스코드 업로드 했죠. 그걸로 저희 ACI 했죠.\n",
      "[00:00:34.200 --> 00:00:40.800]   그 1년의 과정을 지금 여기까지 올라와 있는 거예요. 도커브보다는 ACI로 생각하시면 되고\n",
      "[00:00:40.800 --> 00:00:48.440]   그래서 이 개발자가 소스코드 업로드해서 우리 기톱 액션으로 도커와 만들어서 ACI에다가 업로드를 했죠. 이 과정까지 끝났고\n",
      "[00:00:48.440 --> 00:00:56.480]   오늘은 이제 ROCD로 AKS에다가 배포를 할 건데 이 기독스 레포스토리 그러니까 100포를 할 형상을 소스코드로 관리하겠다는 거거든요.\n",
      "[00:00:56.480 --> 00:01:05.880]   아까 말씀 주신 게 헬륨으로 하셨다고 하셨는데 헬륨하셔도 되고 커스테마이지를 쓰셔도 되고 그냥 쌩자 디플루이먼트랑 서비스랑 그런 야볼 파일도 던지셔도 됩니다.\n",
      "[00:01:05.880 --> 00:01:16.200]   그건 이따가 자유롭게 하실 거고요. 따라서 이 ROCD나 ROCD가 커버네티스 또는 AKS에다가 배포할 형상을 소스코드로 관리하겠다.\n",
      "[00:01:16.200 --> 00:01:26.120]   그게 단일 원천의 소스 이런 용어가 있는데 어쨌든 이 킥에서 관리하는 포인트로 이 스테이터스를 맞춰주는 거죠.\n",
      "[00:01:26.120 --> 00:01:34.280]   그러니까 만약에 커버네티스의 버전1이 업데이트가 돼 있어요. 지금 배포가 돼 있어요. ROCD는 얘가 버전1의 배포함을 알고 있고\n",
      "[00:01:34.280 --> 00:01:43.880]   만약에 개발자가 운영자가 기독스 레포스토리에다가 버전2를 업데이트 해놨어요. 그러면 ROCD는 이 버전2를 커버네티스에다가 업데이트를 해야겠죠.\n",
      "[00:01:43.880 --> 00:01:53.720]   이런 자동화를 하는 게 ROCD이고 또 그 단일화된 소스를 제공하는 게 빛이 돼야 한다는 게 기독스의 개념입니다.\n",
      "[00:01:53.720 --> 00:02:02.360]   그냥 말로는 그렇고 시스포셨던 것처럼 그냥 연성해서 배포하면 되는구나. 그 기독스구나.\n",
      "\n",
      "✅ chunk_034 처리 완료 → ./Text/01/chunk_034_ggml-small.json\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "output_json: saving output to './Text/01/chunk_034_ggml-small.json'\n",
      "\n",
      "whisper_print_timings:     load time =   192.53 ms\n",
      "whisper_print_timings:     fallbacks =   0 p /   0 h\n",
      "whisper_print_timings:      mel time =    28.91 ms\n",
      "whisper_print_timings:   sample time =   491.38 ms /  3075 runs (     0.16 ms per run)\n",
      "whisper_print_timings:   encode time =   599.57 ms /     5 runs (   119.91 ms per run)\n",
      "whisper_print_timings:   decode time =     8.31 ms /     2 runs (     4.15 ms per run)\n",
      "whisper_print_timings:   batchd time =  2863.63 ms /  3051 runs (     0.94 ms per run)\n",
      "whisper_print_timings:   prompt time =    93.60 ms /   794 runs (     0.12 ms per run)\n",
      "whisper_print_timings:    total time =  4359.81 ms\n",
      "ggml_metal_free: deallocating\n",
      "whisper_init_from_file_with_params_no_state: loading model from '/Users/aza1200/.cache/huggingface/hub/models--ggerganov--whisper.cpp/snapshots/5359861c739e955e79d9a303bcbc70fb988958b1/ggml-small.bin'\n",
      "whisper_init_with_params_no_state: use gpu    = 1\n",
      "whisper_init_with_params_no_state: flash attn = 0\n",
      "whisper_init_with_params_no_state: gpu_device = 0\n",
      "whisper_init_with_params_no_state: dtw        = 0\n",
      "whisper_init_with_params_no_state: devices    = 3\n",
      "whisper_init_with_params_no_state: backends   = 3\n",
      "whisper_model_load: loading model\n",
      "whisper_model_load: n_vocab       = 51865\n",
      "whisper_model_load: n_audio_ctx   = 1500\n",
      "whisper_model_load: n_audio_state = 768\n",
      "whisper_model_load: n_audio_head  = 12\n",
      "whisper_model_load: n_audio_layer = 12\n",
      "whisper_model_load: n_text_ctx    = 448\n",
      "whisper_model_load: n_text_state  = 768\n",
      "whisper_model_load: n_text_head   = 12\n",
      "whisper_model_load: n_text_layer  = 12\n",
      "whisper_model_load: n_mels        = 80\n",
      "whisper_model_load: ftype         = 1\n",
      "whisper_model_load: qntvr         = 0\n",
      "whisper_model_load: type          = 3 (small)\n",
      "whisper_model_load: adding 1608 extra tokens\n",
      "whisper_model_load: n_langs       = 99\n",
      "whisper_model_load:        Metal total size =   487.01 MB\n",
      "whisper_model_load: model size    =  487.01 MB\n",
      "whisper_backend_init_gpu: using Metal backend\n",
      "ggml_metal_init: allocating\n",
      "ggml_metal_init: found device: Apple M4 Pro\n",
      "ggml_metal_init: picking default device: Apple M4 Pro\n",
      "ggml_metal_load_library: using embedded metal library\n",
      "ggml_metal_init: GPU name:   Apple M4 Pro\n",
      "ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)\n",
      "ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)\n",
      "ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)\n",
      "ggml_metal_init: simdgroup reduction   = true\n",
      "ggml_metal_init: simdgroup matrix mul. = true\n",
      "ggml_metal_init: has residency sets    = true\n",
      "ggml_metal_init: has bfloat            = true\n",
      "ggml_metal_init: use bfloat            = false\n",
      "ggml_metal_init: hasUnifiedMemory      = true\n",
      "ggml_metal_init: recommendedMaxWorkingSetSize  = 38654.71 MB\n",
      "ggml_metal_init: skipping kernel_get_rows_bf16                     (not supported)\n",
      "ggml_metal_init: skipping kernel_mul_mv_bf16_f32                   (not supported)\n",
      "ggml_metal_init: skipping kernel_mul_mv_bf16_f32_1row              (not supported)\n",
      "ggml_metal_init: skipping kernel_mul_mv_bf16_f32_l4                (not supported)\n",
      "ggml_metal_init: skipping kernel_mul_mv_bf16_bf16                  (not supported)\n",
      "ggml_metal_init: skipping kernel_mul_mv_id_bf16_f32                (not supported)\n",
      "ggml_metal_init: skipping kernel_mul_mm_bf16_f32                   (not supported)\n",
      "ggml_metal_init: skipping kernel_mul_mm_id_bf16_f32                (not supported)\n",
      "ggml_metal_init: skipping kernel_flash_attn_ext_bf16_h64           (not supported)\n",
      "ggml_metal_init: skipping kernel_flash_attn_ext_bf16_h80           (not supported)\n",
      "ggml_metal_init: skipping kernel_flash_attn_ext_bf16_h96           (not supported)\n",
      "ggml_metal_init: skipping kernel_flash_attn_ext_bf16_h112          (not supported)\n",
      "ggml_metal_init: skipping kernel_flash_attn_ext_bf16_h128          (not supported)\n",
      "ggml_metal_init: skipping kernel_flash_attn_ext_bf16_h192          (not supported)\n",
      "ggml_metal_init: skipping kernel_flash_attn_ext_bf16_hk192_hv128   (not supported)\n",
      "ggml_metal_init: skipping kernel_flash_attn_ext_bf16_h256          (not supported)\n",
      "ggml_metal_init: skipping kernel_flash_attn_ext_vec_bf16_h128      (not supported)\n",
      "ggml_metal_init: skipping kernel_flash_attn_ext_vec_bf16_h192      (not supported)\n",
      "ggml_metal_init: skipping kernel_flash_attn_ext_vec_bf16_hk192_hv128 (not supported)\n",
      "ggml_metal_init: skipping kernel_flash_attn_ext_vec_bf16_h256      (not supported)\n",
      "ggml_metal_init: skipping kernel_cpy_f32_bf16                      (not supported)\n",
      "ggml_metal_init: skipping kernel_cpy_bf16_f32                      (not supported)\n",
      "ggml_metal_init: skipping kernel_cpy_bf16_bf16                     (not supported)\n",
      "whisper_backend_init: using BLAS backend\n",
      "whisper_init_state: kv self size  =   18.87 MB\n",
      "whisper_init_state: kv cross size =   56.62 MB\n",
      "whisper_init_state: kv pad  size  =    4.72 MB\n",
      "whisper_init_state: compute buffer (conv)   =   23.37 MB\n",
      "whisper_init_state: compute buffer (encode) =  128.01 MB\n",
      "whisper_init_state: compute buffer (cross)  =    6.18 MB\n",
      "whisper_init_state: compute buffer (decode) =   98.19 MB\n",
      "\n",
      "system_info: n_threads = 4 / 14 | WHISPER : COREML = 0 | OPENVINO = 0 | Metal : EMBED_LIBRARY = 1 | CPU : NEON = 1 | ARM_FMA = 1 | FP16_VA = 1 | MATMUL_INT8 = 1 | DOTPROD = 1 | SME = 1 | ACCELERATE = 1 | AARCH64_REPACK = 1 | \n",
      "\n",
      "main: processing './Sound/01/chunk_035.wav' (1920515 samples, 120.0 sec), 4 threads, 1 processors, 5 beams + best of 5, lang = ko, task = transcribe, timestamps = 1 ...\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "[00:00:00.000 --> 00:00:08.720]   그리고 이제 rgo cd에서 블루글린이랑 사나리를 쓰기 위해서는 rgo rollout 이라는\n",
      "[00:00:08.720 --> 00:00:14.360]   add-on이 필요해요 따라서 rgo cd에 이제 rollout이 추가가 되어야 되고\n",
      "[00:00:14.360 --> 00:00:18.920]   이거를 쓰는 데는 어렵지 않습니다. 여기에 대한 기본적인\n",
      "[00:00:18.920 --> 00:00:23.340]   데시보드 뷰 같은 걸 제공하는데 이거는 제가 래퍼런스에서 따온 거고\n",
      "[00:00:23.340 --> 00:00:29.400]   만약에 지금 배포되어 있는 형상들을 이렇게 볼 수 있다. 따라서\n",
      "[00:00:29.400 --> 00:00:34.680]   배포되어 있는 걸 블루라 하고 배포할 거를 그린이라고 하면은 뭐 그린의 개수를\n",
      "[00:00:34.680 --> 00:00:40.400]   볼 수 있다. 까나리 같은 경우에는 현재 리비전이 몇이고 배포가 배포전 버전이 몇인데\n",
      "[00:00:40.400 --> 00:00:44.840]   이런 식으로 까나리가 진행되고 있다. 이런 테이블 뷰 를 보여줄 수 있고\n",
      "[00:00:44.840 --> 00:00:49.080]   까나리 배포를 할 때는 이걸 이제 스트레스가 그냥 배포 전략이거든요.\n",
      "[00:00:49.080 --> 00:00:54.280]   까나리 할지 블루글린 할지인데 여기서 이 스텝을 나눠서 비중을 주는 거죠.\n",
      "[00:00:54.280 --> 00:00:58.520]   전체 파드 중에 몇 개씩 배포를 할 건지 뭐 이런 옵션들이 있습니다.\n",
      "[00:00:58.520 --> 00:01:04.040]   참고만 해주시면 될 것 같아요. 그래서 그러면 rgo cd에서 까나리나\n",
      "[00:01:04.040 --> 00:01:07.880]   블루글린 어떻게 쓰는 건데 라고 했을 때 간단하게 그냥 야물 파일로 정의해서\n",
      "[00:01:07.880 --> 00:01:14.640]   쓰시면 되고요. 이때 그냥 이 rollout 이라는 타인드라고 할게요.\n",
      "[00:01:14.640 --> 00:01:20.320]   그 커버네티스의 속성에다가 롤아웃에다가 스트레이티지로 전략으로\n",
      "[00:01:20.320 --> 00:01:25.160]   블루글린 넣어주시고 해주시면 됩니다. 그럼 그냥 다 배포가 될 거고\n",
      "[00:01:25.160 --> 00:01:31.800]   까나리 같은 경우도 똑같이 이 rollout 이라는 속성에다가 이 전략에 까나리 넣으셔서\n",
      "[00:01:31.800 --> 00:01:37.520]   최대 몇 퍼센트를 한 번에 바꿀 건지 이런 옵션만 넣어주시면 배포를 하실 수 있어요.\n",
      "[00:01:37.520 --> 00:01:44.160]   이런 어렵지 않다. 그냥 어렵지 않다. 다만 오늘 시습을 준비한 데 블루글린과\n",
      "[00:01:44.160 --> 00:01:48.240]   까나리는 쓸 수 없습니다. 왜냐하면 rollout 을 준비를 안 했기 때문에\n",
      "[00:01:48.240 --> 00:01:52.480]   이건 그냥 이론적으로만 이런 식으로 쓰면 되겠구나 하시면 되고\n",
      "[00:01:52.480 --> 00:01:57.720]   만약에 있다가 여러분들이 실수 파실 때 제가 시간이 되면 rollout 을 설치를 해볼게요.\n",
      "[00:01:57.720 --> 00:02:02.240]   해보고 마지막으로 그럼\n",
      "\n",
      "✅ chunk_035 처리 완료 → ./Text/01/chunk_035_ggml-small.json\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "output_json: saving output to './Text/01/chunk_035_ggml-small.json'\n",
      "\n",
      "whisper_print_timings:     load time =   186.59 ms\n",
      "whisper_print_timings:     fallbacks =   0 p /   0 h\n",
      "whisper_print_timings:      mel time =    28.53 ms\n",
      "whisper_print_timings:   sample time =   450.19 ms /  2869 runs (     0.16 ms per run)\n",
      "whisper_print_timings:   encode time =   579.91 ms /     5 runs (   115.98 ms per run)\n",
      "whisper_print_timings:   decode time =     8.01 ms /     2 runs (     4.00 ms per run)\n",
      "whisper_print_timings:   batchd time =  2664.89 ms /  2845 runs (     0.94 ms per run)\n",
      "whisper_print_timings:   prompt time =    94.40 ms /   804 runs (     0.12 ms per run)\n",
      "whisper_print_timings:    total time =  4083.48 ms\n",
      "ggml_metal_free: deallocating\n",
      "whisper_init_from_file_with_params_no_state: loading model from '/Users/aza1200/.cache/huggingface/hub/models--ggerganov--whisper.cpp/snapshots/5359861c739e955e79d9a303bcbc70fb988958b1/ggml-small.bin'\n",
      "whisper_init_with_params_no_state: use gpu    = 1\n",
      "whisper_init_with_params_no_state: flash attn = 0\n",
      "whisper_init_with_params_no_state: gpu_device = 0\n",
      "whisper_init_with_params_no_state: dtw        = 0\n",
      "whisper_init_with_params_no_state: devices    = 3\n",
      "whisper_init_with_params_no_state: backends   = 3\n",
      "whisper_model_load: loading model\n",
      "whisper_model_load: n_vocab       = 51865\n",
      "whisper_model_load: n_audio_ctx   = 1500\n",
      "whisper_model_load: n_audio_state = 768\n",
      "whisper_model_load: n_audio_head  = 12\n",
      "whisper_model_load: n_audio_layer = 12\n",
      "whisper_model_load: n_text_ctx    = 448\n",
      "whisper_model_load: n_text_state  = 768\n",
      "whisper_model_load: n_text_head   = 12\n",
      "whisper_model_load: n_text_layer  = 12\n",
      "whisper_model_load: n_mels        = 80\n",
      "whisper_model_load: ftype         = 1\n",
      "whisper_model_load: qntvr         = 0\n",
      "whisper_model_load: type          = 3 (small)\n",
      "whisper_model_load: adding 1608 extra tokens\n",
      "whisper_model_load: n_langs       = 99\n",
      "whisper_model_load:        Metal total size =   487.01 MB\n",
      "whisper_model_load: model size    =  487.01 MB\n",
      "whisper_backend_init_gpu: using Metal backend\n",
      "ggml_metal_init: allocating\n",
      "ggml_metal_init: found device: Apple M4 Pro\n",
      "ggml_metal_init: picking default device: Apple M4 Pro\n",
      "ggml_metal_load_library: using embedded metal library\n",
      "ggml_metal_init: GPU name:   Apple M4 Pro\n",
      "ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)\n",
      "ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)\n",
      "ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)\n",
      "ggml_metal_init: simdgroup reduction   = true\n",
      "ggml_metal_init: simdgroup matrix mul. = true\n",
      "ggml_metal_init: has residency sets    = true\n",
      "ggml_metal_init: has bfloat            = true\n",
      "ggml_metal_init: use bfloat            = false\n",
      "ggml_metal_init: hasUnifiedMemory      = true\n",
      "ggml_metal_init: recommendedMaxWorkingSetSize  = 38654.71 MB\n",
      "ggml_metal_init: skipping kernel_get_rows_bf16                     (not supported)\n",
      "ggml_metal_init: skipping kernel_mul_mv_bf16_f32                   (not supported)\n",
      "ggml_metal_init: skipping kernel_mul_mv_bf16_f32_1row              (not supported)\n",
      "ggml_metal_init: skipping kernel_mul_mv_bf16_f32_l4                (not supported)\n",
      "ggml_metal_init: skipping kernel_mul_mv_bf16_bf16                  (not supported)\n",
      "ggml_metal_init: skipping kernel_mul_mv_id_bf16_f32                (not supported)\n",
      "ggml_metal_init: skipping kernel_mul_mm_bf16_f32                   (not supported)\n",
      "ggml_metal_init: skipping kernel_mul_mm_id_bf16_f32                (not supported)\n",
      "ggml_metal_init: skipping kernel_flash_attn_ext_bf16_h64           (not supported)\n",
      "ggml_metal_init: skipping kernel_flash_attn_ext_bf16_h80           (not supported)\n",
      "ggml_metal_init: skipping kernel_flash_attn_ext_bf16_h96           (not supported)\n",
      "ggml_metal_init: skipping kernel_flash_attn_ext_bf16_h112          (not supported)\n",
      "ggml_metal_init: skipping kernel_flash_attn_ext_bf16_h128          (not supported)\n",
      "ggml_metal_init: skipping kernel_flash_attn_ext_bf16_h192          (not supported)\n",
      "ggml_metal_init: skipping kernel_flash_attn_ext_bf16_hk192_hv128   (not supported)\n",
      "ggml_metal_init: skipping kernel_flash_attn_ext_bf16_h256          (not supported)\n",
      "ggml_metal_init: skipping kernel_flash_attn_ext_vec_bf16_h128      (not supported)\n",
      "ggml_metal_init: skipping kernel_flash_attn_ext_vec_bf16_h192      (not supported)\n",
      "ggml_metal_init: skipping kernel_flash_attn_ext_vec_bf16_hk192_hv128 (not supported)\n",
      "ggml_metal_init: skipping kernel_flash_attn_ext_vec_bf16_h256      (not supported)\n",
      "ggml_metal_init: skipping kernel_cpy_f32_bf16                      (not supported)\n",
      "ggml_metal_init: skipping kernel_cpy_bf16_f32                      (not supported)\n",
      "ggml_metal_init: skipping kernel_cpy_bf16_bf16                     (not supported)\n",
      "whisper_backend_init: using BLAS backend\n",
      "whisper_init_state: kv self size  =   18.87 MB\n",
      "whisper_init_state: kv cross size =   56.62 MB\n",
      "whisper_init_state: kv pad  size  =    4.72 MB\n",
      "whisper_init_state: compute buffer (conv)   =   23.37 MB\n",
      "whisper_init_state: compute buffer (encode) =  128.01 MB\n",
      "whisper_init_state: compute buffer (cross)  =    6.18 MB\n",
      "whisper_init_state: compute buffer (decode) =   98.19 MB\n",
      "\n",
      "system_info: n_threads = 4 / 14 | WHISPER : COREML = 0 | OPENVINO = 0 | Metal : EMBED_LIBRARY = 1 | CPU : NEON = 1 | ARM_FMA = 1 | FP16_VA = 1 | MATMUL_INT8 = 1 | DOTPROD = 1 | SME = 1 | ACCELERATE = 1 | AARCH64_REPACK = 1 | \n",
      "\n",
      "main: processing './Sound/01/chunk_036.wav' (1919554 samples, 120.0 sec), 4 threads, 1 processors, 5 beams + best of 5, lang = ko, task = transcribe, timestamps = 1 ...\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "[00:00:00.000 --> 00:00:05.720]   ARGO CD의 컴포넌트들에 대한 구성만 말씀드리고\n",
      "[00:00:05.720 --> 00:00:07.920]   우리 밥을 먹으러 갈건데\n",
      "[00:00:07.920 --> 00:00:16.560]   ARGO CD가 겉으로는 그냥 이렇게 그냥 무너처럼 생겼잖아요\n",
      "[00:00:16.560 --> 00:00:20.080]   여기서 지금 여러분들이 보고 계시는 화면이\n",
      "[00:00:20.080 --> 00:00:25.320]   이 웹 앱 쪽에 우리 이 UI 쪽으로 들어온 거예요\n",
      "[00:00:25.320 --> 00:00:29.280]   그리고 ARGO CD는 Kubernetes를 다루는\n",
      "[00:00:29.280 --> 00:00:32.960]   자동화 추리기 때문에 이거를 모든 걸 처리할\n",
      "[00:00:32.960 --> 00:00:35.160]   ARGO CD 서버라는 컴포넌트가 있습니다\n",
      "[00:00:35.160 --> 00:00:37.760]   ARGO CD 서버라는 컴포넌트가 있고\n",
      "[00:00:37.760 --> 00:00:42.720]   거기에 받는 요청들을 처리하는 코어 단의 것들이 있어요\n",
      "[00:00:42.720 --> 00:00:44.520]   간단하게 보면\n",
      "[00:00:44.520 --> 00:00:49.240]   그리고 이 전체를 구성하는 인프라에 대한 관련된 게 있죠\n",
      "[00:00:49.240 --> 00:00:53.000]   지금 보고 있는 이 화면이 지금 웹 앱으로 들어온 형태고\n",
      "[00:00:53.000 --> 00:00:55.440]   여기에 들어오는 모든 요청들 있죠\n",
      "[00:00:55.440 --> 00:00:57.160]   여러분들 배포 해봤다 했으니까\n",
      "[00:00:57.160 --> 00:01:01.200]   앱 눌러서 어플리케이션에 이름 넣고 이런 식으로 연동하고\n",
      "[00:01:01.200 --> 00:01:06.240]   배포하는 모든 요청에 대한 서버 서비스의 처리를\n",
      "[00:01:06.240 --> 00:01:07.760]   이 API 서버가 합니다\n",
      "[00:01:07.760 --> 00:01:11.280]   API 서버가 명령을 전달해서\n",
      "[00:01:11.280 --> 00:01:14.800]   어플리케이션 관련된 거는 어플리케이션 컨트롤러가 처리하고\n",
      "[00:01:14.800 --> 00:01:17.720]   어플리케이션 셋이라고 해서 어플리케이션을\n",
      "[00:01:17.720 --> 00:01:21.040]   하나의 단위로 논리적 단위로 굽은 이렇게 묶어 가지고\n",
      "[00:01:21.040 --> 00:01:22.840]   한 번에 배포할 수 있는 게 있거든요\n",
      "[00:01:22.840 --> 00:01:26.200]   그런 것도 이제 어플리케이션 셋 컨트롤러가 처리를 하고\n",
      "[00:01:26.200 --> 00:01:30.440]   그 다음에 여기서 이제 던지는 기턱과 기록스 둘이잖아요\n",
      "[00:01:30.440 --> 00:01:33.360]   기턱과 연동하는 거는 레포 서버가 처리를 한다\n",
      "[00:01:33.360 --> 00:01:37.240]   따라서 이런 식으로 표정이 되어있구나라고 이해하시면 될 것 같습니다\n",
      "[00:01:37.240 --> 00:01:41.640]   특히 긴 연동할 때는 레포 서버가 긴을 처리하는 부분에 있어서\n",
      "[00:01:41.640 --> 00:01:46.760]   저희는 이제 기턱 SSO 인증으로도 아르고시디를 쓸 수 있어요\n",
      "[00:01:46.760 --> 00:01:50.800]   그러니까 지금 이렇게 생긴 애는\n",
      "[00:01:50.800 --> 00:01:53.080]   지금 그냥 여기다가 아까 징키스 로그인 하듯이\n",
      "[00:01:53.080 --> 00:01:57.000]   이 로콜에 있는 아르고시디를 입력을 넣는 거거든요\n",
      "[00:01:57.000 --> 00:01:59.400]   ID 패스워드는 그럼 이렇게 넣어가지고 되는데\n",
      "\n",
      "✅ chunk_036 처리 완료 → ./Text/01/chunk_036_ggml-small.json\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "output_json: saving output to './Text/01/chunk_036_ggml-small.json'\n",
      "\n",
      "whisper_print_timings:     load time =   188.04 ms\n",
      "whisper_print_timings:     fallbacks =   0 p /   0 h\n",
      "whisper_print_timings:      mel time =    28.40 ms\n",
      "whisper_print_timings:   sample time =   432.89 ms /  2766 runs (     0.16 ms per run)\n",
      "whisper_print_timings:   encode time =   593.26 ms /     5 runs (   118.65 ms per run)\n",
      "whisper_print_timings:   decode time =    75.41 ms /    18 runs (     4.19 ms per run)\n",
      "whisper_print_timings:   batchd time =  2499.44 ms /  2726 runs (     0.92 ms per run)\n",
      "whisper_print_timings:   prompt time =    90.86 ms /   780 runs (     0.12 ms per run)\n",
      "whisper_print_timings:    total time =  3984.97 ms\n",
      "ggml_metal_free: deallocating\n",
      "whisper_init_from_file_with_params_no_state: loading model from '/Users/aza1200/.cache/huggingface/hub/models--ggerganov--whisper.cpp/snapshots/5359861c739e955e79d9a303bcbc70fb988958b1/ggml-small.bin'\n",
      "whisper_init_with_params_no_state: use gpu    = 1\n",
      "whisper_init_with_params_no_state: flash attn = 0\n",
      "whisper_init_with_params_no_state: gpu_device = 0\n",
      "whisper_init_with_params_no_state: dtw        = 0\n",
      "whisper_init_with_params_no_state: devices    = 3\n",
      "whisper_init_with_params_no_state: backends   = 3\n",
      "whisper_model_load: loading model\n",
      "whisper_model_load: n_vocab       = 51865\n",
      "whisper_model_load: n_audio_ctx   = 1500\n",
      "whisper_model_load: n_audio_state = 768\n",
      "whisper_model_load: n_audio_head  = 12\n",
      "whisper_model_load: n_audio_layer = 12\n",
      "whisper_model_load: n_text_ctx    = 448\n",
      "whisper_model_load: n_text_state  = 768\n",
      "whisper_model_load: n_text_head   = 12\n",
      "whisper_model_load: n_text_layer  = 12\n",
      "whisper_model_load: n_mels        = 80\n",
      "whisper_model_load: ftype         = 1\n",
      "whisper_model_load: qntvr         = 0\n",
      "whisper_model_load: type          = 3 (small)\n",
      "whisper_model_load: adding 1608 extra tokens\n",
      "whisper_model_load: n_langs       = 99\n",
      "whisper_model_load:        Metal total size =   487.01 MB\n",
      "whisper_model_load: model size    =  487.01 MB\n",
      "whisper_backend_init_gpu: using Metal backend\n",
      "ggml_metal_init: allocating\n",
      "ggml_metal_init: found device: Apple M4 Pro\n",
      "ggml_metal_init: picking default device: Apple M4 Pro\n",
      "ggml_metal_load_library: using embedded metal library\n",
      "ggml_metal_init: GPU name:   Apple M4 Pro\n",
      "ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)\n",
      "ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)\n",
      "ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)\n",
      "ggml_metal_init: simdgroup reduction   = true\n",
      "ggml_metal_init: simdgroup matrix mul. = true\n",
      "ggml_metal_init: has residency sets    = true\n",
      "ggml_metal_init: has bfloat            = true\n",
      "ggml_metal_init: use bfloat            = false\n",
      "ggml_metal_init: hasUnifiedMemory      = true\n",
      "ggml_metal_init: recommendedMaxWorkingSetSize  = 38654.71 MB\n",
      "ggml_metal_init: skipping kernel_get_rows_bf16                     (not supported)\n",
      "ggml_metal_init: skipping kernel_mul_mv_bf16_f32                   (not supported)\n",
      "ggml_metal_init: skipping kernel_mul_mv_bf16_f32_1row              (not supported)\n",
      "ggml_metal_init: skipping kernel_mul_mv_bf16_f32_l4                (not supported)\n",
      "ggml_metal_init: skipping kernel_mul_mv_bf16_bf16                  (not supported)\n",
      "ggml_metal_init: skipping kernel_mul_mv_id_bf16_f32                (not supported)\n",
      "ggml_metal_init: skipping kernel_mul_mm_bf16_f32                   (not supported)\n",
      "ggml_metal_init: skipping kernel_mul_mm_id_bf16_f32                (not supported)\n",
      "ggml_metal_init: skipping kernel_flash_attn_ext_bf16_h64           (not supported)\n",
      "ggml_metal_init: skipping kernel_flash_attn_ext_bf16_h80           (not supported)\n",
      "ggml_metal_init: skipping kernel_flash_attn_ext_bf16_h96           (not supported)\n",
      "ggml_metal_init: skipping kernel_flash_attn_ext_bf16_h112          (not supported)\n",
      "ggml_metal_init: skipping kernel_flash_attn_ext_bf16_h128          (not supported)\n",
      "ggml_metal_init: skipping kernel_flash_attn_ext_bf16_h192          (not supported)\n",
      "ggml_metal_init: skipping kernel_flash_attn_ext_bf16_hk192_hv128   (not supported)\n",
      "ggml_metal_init: skipping kernel_flash_attn_ext_bf16_h256          (not supported)\n",
      "ggml_metal_init: skipping kernel_flash_attn_ext_vec_bf16_h128      (not supported)\n",
      "ggml_metal_init: skipping kernel_flash_attn_ext_vec_bf16_h192      (not supported)\n",
      "ggml_metal_init: skipping kernel_flash_attn_ext_vec_bf16_hk192_hv128 (not supported)\n",
      "ggml_metal_init: skipping kernel_flash_attn_ext_vec_bf16_h256      (not supported)\n",
      "ggml_metal_init: skipping kernel_cpy_f32_bf16                      (not supported)\n",
      "ggml_metal_init: skipping kernel_cpy_bf16_f32                      (not supported)\n",
      "ggml_metal_init: skipping kernel_cpy_bf16_bf16                     (not supported)\n",
      "whisper_backend_init: using BLAS backend\n",
      "whisper_init_state: kv self size  =   18.87 MB\n",
      "whisper_init_state: kv cross size =   56.62 MB\n",
      "whisper_init_state: kv pad  size  =    4.72 MB\n",
      "whisper_init_state: compute buffer (conv)   =   23.37 MB\n",
      "whisper_init_state: compute buffer (encode) =  128.01 MB\n",
      "whisper_init_state: compute buffer (cross)  =    6.18 MB\n",
      "whisper_init_state: compute buffer (decode) =   98.19 MB\n",
      "\n",
      "system_info: n_threads = 4 / 14 | WHISPER : COREML = 0 | OPENVINO = 0 | Metal : EMBED_LIBRARY = 1 | CPU : NEON = 1 | ARM_FMA = 1 | FP16_VA = 1 | MATMUL_INT8 = 1 | DOTPROD = 1 | SME = 1 | ACCELERATE = 1 | AARCH64_REPACK = 1 | \n",
      "\n",
      "main: processing './Sound/01/chunk_037.wav' (1920458 samples, 120.0 sec), 4 threads, 1 processors, 5 beams + best of 5, lang = ko, task = transcribe, timestamps = 1 ...\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "[00:00:00.000 --> 00:00:04.180]   KT의 기탑 엔터프라이즈와 연동해서 기탑 SSO 로그인까지 할 수 있어요\n",
      "[00:00:04.180 --> 00:00:06.740]   그래서 그간에 있다가 제가 보송을 해 둘 테니\n",
      "[00:00:06.740 --> 00:00:10.180]   여러분도 한 번씩 로그인 하시는 게 좋을 것 같고\n",
      "[00:00:10.180 --> 00:00:12.700]   음 네 이렇게 보송되어 있습니다\n",
      "[00:00:12.700 --> 00:00:16.300]   오늘 그래서 이 아래 얘기는 길지는 않아서\n",
      "[00:00:16.300 --> 00:00:18.540]   이따 오후에다가 잠깐 다시 하고\n",
      "[00:00:18.540 --> 00:00:21.600]   오늘 여러분들이 RUGO CD에 접속을 하셔야 되고\n",
      "[00:00:21.600 --> 00:00:23.780]   거기에 대한 정보를 제가 여기에 드렸어요\n",
      "[00:00:23.780 --> 00:00:26.500]   이게 사례 PC 충전 되시면은\n",
      "[00:00:26.500 --> 00:00:29.940]   그냥 여기 아래에 있는 이 윈도우 탐색이 있죠\n",
      "[00:00:29.940 --> 00:00:33.820]   여기서 메모짱 탐색해서\n",
      "[00:00:33.820 --> 00:00:36.780]   클릭해서 관리자 권한으로 실행\n",
      "[00:00:36.780 --> 00:00:38.680]   하면 이렇게 그냥 빙 메모짱 하나 뜨거든요\n",
      "[00:00:38.680 --> 00:00:41.280]   얘는 관리자 권한으로 실행되고\n",
      "[00:00:41.280 --> 00:00:48.140]   여기서 이제 열기 파일 열기에서 이 아래 이제 패스 보시면은\n",
      "[00:00:48.140 --> 00:00:50.000]   C드라이브 들어가셔서\n",
      "[00:00:50.000 --> 00:00:54.620]   윈도우에서 아래 시스템 30이 아래\n",
      "[00:00:54.620 --> 00:00:58.840]   드라이버 아래 ETC 아래 딱 들어가면 아무것도 안 보이거든요\n",
      "[00:00:58.840 --> 00:01:01.540]   근데 이때 오른쪽 아래에 모든 파일을 활성하시면\n",
      "[00:01:01.540 --> 00:01:03.340]   이 호스트 파일이 있어요\n",
      "[00:01:03.340 --> 00:01:06.740]   이게 우리가 RUGO CD에 접속할 때는\n",
      "[00:01:06.740 --> 00:01:09.040]   인그레스에 있는 호스트 네임으로 들어갈 거예요\n",
      "[00:01:09.040 --> 00:01:12.140]   우아래의 형태로 돼있겠죠\n",
      "[00:01:12.140 --> 00:01:15.280]   근데 거기에 이제 진입할 우리가 방화병 자범 페논\n",
      "[00:01:15.280 --> 00:01:17.980]   커버네티스 인터널의 컨텐츠 IP 정보를\n",
      "[00:01:17.980 --> 00:01:19.680]   우아래를 알고 있지 않아요\n",
      "[00:01:19.680 --> 00:01:21.380]   그러니까 DNS 등록을 안 해놨습니다\n",
      "[00:01:21.380 --> 00:01:23.240]   따라서 우리 IP로 들어가야 되는데\n",
      "[00:01:23.240 --> 00:01:26.980]   거기에 대한 로컬 DNS를 구성하는 방식이라고 생각하시면 됩니다\n",
      "[00:01:26.980 --> 00:01:29.720]   그러니까 우리가 PC 안에서\n",
      "[00:01:29.720 --> 00:01:33.120]   DNS가 구성되어 있지 않은 URL에 요청을 할 때\n",
      "[00:01:33.120 --> 00:01:35.780]   거기에 대해서 가장 먼저 찾아보는 게 이 호스트 파일이거든요\n",
      "[00:01:35.780 --> 00:01:40.080]   OS에 있는 ETC에서 따라서 여기에다가\n",
      "[00:01:40.080 --> 00:01:41.480]   아래에 이렇게 넣으시면 돼요\n",
      "[00:01:41.480 --> 00:01:43.040]   그냥 IP 덕분에\n",
      "[00:01:43.040 --> 00:01:44.920]   이 위에 꼭 무시해 주시고 제가 쓰는 거로\n",
      "[00:01:44.920 --> 00:01:47.080]   아래 거 넣고 그다음에 이 URL에 넣고\n",
      "[00:01:47.080 --> 00:01:48.980]   그냥 복사 붙여넣기 하셔서\n",
      "[00:01:48.980 --> 00:01:54.720]   버전 사진 다음에 웹 브라우저에 다시 들어가셔서\n",
      "[00:01:54.720 --> 00:01:56.020]   검색 검색이 아니라\n",
      "[00:01:56.020 --> 00:01:58.620]   엔소드 누르면 이제 들어가지는 것까지 확인하셔야 돼요\n",
      "\n",
      "✅ chunk_037 처리 완료 → ./Text/01/chunk_037_ggml-small.json\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "output_json: saving output to './Text/01/chunk_037_ggml-small.json'\n",
      "\n",
      "whisper_print_timings:     load time =   183.02 ms\n",
      "whisper_print_timings:     fallbacks =   0 p /   0 h\n",
      "whisper_print_timings:      mel time =    28.42 ms\n",
      "whisper_print_timings:   sample time =   463.37 ms /  3010 runs (     0.15 ms per run)\n",
      "whisper_print_timings:   encode time =   579.37 ms /     5 runs (   115.87 ms per run)\n",
      "whisper_print_timings:   decode time =     8.72 ms /     2 runs (     4.36 ms per run)\n",
      "whisper_print_timings:   batchd time =  2740.87 ms /  2989 runs (     0.92 ms per run)\n",
      "whisper_print_timings:   prompt time =    72.08 ms /   620 runs (     0.12 ms per run)\n",
      "whisper_print_timings:    total time =  4142.25 ms\n",
      "ggml_metal_free: deallocating\n",
      "whisper_init_from_file_with_params_no_state: loading model from '/Users/aza1200/.cache/huggingface/hub/models--ggerganov--whisper.cpp/snapshots/5359861c739e955e79d9a303bcbc70fb988958b1/ggml-small.bin'\n",
      "whisper_init_with_params_no_state: use gpu    = 1\n",
      "whisper_init_with_params_no_state: flash attn = 0\n",
      "whisper_init_with_params_no_state: gpu_device = 0\n",
      "whisper_init_with_params_no_state: dtw        = 0\n",
      "whisper_init_with_params_no_state: devices    = 3\n",
      "whisper_init_with_params_no_state: backends   = 3\n",
      "whisper_model_load: loading model\n",
      "whisper_model_load: n_vocab       = 51865\n",
      "whisper_model_load: n_audio_ctx   = 1500\n",
      "whisper_model_load: n_audio_state = 768\n",
      "whisper_model_load: n_audio_head  = 12\n",
      "whisper_model_load: n_audio_layer = 12\n",
      "whisper_model_load: n_text_ctx    = 448\n",
      "whisper_model_load: n_text_state  = 768\n",
      "whisper_model_load: n_text_head   = 12\n",
      "whisper_model_load: n_text_layer  = 12\n",
      "whisper_model_load: n_mels        = 80\n",
      "whisper_model_load: ftype         = 1\n",
      "whisper_model_load: qntvr         = 0\n",
      "whisper_model_load: type          = 3 (small)\n",
      "whisper_model_load: adding 1608 extra tokens\n",
      "whisper_model_load: n_langs       = 99\n",
      "whisper_model_load:        Metal total size =   487.01 MB\n",
      "whisper_model_load: model size    =  487.01 MB\n",
      "whisper_backend_init_gpu: using Metal backend\n",
      "ggml_metal_init: allocating\n",
      "ggml_metal_init: found device: Apple M4 Pro\n",
      "ggml_metal_init: picking default device: Apple M4 Pro\n",
      "ggml_metal_load_library: using embedded metal library\n",
      "ggml_metal_init: GPU name:   Apple M4 Pro\n",
      "ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)\n",
      "ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)\n",
      "ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)\n",
      "ggml_metal_init: simdgroup reduction   = true\n",
      "ggml_metal_init: simdgroup matrix mul. = true\n",
      "ggml_metal_init: has residency sets    = true\n",
      "ggml_metal_init: has bfloat            = true\n",
      "ggml_metal_init: use bfloat            = false\n",
      "ggml_metal_init: hasUnifiedMemory      = true\n",
      "ggml_metal_init: recommendedMaxWorkingSetSize  = 38654.71 MB\n",
      "ggml_metal_init: skipping kernel_get_rows_bf16                     (not supported)\n",
      "ggml_metal_init: skipping kernel_mul_mv_bf16_f32                   (not supported)\n",
      "ggml_metal_init: skipping kernel_mul_mv_bf16_f32_1row              (not supported)\n",
      "ggml_metal_init: skipping kernel_mul_mv_bf16_f32_l4                (not supported)\n",
      "ggml_metal_init: skipping kernel_mul_mv_bf16_bf16                  (not supported)\n",
      "ggml_metal_init: skipping kernel_mul_mv_id_bf16_f32                (not supported)\n",
      "ggml_metal_init: skipping kernel_mul_mm_bf16_f32                   (not supported)\n",
      "ggml_metal_init: skipping kernel_mul_mm_id_bf16_f32                (not supported)\n",
      "ggml_metal_init: skipping kernel_flash_attn_ext_bf16_h64           (not supported)\n",
      "ggml_metal_init: skipping kernel_flash_attn_ext_bf16_h80           (not supported)\n",
      "ggml_metal_init: skipping kernel_flash_attn_ext_bf16_h96           (not supported)\n",
      "ggml_metal_init: skipping kernel_flash_attn_ext_bf16_h112          (not supported)\n",
      "ggml_metal_init: skipping kernel_flash_attn_ext_bf16_h128          (not supported)\n",
      "ggml_metal_init: skipping kernel_flash_attn_ext_bf16_h192          (not supported)\n",
      "ggml_metal_init: skipping kernel_flash_attn_ext_bf16_hk192_hv128   (not supported)\n",
      "ggml_metal_init: skipping kernel_flash_attn_ext_bf16_h256          (not supported)\n",
      "ggml_metal_init: skipping kernel_flash_attn_ext_vec_bf16_h128      (not supported)\n",
      "ggml_metal_init: skipping kernel_flash_attn_ext_vec_bf16_h192      (not supported)\n",
      "ggml_metal_init: skipping kernel_flash_attn_ext_vec_bf16_hk192_hv128 (not supported)\n",
      "ggml_metal_init: skipping kernel_flash_attn_ext_vec_bf16_h256      (not supported)\n",
      "ggml_metal_init: skipping kernel_cpy_f32_bf16                      (not supported)\n",
      "ggml_metal_init: skipping kernel_cpy_bf16_f32                      (not supported)\n",
      "ggml_metal_init: skipping kernel_cpy_bf16_bf16                     (not supported)\n",
      "whisper_backend_init: using BLAS backend\n",
      "whisper_init_state: kv self size  =   18.87 MB\n",
      "whisper_init_state: kv cross size =   56.62 MB\n",
      "whisper_init_state: kv pad  size  =    4.72 MB\n",
      "whisper_init_state: compute buffer (conv)   =   23.37 MB\n",
      "whisper_init_state: compute buffer (encode) =  128.01 MB\n",
      "whisper_init_state: compute buffer (cross)  =    6.18 MB\n",
      "whisper_init_state: compute buffer (decode) =   98.19 MB\n",
      "\n",
      "system_info: n_threads = 4 / 14 | WHISPER : COREML = 0 | OPENVINO = 0 | Metal : EMBED_LIBRARY = 1 | CPU : NEON = 1 | ARM_FMA = 1 | FP16_VA = 1 | MATMUL_INT8 = 1 | DOTPROD = 1 | SME = 1 | ACCELERATE = 1 | AARCH64_REPACK = 1 | \n",
      "\n",
      "main: processing './Sound/01/chunk_038.wav' (1920225 samples, 120.0 sec), 4 threads, 1 processors, 5 beams + best of 5, lang = ko, task = transcribe, timestamps = 1 ...\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "[00:00:00.000 --> 00:00:10.220]   그래서 배터리 충전 좀 해보시고 rgocd 적극하시면 로그인까지 해보시고 로그인 입니다.\n",
      "[00:00:10.220 --> 00:00:15.160]   이거 하고 11시 반에 밥 먹으러 가시면 돼요. 아시겠죠.\n",
      "[00:00:30.000 --> 00:00:40.000]  . [두 번째 질문]\n",
      "[00:00:40.000 --> 00:00:59.000]   [두 번째 질문]\n",
      "[00:00:59.000 --> 00:01:09.000]   [두 번째 질문]\n",
      "[00:01:09.000 --> 00:01:19.000]   [두 번째 질문]\n",
      "[00:01:19.000 --> 00:01:29.000]   [두 번째 질문]\n",
      "[00:01:29.000 --> 00:01:39.000]   [두 번째 질문]\n",
      "[00:01:39.000 --> 00:01:49.000]   [두 번째 질문]\n",
      "[00:01:49.000 --> 00:01:59.000]   [두 번째 질문]\n",
      "[00:01:59.000 --> 00:02:01.000]   [두 번째 질문]\n",
      "\n",
      "✅ chunk_038 처리 완료 → ./Text/01/chunk_038_ggml-small.json\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "output_json: saving output to './Text/01/chunk_038_ggml-small.json'\n",
      "\n",
      "whisper_print_timings:     load time =   190.29 ms\n",
      "whisper_print_timings:     fallbacks =   1 p /   0 h\n",
      "whisper_print_timings:      mel time =    28.91 ms\n",
      "whisper_print_timings:   sample time =   216.38 ms /  1313 runs (     0.16 ms per run)\n",
      "whisper_print_timings:   encode time =   578.24 ms /     5 runs (   115.65 ms per run)\n",
      "whisper_print_timings:   decode time =   142.33 ms /    38 runs (     3.75 ms per run)\n",
      "whisper_print_timings:   batchd time =  1551.19 ms /  1248 runs (     1.24 ms per run)\n",
      "whisper_print_timings:   prompt time =    57.52 ms /   392 runs (     0.15 ms per run)\n",
      "whisper_print_timings:    total time =  2826.94 ms\n",
      "ggml_metal_free: deallocating\n",
      "whisper_init_from_file_with_params_no_state: loading model from '/Users/aza1200/.cache/huggingface/hub/models--ggerganov--whisper.cpp/snapshots/5359861c739e955e79d9a303bcbc70fb988958b1/ggml-small.bin'\n",
      "whisper_init_with_params_no_state: use gpu    = 1\n",
      "whisper_init_with_params_no_state: flash attn = 0\n",
      "whisper_init_with_params_no_state: gpu_device = 0\n",
      "whisper_init_with_params_no_state: dtw        = 0\n",
      "whisper_init_with_params_no_state: devices    = 3\n",
      "whisper_init_with_params_no_state: backends   = 3\n",
      "whisper_model_load: loading model\n",
      "whisper_model_load: n_vocab       = 51865\n",
      "whisper_model_load: n_audio_ctx   = 1500\n",
      "whisper_model_load: n_audio_state = 768\n",
      "whisper_model_load: n_audio_head  = 12\n",
      "whisper_model_load: n_audio_layer = 12\n",
      "whisper_model_load: n_text_ctx    = 448\n",
      "whisper_model_load: n_text_state  = 768\n",
      "whisper_model_load: n_text_head   = 12\n",
      "whisper_model_load: n_text_layer  = 12\n",
      "whisper_model_load: n_mels        = 80\n",
      "whisper_model_load: ftype         = 1\n",
      "whisper_model_load: qntvr         = 0\n",
      "whisper_model_load: type          = 3 (small)\n",
      "whisper_model_load: adding 1608 extra tokens\n",
      "whisper_model_load: n_langs       = 99\n",
      "whisper_model_load:        Metal total size =   487.01 MB\n",
      "whisper_model_load: model size    =  487.01 MB\n",
      "whisper_backend_init_gpu: using Metal backend\n",
      "ggml_metal_init: allocating\n",
      "ggml_metal_init: found device: Apple M4 Pro\n",
      "ggml_metal_init: picking default device: Apple M4 Pro\n",
      "ggml_metal_load_library: using embedded metal library\n",
      "ggml_metal_init: GPU name:   Apple M4 Pro\n",
      "ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)\n",
      "ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)\n",
      "ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)\n",
      "ggml_metal_init: simdgroup reduction   = true\n",
      "ggml_metal_init: simdgroup matrix mul. = true\n",
      "ggml_metal_init: has residency sets    = true\n",
      "ggml_metal_init: has bfloat            = true\n",
      "ggml_metal_init: use bfloat            = false\n",
      "ggml_metal_init: hasUnifiedMemory      = true\n",
      "ggml_metal_init: recommendedMaxWorkingSetSize  = 38654.71 MB\n",
      "ggml_metal_init: skipping kernel_get_rows_bf16                     (not supported)\n",
      "ggml_metal_init: skipping kernel_mul_mv_bf16_f32                   (not supported)\n",
      "ggml_metal_init: skipping kernel_mul_mv_bf16_f32_1row              (not supported)\n",
      "ggml_metal_init: skipping kernel_mul_mv_bf16_f32_l4                (not supported)\n",
      "ggml_metal_init: skipping kernel_mul_mv_bf16_bf16                  (not supported)\n",
      "ggml_metal_init: skipping kernel_mul_mv_id_bf16_f32                (not supported)\n",
      "ggml_metal_init: skipping kernel_mul_mm_bf16_f32                   (not supported)\n",
      "ggml_metal_init: skipping kernel_mul_mm_id_bf16_f32                (not supported)\n",
      "ggml_metal_init: skipping kernel_flash_attn_ext_bf16_h64           (not supported)\n",
      "ggml_metal_init: skipping kernel_flash_attn_ext_bf16_h80           (not supported)\n",
      "ggml_metal_init: skipping kernel_flash_attn_ext_bf16_h96           (not supported)\n",
      "ggml_metal_init: skipping kernel_flash_attn_ext_bf16_h112          (not supported)\n",
      "ggml_metal_init: skipping kernel_flash_attn_ext_bf16_h128          (not supported)\n",
      "ggml_metal_init: skipping kernel_flash_attn_ext_bf16_h192          (not supported)\n",
      "ggml_metal_init: skipping kernel_flash_attn_ext_bf16_hk192_hv128   (not supported)\n",
      "ggml_metal_init: skipping kernel_flash_attn_ext_bf16_h256          (not supported)\n",
      "ggml_metal_init: skipping kernel_flash_attn_ext_vec_bf16_h128      (not supported)\n",
      "ggml_metal_init: skipping kernel_flash_attn_ext_vec_bf16_h192      (not supported)\n",
      "ggml_metal_init: skipping kernel_flash_attn_ext_vec_bf16_hk192_hv128 (not supported)\n",
      "ggml_metal_init: skipping kernel_flash_attn_ext_vec_bf16_h256      (not supported)\n",
      "ggml_metal_init: skipping kernel_cpy_f32_bf16                      (not supported)\n",
      "ggml_metal_init: skipping kernel_cpy_bf16_f32                      (not supported)\n",
      "ggml_metal_init: skipping kernel_cpy_bf16_bf16                     (not supported)\n",
      "whisper_backend_init: using BLAS backend\n",
      "whisper_init_state: kv self size  =   18.87 MB\n",
      "whisper_init_state: kv cross size =   56.62 MB\n",
      "whisper_init_state: kv pad  size  =    4.72 MB\n",
      "whisper_init_state: compute buffer (conv)   =   23.37 MB\n",
      "whisper_init_state: compute buffer (encode) =  128.01 MB\n",
      "whisper_init_state: compute buffer (cross)  =    6.18 MB\n",
      "whisper_init_state: compute buffer (decode) =   98.19 MB\n",
      "\n",
      "system_info: n_threads = 4 / 14 | WHISPER : COREML = 0 | OPENVINO = 0 | Metal : EMBED_LIBRARY = 1 | CPU : NEON = 1 | ARM_FMA = 1 | FP16_VA = 1 | MATMUL_INT8 = 1 | DOTPROD = 1 | SME = 1 | ACCELERATE = 1 | AARCH64_REPACK = 1 | \n",
      "\n",
      "main: processing './Sound/01/chunk_039.wav' (1919854 samples, 120.0 sec), 4 threads, 1 processors, 5 beams + best of 5, lang = ko, task = transcribe, timestamps = 1 ...\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "[00:00:00.000 --> 00:00:26.000]   [두 번째 질문]\n",
      "[00:00:26.000 --> 00:00:33.000]   [두 번째 질문]\n",
      "[00:00:33.000 --> 00:00:37.000]   [두 번째 질문]\n",
      "[00:00:37.000 --> 00:00:46.000]   [두 번째 질문]\n",
      "[00:00:46.000 --> 00:01:01.000]   [두 번째 질문]\n",
      "[00:01:01.000 --> 00:01:23.000]   [두 번째 질문]\n",
      "[00:01:23.000 --> 00:01:47.000]   [두 번째 질문]\n",
      "[00:01:47.000 --> 00:01:59.000]   [두 번째 질문]\n",
      "\n",
      "✅ chunk_039 처리 완료 → ./Text/01/chunk_039_ggml-small.json\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "output_json: saving output to './Text/01/chunk_039_ggml-small.json'\n",
      "\n",
      "whisper_print_timings:     load time =   183.90 ms\n",
      "whisper_print_timings:     fallbacks =   1 p /   3 h\n",
      "whisper_print_timings:      mel time =    28.46 ms\n",
      "whisper_print_timings:   sample time =   162.87 ms /   736 runs (     0.22 ms per run)\n",
      "whisper_print_timings:   encode time =   689.48 ms /     6 runs (   114.91 ms per run)\n",
      "whisper_print_timings:   decode time =   793.40 ms /   208 runs (     3.81 ms per run)\n",
      "whisper_print_timings:   batchd time =   567.96 ms /   512 runs (     1.11 ms per run)\n",
      "whisper_print_timings:   prompt time =    39.27 ms /   206 runs (     0.19 ms per run)\n",
      "whisper_print_timings:    total time =  2522.52 ms\n",
      "ggml_metal_free: deallocating\n",
      "whisper_init_from_file_with_params_no_state: loading model from '/Users/aza1200/.cache/huggingface/hub/models--ggerganov--whisper.cpp/snapshots/5359861c739e955e79d9a303bcbc70fb988958b1/ggml-small.bin'\n",
      "whisper_init_with_params_no_state: use gpu    = 1\n",
      "whisper_init_with_params_no_state: flash attn = 0\n",
      "whisper_init_with_params_no_state: gpu_device = 0\n",
      "whisper_init_with_params_no_state: dtw        = 0\n",
      "whisper_init_with_params_no_state: devices    = 3\n",
      "whisper_init_with_params_no_state: backends   = 3\n",
      "whisper_model_load: loading model\n",
      "whisper_model_load: n_vocab       = 51865\n",
      "whisper_model_load: n_audio_ctx   = 1500\n",
      "whisper_model_load: n_audio_state = 768\n",
      "whisper_model_load: n_audio_head  = 12\n",
      "whisper_model_load: n_audio_layer = 12\n",
      "whisper_model_load: n_text_ctx    = 448\n",
      "whisper_model_load: n_text_state  = 768\n",
      "whisper_model_load: n_text_head   = 12\n",
      "whisper_model_load: n_text_layer  = 12\n",
      "whisper_model_load: n_mels        = 80\n",
      "whisper_model_load: ftype         = 1\n",
      "whisper_model_load: qntvr         = 0\n",
      "whisper_model_load: type          = 3 (small)\n",
      "whisper_model_load: adding 1608 extra tokens\n",
      "whisper_model_load: n_langs       = 99\n",
      "whisper_model_load:        Metal total size =   487.01 MB\n",
      "whisper_model_load: model size    =  487.01 MB\n",
      "whisper_backend_init_gpu: using Metal backend\n",
      "ggml_metal_init: allocating\n",
      "ggml_metal_init: found device: Apple M4 Pro\n",
      "ggml_metal_init: picking default device: Apple M4 Pro\n",
      "ggml_metal_load_library: using embedded metal library\n",
      "ggml_metal_init: GPU name:   Apple M4 Pro\n",
      "ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)\n",
      "ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)\n",
      "ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)\n",
      "ggml_metal_init: simdgroup reduction   = true\n",
      "ggml_metal_init: simdgroup matrix mul. = true\n",
      "ggml_metal_init: has residency sets    = true\n",
      "ggml_metal_init: has bfloat            = true\n",
      "ggml_metal_init: use bfloat            = false\n",
      "ggml_metal_init: hasUnifiedMemory      = true\n",
      "ggml_metal_init: recommendedMaxWorkingSetSize  = 38654.71 MB\n",
      "ggml_metal_init: skipping kernel_get_rows_bf16                     (not supported)\n",
      "ggml_metal_init: skipping kernel_mul_mv_bf16_f32                   (not supported)\n",
      "ggml_metal_init: skipping kernel_mul_mv_bf16_f32_1row              (not supported)\n",
      "ggml_metal_init: skipping kernel_mul_mv_bf16_f32_l4                (not supported)\n",
      "ggml_metal_init: skipping kernel_mul_mv_bf16_bf16                  (not supported)\n",
      "ggml_metal_init: skipping kernel_mul_mv_id_bf16_f32                (not supported)\n",
      "ggml_metal_init: skipping kernel_mul_mm_bf16_f32                   (not supported)\n",
      "ggml_metal_init: skipping kernel_mul_mm_id_bf16_f32                (not supported)\n",
      "ggml_metal_init: skipping kernel_flash_attn_ext_bf16_h64           (not supported)\n",
      "ggml_metal_init: skipping kernel_flash_attn_ext_bf16_h80           (not supported)\n",
      "ggml_metal_init: skipping kernel_flash_attn_ext_bf16_h96           (not supported)\n",
      "ggml_metal_init: skipping kernel_flash_attn_ext_bf16_h112          (not supported)\n",
      "ggml_metal_init: skipping kernel_flash_attn_ext_bf16_h128          (not supported)\n",
      "ggml_metal_init: skipping kernel_flash_attn_ext_bf16_h192          (not supported)\n",
      "ggml_metal_init: skipping kernel_flash_attn_ext_bf16_hk192_hv128   (not supported)\n",
      "ggml_metal_init: skipping kernel_flash_attn_ext_bf16_h256          (not supported)\n",
      "ggml_metal_init: skipping kernel_flash_attn_ext_vec_bf16_h128      (not supported)\n",
      "ggml_metal_init: skipping kernel_flash_attn_ext_vec_bf16_h192      (not supported)\n",
      "ggml_metal_init: skipping kernel_flash_attn_ext_vec_bf16_hk192_hv128 (not supported)\n",
      "ggml_metal_init: skipping kernel_flash_attn_ext_vec_bf16_h256      (not supported)\n",
      "ggml_metal_init: skipping kernel_cpy_f32_bf16                      (not supported)\n",
      "ggml_metal_init: skipping kernel_cpy_bf16_f32                      (not supported)\n",
      "ggml_metal_init: skipping kernel_cpy_bf16_bf16                     (not supported)\n",
      "whisper_backend_init: using BLAS backend\n",
      "whisper_init_state: kv self size  =   18.87 MB\n",
      "whisper_init_state: kv cross size =   56.62 MB\n",
      "whisper_init_state: kv pad  size  =    4.72 MB\n",
      "whisper_init_state: compute buffer (conv)   =   23.37 MB\n",
      "whisper_init_state: compute buffer (encode) =  128.01 MB\n",
      "whisper_init_state: compute buffer (cross)  =    6.18 MB\n",
      "whisper_init_state: compute buffer (decode) =   98.19 MB\n",
      "\n",
      "system_info: n_threads = 4 / 14 | WHISPER : COREML = 0 | OPENVINO = 0 | Metal : EMBED_LIBRARY = 1 | CPU : NEON = 1 | ARM_FMA = 1 | FP16_VA = 1 | MATMUL_INT8 = 1 | DOTPROD = 1 | SME = 1 | ACCELERATE = 1 | AARCH64_REPACK = 1 | \n",
      "\n",
      "main: processing './Sound/01/chunk_040.wav' (1919717 samples, 120.0 sec), 4 threads, 1 processors, 5 beams + best of 5, lang = ko, task = transcribe, timestamps = 1 ...\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "[00:00:00.000 --> 00:00:03.720]   여기 이제 우리 오버니제이션메인으로 들어오셔가지고\n",
      "[00:00:03.720 --> 00:00:07.000]   배포 스토리를 새로 하나 만들거에요. 근데\n",
      "[00:00:07.000 --> 00:00:09.160]   어 이제\n",
      "[00:00:09.160 --> 00:00:11.620]   저번에 보니까 이제 소스코도 안에\n",
      "[00:00:11.620 --> 00:00:14.600]   포어네티스 형상들이 있는 경우가 있었어요.\n",
      "[00:00:14.600 --> 00:00:18.900]   이분은 없으신?\n",
      "[00:00:18.900 --> 00:00:23.500]   혹시 배포 뭘로 하셨어요? 그때? 각자 배포 해보셨을 때?\n",
      "[00:00:23.500 --> 00:00:28.580]   포어네티스 배포 할 때? 헬렌 배포 이런거 할 때?\n",
      "[00:00:28.580 --> 00:00:30.960]   아 그래요? 그럼 거기에 배포 형상들이 있어요.\n",
      "[00:00:30.960 --> 00:00:34.900]   각자 하나씩? 그러면 그거 활용하시면 될 것 같고\n",
      "[00:00:34.900 --> 00:00:39.300]   동일하게 해야되는데 우리는\n",
      "[00:00:39.300 --> 00:00:44.120]   기돕스로, HTTPS로 배포를 해볼게요. 그러면\n",
      "[00:00:44.120 --> 00:00:47.360]   지금 어제 이렇게 올려주신 랩코스토리는\n",
      "[00:00:47.360 --> 00:00:52.260]   소스코드 랩코스토리라고 하는거에요. 그러니까 어플리케이션 개발을 위한 소스코드들만\n",
      "[00:00:52.260 --> 00:00:53.880]   저장을 해둘거고\n",
      "[00:00:53.880 --> 00:00:56.540]   우리는 이제 기돕스 랩포를 하나 만들어야 되죠. 여기\n",
      "[00:00:56.540 --> 00:00:59.760]   ARGO CD 전용으로 기돕스 랩포를 하나 만들거에요. 그래서\n",
      "[00:00:59.760 --> 00:01:04.640]   그 두개의 형상을 분리하는게 좋겠죠. 아무래도 개발용이랑 운영용이 달라져야 되니까\n",
      "[00:01:04.640 --> 00:01:05.660]   그래서\n",
      "[00:01:05.660 --> 00:01:09.040]   여기도 그냥 어제처럼 간단하게 레이밍 롤을 제가 드릴게요.\n",
      "[00:01:09.040 --> 00:01:12.280]   만약에 이 분명 위에 계시니까 이거 쓸게요.\n",
      "[00:01:12.280 --> 00:01:14.400]   이 분 이시라면\n",
      "[00:01:14.400 --> 00:01:16.640]   여기에다가 그냥 기돕스도\n",
      "[00:01:16.640 --> 00:01:18.720]   약간 이런식으로\n",
      "[00:01:18.720 --> 00:01:21.600]   랩포스토리를 하나씩 만들어주시면 돼요.\n",
      "[00:01:21.600 --> 00:01:30.260]   뒤에 하이폰 기록스만 붙여주세요.\n",
      "[00:01:30.260 --> 00:01:32.460]   아 그래요? 그러면은\n",
      "[00:01:32.460 --> 00:01:39.460]   ARGO CD 하기 전이니까 지금은 개발 링으로 GTNA부터 하셔도 될거 같아요.\n",
      "[00:01:39.460 --> 00:01:59.960]   혹시 그 프라이빗 헬름 레지스트리는 어디에 구참하시나요?\n",
      "\n",
      "✅ chunk_040 처리 완료 → ./Text/01/chunk_040_ggml-small.json\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "output_json: saving output to './Text/01/chunk_040_ggml-small.json'\n",
      "\n",
      "whisper_print_timings:     load time =   192.72 ms\n",
      "whisper_print_timings:     fallbacks =   0 p /   0 h\n",
      "whisper_print_timings:      mel time =    28.80 ms\n",
      "whisper_print_timings:   sample time =   401.99 ms /  2524 runs (     0.16 ms per run)\n",
      "whisper_print_timings:   encode time =   591.95 ms /     5 runs (   118.39 ms per run)\n",
      "whisper_print_timings:   decode time =    80.56 ms /    19 runs (     4.24 ms per run)\n",
      "whisper_print_timings:   batchd time =  2327.57 ms /  2483 runs (     0.94 ms per run)\n",
      "whisper_print_timings:   prompt time =    96.01 ms /   816 runs (     0.12 ms per run)\n",
      "whisper_print_timings:    total time =  3793.90 ms\n",
      "ggml_metal_free: deallocating\n",
      "whisper_init_from_file_with_params_no_state: loading model from '/Users/aza1200/.cache/huggingface/hub/models--ggerganov--whisper.cpp/snapshots/5359861c739e955e79d9a303bcbc70fb988958b1/ggml-small.bin'\n",
      "whisper_init_with_params_no_state: use gpu    = 1\n",
      "whisper_init_with_params_no_state: flash attn = 0\n",
      "whisper_init_with_params_no_state: gpu_device = 0\n",
      "whisper_init_with_params_no_state: dtw        = 0\n",
      "whisper_init_with_params_no_state: devices    = 3\n",
      "whisper_init_with_params_no_state: backends   = 3\n",
      "whisper_model_load: loading model\n",
      "whisper_model_load: n_vocab       = 51865\n",
      "whisper_model_load: n_audio_ctx   = 1500\n",
      "whisper_model_load: n_audio_state = 768\n",
      "whisper_model_load: n_audio_head  = 12\n",
      "whisper_model_load: n_audio_layer = 12\n",
      "whisper_model_load: n_text_ctx    = 448\n",
      "whisper_model_load: n_text_state  = 768\n",
      "whisper_model_load: n_text_head   = 12\n",
      "whisper_model_load: n_text_layer  = 12\n",
      "whisper_model_load: n_mels        = 80\n",
      "whisper_model_load: ftype         = 1\n",
      "whisper_model_load: qntvr         = 0\n",
      "whisper_model_load: type          = 3 (small)\n",
      "whisper_model_load: adding 1608 extra tokens\n",
      "whisper_model_load: n_langs       = 99\n",
      "whisper_model_load:        Metal total size =   487.01 MB\n",
      "whisper_model_load: model size    =  487.01 MB\n",
      "whisper_backend_init_gpu: using Metal backend\n",
      "ggml_metal_init: allocating\n",
      "ggml_metal_init: found device: Apple M4 Pro\n",
      "ggml_metal_init: picking default device: Apple M4 Pro\n",
      "ggml_metal_load_library: using embedded metal library\n",
      "ggml_metal_init: GPU name:   Apple M4 Pro\n",
      "ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)\n",
      "ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)\n",
      "ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)\n",
      "ggml_metal_init: simdgroup reduction   = true\n",
      "ggml_metal_init: simdgroup matrix mul. = true\n",
      "ggml_metal_init: has residency sets    = true\n",
      "ggml_metal_init: has bfloat            = true\n",
      "ggml_metal_init: use bfloat            = false\n",
      "ggml_metal_init: hasUnifiedMemory      = true\n",
      "ggml_metal_init: recommendedMaxWorkingSetSize  = 38654.71 MB\n",
      "ggml_metal_init: skipping kernel_get_rows_bf16                     (not supported)\n",
      "ggml_metal_init: skipping kernel_mul_mv_bf16_f32                   (not supported)\n",
      "ggml_metal_init: skipping kernel_mul_mv_bf16_f32_1row              (not supported)\n",
      "ggml_metal_init: skipping kernel_mul_mv_bf16_f32_l4                (not supported)\n",
      "ggml_metal_init: skipping kernel_mul_mv_bf16_bf16                  (not supported)\n",
      "ggml_metal_init: skipping kernel_mul_mv_id_bf16_f32                (not supported)\n",
      "ggml_metal_init: skipping kernel_mul_mm_bf16_f32                   (not supported)\n",
      "ggml_metal_init: skipping kernel_mul_mm_id_bf16_f32                (not supported)\n",
      "ggml_metal_init: skipping kernel_flash_attn_ext_bf16_h64           (not supported)\n",
      "ggml_metal_init: skipping kernel_flash_attn_ext_bf16_h80           (not supported)\n",
      "ggml_metal_init: skipping kernel_flash_attn_ext_bf16_h96           (not supported)\n",
      "ggml_metal_init: skipping kernel_flash_attn_ext_bf16_h112          (not supported)\n",
      "ggml_metal_init: skipping kernel_flash_attn_ext_bf16_h128          (not supported)\n",
      "ggml_metal_init: skipping kernel_flash_attn_ext_bf16_h192          (not supported)\n",
      "ggml_metal_init: skipping kernel_flash_attn_ext_bf16_hk192_hv128   (not supported)\n",
      "ggml_metal_init: skipping kernel_flash_attn_ext_bf16_h256          (not supported)\n",
      "ggml_metal_init: skipping kernel_flash_attn_ext_vec_bf16_h128      (not supported)\n",
      "ggml_metal_init: skipping kernel_flash_attn_ext_vec_bf16_h192      (not supported)\n",
      "ggml_metal_init: skipping kernel_flash_attn_ext_vec_bf16_hk192_hv128 (not supported)\n",
      "ggml_metal_init: skipping kernel_flash_attn_ext_vec_bf16_h256      (not supported)\n",
      "ggml_metal_init: skipping kernel_cpy_f32_bf16                      (not supported)\n",
      "ggml_metal_init: skipping kernel_cpy_bf16_f32                      (not supported)\n",
      "ggml_metal_init: skipping kernel_cpy_bf16_bf16                     (not supported)\n",
      "whisper_backend_init: using BLAS backend\n",
      "whisper_init_state: kv self size  =   18.87 MB\n",
      "whisper_init_state: kv cross size =   56.62 MB\n",
      "whisper_init_state: kv pad  size  =    4.72 MB\n",
      "whisper_init_state: compute buffer (conv)   =   23.37 MB\n",
      "whisper_init_state: compute buffer (encode) =  128.01 MB\n",
      "whisper_init_state: compute buffer (cross)  =    6.18 MB\n",
      "whisper_init_state: compute buffer (decode) =   98.19 MB\n",
      "\n",
      "system_info: n_threads = 4 / 14 | WHISPER : COREML = 0 | OPENVINO = 0 | Metal : EMBED_LIBRARY = 1 | CPU : NEON = 1 | ARM_FMA = 1 | FP16_VA = 1 | MATMUL_INT8 = 1 | DOTPROD = 1 | SME = 1 | ACCELERATE = 1 | AARCH64_REPACK = 1 | \n",
      "\n",
      "main: processing './Sound/01/chunk_041.wav' (1075053 samples, 67.2 sec), 4 threads, 1 processors, 5 beams + best of 5, lang = ko, task = transcribe, timestamps = 1 ...\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "[00:00:00.000 --> 00:00:02.000]   전화해 주시는 거 같아요.\n",
      "[00:00:02.000 --> 00:00:05.000]   (댓글 읽는 중)\n",
      "[00:00:05.000 --> 00:00:07.000]   아 거기 있어요?\n",
      "[00:00:07.000 --> 00:00:09.000]   (댓글 읽는 중)\n",
      "[00:00:09.000 --> 00:00:11.000]   알겠습니다.\n",
      "[00:00:11.000 --> 00:00:13.000]   (댓글 읽는 중)\n",
      "[00:00:13.000 --> 00:00:17.000]   그래서 래퍼 하나씩 만들어 보네요.\n",
      "[00:00:17.000 --> 00:00:20.000]   (댓글 읽는 중)\n",
      "[00:00:20.000 --> 00:00:22.000]   (댓글 읽는 중)\n",
      "[00:00:22.000 --> 00:00:25.000]   (댓글 읽는 중)\n",
      "[00:00:25.000 --> 00:00:28.000]   (댓글 읽는 중)\n",
      "[00:00:28.000 --> 00:00:30.000]   (댓글 읽는 중)\n",
      "[00:00:30.000 --> 00:00:32.000]   (댓글 읽는 중)\n",
      "[00:00:32.000 --> 00:00:34.000]   (댓글 읽는 중)\n",
      "[00:00:34.000 --> 00:00:36.000]   (댓글 읽는 중)\n",
      "[00:00:36.000 --> 00:00:38.000]   (댓글 읽는 중)\n",
      "[00:00:38.000 --> 00:00:40.000]   (댓글 읽는 중)\n",
      "[00:00:40.000 --> 00:00:42.000]   (댓글 읽는 중)\n",
      "[00:00:42.000 --> 00:00:44.000]   (댓글 읽는 중)\n",
      "[00:00:44.000 --> 00:00:46.000]   (댓글 읽는 중)\n",
      "[00:00:46.000 --> 00:00:48.000]   (댓글 읽는 중)\n",
      "[00:00:48.000 --> 00:00:50.000]   (댓글 읽는 중)\n",
      "[00:00:50.000 --> 00:00:52.000]   (댓글 읽는 중)\n",
      "[00:00:52.000 --> 00:00:54.000]   (댓글 읽는 중)\n",
      "[00:00:54.000 --> 00:00:56.000]   (댓글 읽는 중)\n",
      "[00:00:56.000 --> 00:00:58.000]   (댓글 읽는 중)\n",
      "[00:00:58.000 --> 00:01:00.000]   (댓글 읽는 중)\n",
      "[00:01:00.000 --> 00:01:02.000]   (댓글 읽는 중)\n",
      "[00:01:02.000 --> 00:01:04.000]   (댓글 읽는 중)\n",
      "[00:01:04.000 --> 00:01:06.000]   (댓글 읽는 중)\n",
      "\n",
      "✅ chunk_041 처리 완료 → ./Text/01/chunk_041_ggml-small.json\n",
      "✅ 전체 병합 완료 → ./Text/01/01_merged_ggml-small.json\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "output_json: saving output to './Text/01/chunk_041_ggml-small.json'\n",
      "\n",
      "whisper_print_timings:     load time =   199.82 ms\n",
      "whisper_print_timings:     fallbacks =   1 p /   0 h\n",
      "whisper_print_timings:      mel time =    16.60 ms\n",
      "whisper_print_timings:   sample time =   262.86 ms /  1679 runs (     0.16 ms per run)\n",
      "whisper_print_timings:   encode time =   355.43 ms /     3 runs (   118.48 ms per run)\n",
      "whisper_print_timings:   decode time =    49.51 ms /    12 runs (     4.13 ms per run)\n",
      "whisper_print_timings:   batchd time =  1785.70 ms /  1653 runs (     1.08 ms per run)\n",
      "whisper_print_timings:   prompt time =    44.13 ms /   343 runs (     0.13 ms per run)\n",
      "whisper_print_timings:    total time =  2781.29 ms\n",
      "ggml_metal_free: deallocating\n"
     ]
    }
   ],
   "source": [
    "# ✅ .wav 파일만 필터링\n",
    "sound_dir = f\"./Sound/{LECTURE_FILE_NAME}\"\n",
    "chunk_files = sorted([\n",
    "    f for f in os.listdir(sound_dir)\n",
    "    if f.endswith(\".wav\")\n",
    "])\n",
    "\n",
    "chunk_count = len(chunk_files)\n",
    "print(\"📦 chunk count:\", chunk_count)\n",
    "print(\"🎵 chunk files:\", chunk_files[:5], \"...\")\n",
    "\n",
    "# ✅ 병합용 경로 설정\n",
    "model_label = MODEL_NAME.replace(\".bin\", \"\")\n",
    "output_merged_path = f\"./Text/{LECTURE_FILE_NAME}/{LECTURE_FILE_NAME}_merged_{model_label}.json\"\n",
    "merged_segments = []\n",
    "\n",
    "# ✅ 42개의 chunk 처리\n",
    "for i in range(chunk_count):\n",
    "    chunk_id = f\"chunk_{i:03d}\"\n",
    "\n",
    "    # whisper 처리 (여기 함수는 기존 그대로 사용)\n",
    "    whisper_cpp_transcribe(chunk_id, MODEL_NAME)\n",
    "\n",
    "    # 결과 JSON 읽기\n",
    "    json_path = f\"./Text/{LECTURE_FILE_NAME}/{chunk_id}_{model_label}.json\"\n",
    "    if os.path.exists(json_path):\n",
    "        with open(json_path, \"r\", encoding=\"utf-8\") as f:\n",
    "            data = json.load(f)\n",
    "            merged_segments.extend(data.get(\"transcription\", []))\n",
    "\n",
    "# ✅ 최종 병합된 JSON 저장\n",
    "with open(output_merged_path, \"w\", encoding=\"utf-8\") as f:\n",
    "    json.dump({\"transcription\": merged_segments}, f, indent=2, ensure_ascii=False)\n",
    "\n",
    "print(f\"✅ 전체 병합 완료 → {output_merged_path}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "id": "f3e71e72",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[{'timestamps': {'from': '00:00:00,000', 'to': '00:00:20,980'},\n",
       "  'offsets': {'from': 0, 'to': 20980},\n",
       "  'text': ' (인터뷰)'},\n",
       " {'timestamps': {'from': '00:00:20,980', 'to': '00:00:25,600'},\n",
       "  'offsets': {'from': 20980, 'to': 25600},\n",
       "  'text': ' 혹시 그 여러분 젠킨스라는 투를 해본 적이 있다'},\n",
       " {'timestamps': {'from': '00:00:25,600', 'to': '00:00:27,440'},\n",
       "  'offsets': {'from': 25600, 'to': 27440},\n",
       "  'text': ' 젠킨스 써본 적 있다'},\n",
       " {'timestamps': {'from': '00:00:27,440', 'to': '00:00:29,640'},\n",
       "  'offsets': {'from': 27440, 'to': 29640},\n",
       "  'text': ' (인터뷰)'},\n",
       " {'timestamps': {'from': '00:00:29,640', 'to': '00:00:32,480'},\n",
       "  'offsets': {'from': 29640, 'to': 32480},\n",
       "  'text': ' 혹시 써보실 때 어느 정도 쓰시니 까지 써보셨어요'},\n",
       " {'timestamps': {'from': '00:00:32,480', 'to': '00:00:36,160'},\n",
       "  'offsets': {'from': 32480, 'to': 36160},\n",
       "  'text': ' 그냥 사용자 입장에서 파이프라인이 짜는 정도?'},\n",
       " {'timestamps': {'from': '00:00:36,160', 'to': '00:00:39,580'},\n",
       "  'offsets': {'from': 36160, 'to': 39580},\n",
       "  'text': ' 짜고 젠킨스로 그러면 뭘 하셨어요 혹시?'},\n",
       " {'timestamps': {'from': '00:00:39,580', 'to': '00:00:40,740'},\n",
       "  'offsets': {'from': 39580, 'to': 40740},\n",
       "  'text': ' (인터뷰)'},\n",
       " {'timestamps': {'from': '00:00:40,740', 'to': '00:00:44,720'},\n",
       "  'offsets': {'from': 40740, 'to': 44720},\n",
       "  'text': ' 씨아이만 했어요 젠킨스로'},\n",
       " {'timestamps': {'from': '00:00:44,720', 'to': '00:00:47,160'},\n",
       "  'offsets': {'from': 44720, 'to': 47160},\n",
       "  'text': ' 저희는 어제 컨테이너 빌드 를 했기 때문에'},\n",
       " {'timestamps': {'from': '00:00:47,160', 'to': '00:00:50,040'},\n",
       "  'offsets': {'from': 47160, 'to': 50040},\n",
       "  'text': ' 오늘은 rgo cd로 별풀할 건데'},\n",
       " {'timestamps': {'from': '00:00:50,040', 'to': '00:00:52,920'},\n",
       "  'offsets': {'from': 50040, 'to': 52920},\n",
       "  'text': ' kt 안에서 이제 vmv4 대상으로는'},\n",
       " {'timestamps': {'from': '00:00:52,920', 'to': '00:00:55,440'},\n",
       "  'offsets': {'from': 52920, 'to': 55440},\n",
       "  'text': ' cd2로 젠킨스를 쓰고 있거든요'},\n",
       " {'timestamps': {'from': '00:00:55,440', 'to': '00:01:06,560'},\n",
       "  'offsets': {'from': 55440, 'to': 66560},\n",
       "  'text': ' 그래서 그래서 이제 저희 개발맥을 써서'},\n",
       " {'timestamps': {'from': '00:01:06,560', 'to': '00:01:08,960'},\n",
       "  'offsets': {'from': 66560, 'to': 68960},\n",
       "  'text': ' 간단하게 이제 토커업을 해서 이미지 가져와서 안에'},\n",
       " {'timestamps': {'from': '00:01:08,960', 'to': '00:01:12,520'},\n",
       "  'offsets': {'from': 68960, 'to': 72520},\n",
       "  'text': ' 어떻게 부성되어 있고 어떻게 쓰는 지 정도만 확인을 하고'},\n",
       " {'timestamps': {'from': '00:01:12,520', 'to': '00:01:16,600'},\n",
       "  'offsets': {'from': 72520, 'to': 76600},\n",
       "  'text': ' 그 다음에 저희가 교육용으로 구성해서 rgo cd로 넘어갈게요'},\n",
       " {'timestamps': {'from': '00:01:16,600', 'to': '00:01:22,240'},\n",
       "  'offsets': {'from': 76600, 'to': 82240},\n",
       "  'text': ' 그러면 다들 토커업을 쓸 수 있는 개발맥을 가져와 보시고'},\n",
       " {'timestamps': {'from': '00:01:22,240', 'to': '00:01:29,200'},\n",
       "  'offsets': {'from': 82240, 'to': 89200},\n",
       "  'text': ' 그걸 토커업에 이렇게 젠킨스 슬래쉬 젠킨스로 검색을 하면'},\n",
       " {'timestamps': {'from': '00:01:29,200', 'to': '00:01:32,520'},\n",
       "  'offsets': {'from': 89200, 'to': 92520},\n",
       "  'text': ' 젠킨스라는 오픈소스를 만드는 곳에서'},\n",
       " {'timestamps': {'from': '00:01:32,520', 'to': '00:01:35,440'},\n",
       "  'offsets': {'from': 92520, 'to': 95440},\n",
       "  'text': ' 배포하는 이미지가 있거든요 컨테이너 이미지'},\n",
       " {'timestamps': {'from': '00:01:35,440', 'to': '00:01:38,520'},\n",
       "  'offsets': {'from': 95440, 'to': 98520},\n",
       "  'text': ' 이거를 각자 터미널 로컬로 토커풀 받으셔가지고'},\n",
       " {'timestamps': {'from': '00:01:38,520', 'to': '00:01:40,560'},\n",
       "  'offsets': {'from': 98520, 'to': 100560},\n",
       "  'text': ' 레이티스트로 받으셔도 돼요'},\n",
       " {'timestamps': {'from': '00:01:40,560', 'to': '00:01:43,360'},\n",
       "  'offsets': {'from': 100560, 'to': 103360},\n",
       "  'text': ' 카피해서 명령어 넣은 다음에'},\n",
       " {'timestamps': {'from': '00:01:43,360', 'to': '00:01:47,280'},\n",
       "  'offsets': {'from': 103360, 'to': 107280},\n",
       "  'text': ' 이제 로컬PC에 다운로드 받아보겠습니다'},\n",
       " {'timestamps': {'from': '00:01:47,280', 'to': '00:01:52,040'},\n",
       "  'offsets': {'from': 107280, 'to': 112040},\n",
       "  'text': ' 수업을 직접 들어가서 구성도 되고 명령어를 드릴게요'},\n",
       " {'timestamps': {'from': '00:00:00,000', 'to': '00:00:09,880'},\n",
       "  'offsets': {'from': 0, 'to': 9880},\n",
       "  'text': ' 공사는 4월 27일까지 예정되어 있으며 공사의 기간 중 다소의 소음과 진동이 발생될 수 있고'},\n",
       " {'timestamps': {'from': '00:00:09,880', 'to': '00:00:16,640'},\n",
       "  'offsets': {'from': 9880, 'to': 16640},\n",
       "  'text': ' 비상 엘리베이서 오랜 제한의 불편사항이 출해될 수 있으니 양해해 주시기 바랍니다.'},\n",
       " {'timestamps': {'from': '00:00:16,640', 'to': '00:00:19,120'},\n",
       "  'offsets': {'from': 16640, 'to': 19120},\n",
       "  'text': ' 다시 한 번 알려드립니다.'},\n",
       " {'timestamps': {'from': '00:00:19,120', 'to': '00:00:25,120'},\n",
       "  'offsets': {'from': 19120, 'to': 25120},\n",
       "  'text': ' 현재 지경동 살칭에서 교육장 공축 공사가 진행되고 있습니다.'},\n",
       " {'timestamps': {'from': '00:00:25,120', 'to': '00:00:32,960'},\n",
       "  'offsets': {'from': 25120, 'to': 32960},\n",
       "  'text': ' 공사는 4월 27일까지 예정되어 있으며 공사의 기간 중 다소의 소음과 진동이 발생될 수 있고'},\n",
       " {'timestamps': {'from': '00:00:32,960', 'to': '00:00:39,680'},\n",
       "  'offsets': {'from': 32960, 'to': 39680},\n",
       "  'text': ' 비상 엘리베이서 오랜 제한의 불편사항이 출해물 수 있으니 양해해 주시기 바랍니다.'},\n",
       " {'timestamps': {'from': '00:00:39,680', 'to': '00:00:41,680'},\n",
       "  'offsets': {'from': 39680, 'to': 41680},\n",
       "  'text': ' 감사합니다.'},\n",
       " {'timestamps': {'from': '00:00:41,680', 'to': '00:00:45,680'},\n",
       "  'offsets': {'from': 41680, 'to': 45680},\n",
       "  'text': ' 다음 되셨나요?'},\n",
       " {'timestamps': {'from': '00:00:45,680', 'to': '00:00:51,520'},\n",
       "  'offsets': {'from': 45680, 'to': 51520},\n",
       "  'text': ' 도커플 되셨으면 저는 이제 사내 환경에서 보여드릴 거고 여러분은 편하게 개발맥에서 하시면 되는데'},\n",
       " {'timestamps': {'from': '00:00:51,520', 'to': '00:00:56,280'},\n",
       "  'offsets': {'from': 51520, 'to': 56280},\n",
       "  'text': ' 도커플 컨테이너를 실행하는 명령원은 잘 계시겠죠?'},\n",
       " {'timestamps': {'from': '00:00:56,280', 'to': '00:00:59,360'},\n",
       "  'offsets': {'from': 56280, 'to': 59360},\n",
       "  'text': ' 도커플은 대본으로 돌릴 거고'},\n",
       " {'timestamps': {'from': '00:00:59,360', 'to': '00:01:01,360'},\n",
       "  'offsets': {'from': 59360, 'to': 61360},\n",
       "  'text': ' 그다음에 컨테이너 이름 저하고'},\n",
       " {'timestamps': {'from': '00:01:01,360', 'to': '00:01:04,200'},\n",
       "  'offsets': {'from': 61360, 'to': 64200},\n",
       "  'text': ' 진킨스 뭐 더비스 이런 거 하고'},\n",
       " {'timestamps': {'from': '00:01:04,200', 'to': '00:01:05,120'},\n",
       "  'offsets': {'from': 64200, 'to': 65120},\n",
       "  'text': ' 포트를 열어줘야 돼요.'},\n",
       " {'timestamps': {'from': '00:01:05,120', 'to': '00:01:09,600'},\n",
       "  'offsets': {'from': 65120, 'to': 69600},\n",
       "  'text': ' 8080에 서비스 포트가 8080이라서 그냥 편하게 열어주시고'},\n",
       " {'timestamps': {'from': '00:01:09,600', 'to': '00:01:11,840'},\n",
       "  'offsets': {'from': 69600, 'to': 71840},\n",
       "  'text': ' 그다음에 컨테이너 이미지 있겠죠.'},\n",
       " {'timestamps': {'from': '00:01:11,840', 'to': '00:01:15,200'},\n",
       "  'offsets': {'from': 71840, 'to': 75200},\n",
       "  'text': ' 오레이 키스 이런 거 이길죠.'},\n",
       " {'timestamps': {'from': '00:01:15,200', 'to': '00:01:19,400'},\n",
       "  'offsets': {'from': 75200, 'to': 79400},\n",
       "  'text': ' 뭐 에러가 날 수도 있긴 한데 어쨌든 이런 식으로 명령어를 쓰셔서'},\n",
       " {'timestamps': {'from': '00:01:19,400', 'to': '00:01:23,680'},\n",
       "  'offsets': {'from': 79400, 'to': 83680},\n",
       "  'text': ' 지금 혹시 다른 거 8080 포트로 돌리고 계시면'},\n",
       " {'timestamps': {'from': '00:01:23,680', 'to': '00:01:28,360'},\n",
       "  'offsets': {'from': 83680, 'to': 88360},\n",
       "  'text': ' 옆에 있는 포트 포딩 해주는 포트를 그냥 커스테면서 바꿔주시면 되고'},\n",
       " {'timestamps': {'from': '00:01:28,360', 'to': '00:01:31,360'},\n",
       "  'offsets': {'from': 88360, 'to': 91360},\n",
       "  'text': ' 그렇게 해서 한 번 프로세스 띄워볼게요.'},\n",
       " {'timestamps': {'from': '00:01:31,360', 'to': '00:01:34,760'},\n",
       "  'offsets': {'from': 91360, 'to': 94760},\n",
       "  'text': ' 도커플 어느 명령어 실행할게요.'},\n",
       " {'timestamps': {'from': '00:01:34,760', 'to': '00:01:36,760'},\n",
       "  'offsets': {'from': 94760, 'to': 96760},\n",
       "  'text': ' 도커플이 오래 걸려요?'},\n",
       " {'timestamps': {'from': '00:01:36,760', 'to': '00:01:38,200'},\n",
       "  'offsets': {'from': 96760, 'to': 98200},\n",
       "  'text': ' 아 그래요?'},\n",
       " {'timestamps': {'from': '00:01:38,200', 'to': '00:01:41,440'},\n",
       "  'offsets': {'from': 98200, 'to': 101440},\n",
       "  'text': ' 혹시 컨테이너 프로세스 띄우셨나요?'},\n",
       " {'timestamps': {'from': '00:01:41,440', 'to': '00:01:46,680'},\n",
       "  'offsets': {'from': 101440, 'to': 106680},\n",
       "  'text': ' 만약에 제 명령어가 잘 안 먹으면 성공하신 분이 그냥 슬레게 공교를 주셔도 돼요.'},\n",
       " {'timestamps': {'from': '00:01:46,680', 'to': '00:01:50,160'},\n",
       "  'offsets': {'from': 106680, 'to': 110160},\n",
       "  'text': ' 저는 약간 가이드 시겨드로만 드리는 거라서'},\n",
       " {'timestamps': {'from': '00:01:50,160', 'to': '00:01:54,600'},\n",
       "  'offsets': {'from': 110160, 'to': 114600},\n",
       "  'text': ' 혹시 이진홍 님 승인됐다고 하는데'},\n",
       " {'timestamps': {'from': '00:01:54,600', 'to': '00:01:59,560'},\n",
       "  'offsets': {'from': 114600, 'to': 119560},\n",
       "  'text': ' 멤버 추가 해볼게요.'},\n",
       " {'timestamps': {'from': '00:00:00,000', 'to': '00:00:02,000'},\n",
       "  'offsets': {'from': 0, 'to': 2000},\n",
       "  'text': ' 기품지력시팀 맞으세요?'},\n",
       " {'timestamps': {'from': '00:00:02,000', 'to': '00:00:04,760'},\n",
       "  'offsets': {'from': 2000, 'to': 4760},\n",
       "  'text': ' 네 드디어 얻어드렸습니다.'},\n",
       " {'timestamps': {'from': '00:00:04,760', 'to': '00:00:06,760'},\n",
       "  'offsets': {'from': 4760, 'to': 6760},\n",
       "  'text': ' 감사합니다.'},\n",
       " {'timestamps': {'from': '00:00:06,760', 'to': '00:00:11,940'},\n",
       "  'offsets': {'from': 6760, 'to': 11940},\n",
       "  'text': ' 그래도 진호님은 오늘 배포할 이미지가 아직 없을 거잖아요.'},\n",
       " {'timestamps': {'from': '00:00:11,940', 'to': '00:00:18,280'},\n",
       "  'offsets': {'from': 11940, 'to': 18280},\n",
       "  'text': ' acr에다가 그거 이제 옆에 동지분 거 같이 오늘 배포 실습만 하는 거라서'},\n",
       " {'timestamps': {'from': '00:00:18,280', 'to': '00:00:22,160'},\n",
       "  'offsets': {'from': 18280, 'to': 22160},\n",
       "  'text': ' 호주 배꼽하고 ci는 다 흐름만 이해해줄 것 같아요.'},\n",
       " {'timestamps': {'from': '00:00:22,160', 'to': '00:00:27,760'},\n",
       "  'offsets': {'from': 22160, 'to': 27760},\n",
       "  'text': ' 처음에 이제 그래서 컨테이너 프로세스 띄우고 로커러스트 8080으로 웹접습 하면'},\n",
       " {'timestamps': {'from': '00:00:27,760', 'to': '00:00:31,980'},\n",
       "  'offsets': {'from': 27760, 'to': 31980},\n",
       "  'text': ' 이렇게 처음에 어드민 세덕을 하는 화면이 나와야 되거든요.'},\n",
       " {'timestamps': {'from': '00:00:31,980', 'to': '00:00:35,980'},\n",
       "  'offsets': {'from': 31980, 'to': 35980},\n",
       "  'text': ' 저희 나오셨네요.'},\n",
       " {'timestamps': {'from': '00:00:35,980', 'to': '00:00:37,980'},\n",
       "  'offsets': {'from': 35980, 'to': 37980},\n",
       "  'text': ' 저희는 저 화면이 안되고.'},\n",
       " {'timestamps': {'from': '00:00:37,980', 'to': '00:00:39,980'},\n",
       "  'offsets': {'from': 37980, 'to': 39980},\n",
       "  'text': ' 언급하시려고 하고 있었어요.'},\n",
       " {'timestamps': {'from': '00:00:39,980', 'to': '00:00:41,980'},\n",
       "  'offsets': {'from': 39980, 'to': 41980},\n",
       "  'text': ' 아 그래? 잠시만요.'},\n",
       " {'timestamps': {'from': '00:00:41,980', 'to': '00:00:49,020'},\n",
       "  'offsets': {'from': 41980, 'to': 49020},\n",
       "  'text': ' 아니 이게 이미지가 약간 다른데.'},\n",
       " {'timestamps': {'from': '00:00:49,020', 'to': '00:00:55,640'},\n",
       "  'offsets': {'from': 49020, 'to': 55640},\n",
       "  'text': ' 그러면 그 벌랍진 킨스로 이제 어드민 패스워드가 필요한데'},\n",
       " {'timestamps': {'from': '00:00:55,640', 'to': '00:01:00,620'},\n",
       "  'offsets': {'from': 55640, 'to': 60620},\n",
       "  'text': ' 이게 저랑 이미지가 약간 달라질 거 같고 그 경우에는 그냥 터미널 하나 더 여셔서'},\n",
       " {'timestamps': {'from': '00:01:00,620', 'to': '00:01:01,620'},\n",
       "  'offsets': {'from': 60620, 'to': 61620},\n",
       "  'text': ' 선생님께 공유해 드릴게요.'},\n",
       " {'timestamps': {'from': '00:01:01,620', 'to': '00:01:03,620'},\n",
       "  'offsets': {'from': 61620, 'to': 63620},\n",
       "  'text': ' 터미널 하나 더 열어서'},\n",
       " {'timestamps': {'from': '00:01:03,620', 'to': '00:01:06,620'},\n",
       "  'offsets': {'from': 63620, 'to': 66620},\n",
       "  'text': ' 김소원님께서 공유해 주셨네요.'},\n",
       " {'timestamps': {'from': '00:01:06,620', 'to': '00:01:07,620'},\n",
       "  'offsets': {'from': 66620, 'to': 67620},\n",
       "  'text': ' 터미널 하나 더 열어서.'},\n",
       " {'timestamps': {'from': '00:01:07,620', 'to': '00:01:13,620'},\n",
       "  'offsets': {'from': 67620, 'to': 73620},\n",
       "  'text': ' 아니 아까 데몬 뒤로 띄었으니까 그냥 바로 그 터미널에서 이 아래 명료 복사 무척 하시면 되거든요.'},\n",
       " {'timestamps': {'from': '00:01:13,620', 'to': '00:01:17,620'},\n",
       "  'offsets': {'from': 73620, 'to': 77620},\n",
       "  'text': ' 그럼 이게 초기 세덕 비밀번호가 이제 프로세스에 남아 있는 거예요.'},\n",
       " {'timestamps': {'from': '00:01:17,620', 'to': '00:01:22,620'},\n",
       "  'offsets': {'from': 77620, 'to': 82620},\n",
       "  'text': ' 그래서 이걸로 확인하셔서 넣으시면 저랑 같은 화면이 나올 겁니다.'},\n",
       " {'timestamps': {'from': '00:01:22,620', 'to': '00:01:26,620'},\n",
       "  'offsets': {'from': 82620, 'to': 86620},\n",
       "  'text': ' 플로그인 깔면 됩니다.'},\n",
       " {'timestamps': {'from': '00:01:26,620', 'to': '00:01:29,600'},\n",
       "  'offsets': {'from': 86620, 'to': 89600},\n",
       "  'text': ' 그냥 추천해 주는 걸로 다 갈아주시면 되요.'},\n",
       " {'timestamps': {'from': '00:01:29,600', 'to': '00:01:33,600'},\n",
       "  'offsets': {'from': 89600, 'to': 93600},\n",
       "  'text': ' 네.'},\n",
       " {'timestamps': {'from': '00:01:33,600', 'to': '00:01:35,600'},\n",
       "  'offsets': {'from': 93600, 'to': 95600},\n",
       "  'text': ' 아니 안 보여요.'},\n",
       " {'timestamps': {'from': '00:01:35,600', 'to': '00:01:59,600'},\n",
       "  'offsets': {'from': 95600, 'to': 119600},\n",
       "  'text': ' (댓글 읽는 중)'},\n",
       " {'timestamps': {'from': '00:00:00,000', 'to': '00:00:02,000'},\n",
       "  'offsets': {'from': 0, 'to': 2000},\n",
       "  'text': ' 괜찮았으시다면 안 됐겠지?'},\n",
       " {'timestamps': {'from': '00:00:30,000', 'to': '00:00:32,000'},\n",
       "  'offsets': {'from': 30000, 'to': 32000},\n",
       "  'text': ' 괜찮았으시다면 안 됐겠지?'},\n",
       " {'timestamps': {'from': '00:00:32,000', 'to': '00:00:34,000'},\n",
       "  'offsets': {'from': 32000, 'to': 34000},\n",
       "  'text': ' 괜찮았으시다면 안 됐겠지?'},\n",
       " {'timestamps': {'from': '00:00:34,000', 'to': '00:00:36,000'},\n",
       "  'offsets': {'from': 34000, 'to': 36000},\n",
       "  'text': ' 괜찮았으시다면 안 됐겠지?'},\n",
       " {'timestamps': {'from': '00:00:36,000', 'to': '00:00:38,000'},\n",
       "  'offsets': {'from': 36000, 'to': 38000},\n",
       "  'text': ' 괜찮았으시다면 안 됐겠지?'},\n",
       " {'timestamps': {'from': '00:00:38,000', 'to': '00:00:40,000'},\n",
       "  'offsets': {'from': 38000, 'to': 40000},\n",
       "  'text': ' 괜찮았으시다면 안 됐겠지?'},\n",
       " {'timestamps': {'from': '00:00:40,000', 'to': '00:00:42,000'},\n",
       "  'offsets': {'from': 40000, 'to': 42000},\n",
       "  'text': ' 괜찮았으시다면 안 됐겠지?'},\n",
       " {'timestamps': {'from': '00:00:42,000', 'to': '00:00:44,000'},\n",
       "  'offsets': {'from': 42000, 'to': 44000},\n",
       "  'text': ' 괜찮았으시다면 안 됐겠지?'},\n",
       " {'timestamps': {'from': '00:00:44,000', 'to': '00:00:46,000'},\n",
       "  'offsets': {'from': 44000, 'to': 46000},\n",
       "  'text': ' 괜찮았으시다면 안 됐겠지?'},\n",
       " {'timestamps': {'from': '00:00:46,000', 'to': '00:00:48,000'},\n",
       "  'offsets': {'from': 46000, 'to': 48000},\n",
       "  'text': ' 괜찮았으시다면 안 됐겠지?'},\n",
       " {'timestamps': {'from': '00:00:48,000', 'to': '00:00:50,000'},\n",
       "  'offsets': {'from': 48000, 'to': 50000},\n",
       "  'text': ' 괜찮았으시다면 안 됐겠지?'},\n",
       " {'timestamps': {'from': '00:00:50,000', 'to': '00:00:52,000'},\n",
       "  'offsets': {'from': 50000, 'to': 52000},\n",
       "  'text': ' 괜찮았으시다면 안 됐겠지?'},\n",
       " {'timestamps': {'from': '00:00:52,000', 'to': '00:00:54,000'},\n",
       "  'offsets': {'from': 52000, 'to': 54000},\n",
       "  'text': ' 괜찮았으시다면 안 됐겠지?'},\n",
       " {'timestamps': {'from': '00:00:54,000', 'to': '00:00:56,000'},\n",
       "  'offsets': {'from': 54000, 'to': 56000},\n",
       "  'text': ' 괜찮았으시다면 안 됐겠지?'},\n",
       " {'timestamps': {'from': '00:00:56,000', 'to': '00:00:58,000'},\n",
       "  'offsets': {'from': 56000, 'to': 58000},\n",
       "  'text': ' 괜찮았으시다면 안 됐겠지?'},\n",
       " {'timestamps': {'from': '00:00:58,000', 'to': '00:01:00,000'},\n",
       "  'offsets': {'from': 58000, 'to': 60000},\n",
       "  'text': ' 괜찮았으시다면 안 됐겠지?'},\n",
       " {'timestamps': {'from': '00:01:00,000', 'to': '00:01:02,000'},\n",
       "  'offsets': {'from': 60000, 'to': 62000},\n",
       "  'text': ' 괜찮았으시다면 안 됐겠지?'},\n",
       " {'timestamps': {'from': '00:01:02,000', 'to': '00:01:04,000'},\n",
       "  'offsets': {'from': 62000, 'to': 64000},\n",
       "  'text': ' 괜찮았으시다면 안 됐겠지?'}]"
      ]
     },
     "execution_count": 56,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# STEP 1. Whisper JSON 불러오기\n",
    "with open(f\"./Text/{LECTURE_FILE_NAME}/{LECTURE_FILE_NAME}_merged_ggml-small.json\", \"r\", encoding=\"utf-8\") as f:\n",
    "    data = json.load(f)\n",
    "\n",
    "segments = data[\"transcription\"]\n",
    "segments[:100]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "id": "ef913b94",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "' (인터뷰)\\n 혹시 그 여러분 젠킨스라는 투를 해본 적이 있다\\n 젠킨스 써본 적 있다\\n (인터뷰)\\n 혹시 써보실 때 어느 정도 쓰시니 까지 써보셨어요\\n 그냥 사용자 입장에서 파이프'"
      ]
     },
     "execution_count": 57,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# STEP 2. 전체 텍스트 이어붙이기\n",
    "full_text = \"\\n\".join([seg[\"text\"] for seg in segments])\n",
    "full_text[:100]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "id": "8648ab9b",
   "metadata": {},
   "outputs": [],
   "source": [
    "text_splitter = RecursiveCharacterTextSplitter(\n",
    "    chunk_size=1000,\n",
    "    chunk_overlap=50\n",
    ")\n",
    "\n",
    "chunks = text_splitter.split_text(full_text)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "id": "79a75c69",
   "metadata": {},
   "outputs": [],
   "source": [
    "os.environ[\"OPENAI_API_KEY\"] = getpass(\"🔐 OpenAI API Key를 입력하세요: \")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "id": "82d5b98b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# STEP 4. Document 리스트로 변환\n",
    "docs = [Document(page_content=chunk, metadata={}) for chunk in chunks]\n",
    "\n",
    "# STEP 5. Embedding + Vectorstore 생성\n",
    "embedding = OpenAIEmbeddings()\n",
    "vectorstore = FAISS.from_documents(docs, embedding)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "id": "e2f692dc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# STEP 6. Retriever + QA Chain 구성\n",
    "retriever = vectorstore.as_retriever(search_type=\"mmr\")\n",
    "retriever.search_kwargs.update({\n",
    "    \"k\": 10,\n",
    "    \"fetch_k\": 100,\n",
    "    \"maximal_marginal_relevance\": True\n",
    "})\n",
    "\n",
    "# custom_prompt = PromptTemplate(\n",
    "#     input_variables=[\"context\", \"question\"],\n",
    "#     template=\"\"\"\n",
    "# 다음 문서를 참고하여 질문에 답하세요.\n",
    "\n",
    "# 문서에 명확한 내용이 없으면 반드시 아래와 같이 대답하세요:\n",
    "# '잘 모르겠습니다. 문서에 관련 정보가 없습니다.'\n",
    "\n",
    "# 문서:\n",
    "# {context}\n",
    "\n",
    "# 질문:\n",
    "# {question}\n",
    "# \"\"\"\n",
    "# )\n",
    "\n",
    "from langchain.callbacks import StdOutCallbackHandler\n",
    "\n",
    "\n",
    "qa = RetrievalQA.from_chain_type(\n",
    "    llm=ChatOpenAI(temperature=0),\n",
    "    chain_type=\"stuff\",\n",
    "    retriever=retriever,\n",
    "    return_source_documents=True,\n",
    "    callbacks=[StdOutCallbackHandler()]\n",
    "    # chain_type_kwargs={\"prompt\": custom_prompt}\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "id": "af44757c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "\u001b[1m> Entering new RetrievalQA chain...\u001b[0m\n",
      "\n",
      "\u001b[1m> Finished chain.\u001b[0m\n",
      "💬 Question: 젠킨스는 오픈소스 CI/CD 자동화 서버이다. 이거 맞아?\n",
      "{'query': '젠킨스는 오픈소스 CI/CD 자동화 서버이다. 이거 맞아?', 'result': '', 'source_documents': [Document(id='b7558aae-d869-48a8-92f6-ff99ea27d4d4', metadata={}, page_content='그런 경우에는 그냥 커버네티스를 쓴게 났습니다. 뭐 근데 이런 여러 가지 선택지가 장난점은 있지만 뭐가 좋고 나쁘다 라기보다는\\n 본인의 서비스에 맞춰서 이제 사용하시면 될 것 같아요. 그리고 이제 저희가 기본적으로 지금까지 오늘 오전에 좀 vm 배포도구로 간단하게 젠킷을 썼죠.\\n 젠킷으로 이제 대충 뭐가 교정되어 있는지까지 봤는데 이렇게 배포 자동화도구로 vm에다 배포하는 경우에 앤서블이라는 걸 많이 쓰고 있어요. 앤서블은\\n 플레이북이라는 단위로 약간 이런거죠. 앤서블 하나\\n 자다가 연동되어 있는 모든 VM에다가 한 번의 명령어를 때려서 안에 있는 OS 단 부터 해서\\n 그 안에 있는 서버의 구성이라든가 그런 거를 한 번에 취할 수 있는 중앙화된 SSH 기반의 트리거든요.\\n 이런 것도 있고, 오늘 다뤘던 진키스는 굉장히 유해가 잘 되어 있기 때문에\\n 타입 프라인을 잘 짜면 앤선분처럼 잘 유용하게 쓸 수 있고요.\\n 그 다음에 KT 앤섭프라이즈 환경에서는 저희가 기톱 액션으로 배포하지 않지만\\n 그냥 이제 퍼블릿 환경에서는 기톱 액션으로도 배포할 수 있습니다. 이런 게 있고, 이해하시면 되고\\n 컨테이너 같은 경우는 진키스로도 충분히 배폴할 수 있어요.\\n 그래서 여러분들이 입사하시기 전에 제가 예전에 KT 내부의 개발자 플랫폼이라고 해서\\n 데브스페이스라는 거 운영을 했는데 그때는 이제 여러분들의 애저에 배포하시겠지만\\n KT 클라우드라는 그룹사가 있죠. KT 클라우드는 데이터센터로 운영하고 있고\\n 거기에 산회 KT 전사를 위한 IPC라고 해서 인터넷 커블릿 클라우드라고 해서\\n 산회를 대상으로 하는 서비스가 있어요. 거기도 온크래미나 VM이 있고 커버렉티스가 있죠.\\n 거기에다가 배포하는 툴로 댕킹세로 썼습니다. 그때 이게 사실 굉장히 조확한 방법인데\\n 이때는 아르보 시즈를 도입하기 전이었고, 댕킹스로도 어쨌든 배포는 가능하다.\\n 근데 사실 효율적이지 않습니다. 그렇게 이해하시면 될 것 같고\\n 키터맥션으로도 배포 가능하고 오늘 아르보 시디 할 거죠.'), Document(id='349a512f-9e4f-40ee-b669-7d5a6f9a855c', metadata={}, page_content='근데 7번 학생도 해봤는데 7번 학생도 해봤는데 7번 학생도 해봤다고요?\\n 아 아 아 아 아 아 아 아 아 아 아 아 아 아 아 아 아 아 아 아 아 아 아 아 아 아 아 아 아 아 아 아 아 아 아 아 아 아 아 아 아 아아 아 아 아 아 아 아 아 아 아 아 아 아 아 아 아 아 아 아 아 아 아 아 아 아 아 아 아 아 아 아 아 아 아 아 아 아 아 아 아 아 아 아 아 아 아 아 아 아 아 아 아 아 아 아 아 아 아 아 아 아 아 아 아 아 아 아 아 아 아 아 아 아 아 아 아 아 아 아 아 아 아 아 아 아 아 아 아 아 아 아 아 아 아 아 아 아 아 아 아 아 아 아 아 아 아 아 아 아 아 아 아 아 아 아 아 아 아 아 아 아 아 아 아 아 아 아 아 아 아 아 아 아 아 아 아 아 아 아 아 아 아 아 아 아 아 아 아 아 아 아 아 아 아 아 아 아 아 아 아 아 아 아 아 아 아 아 아 아 아 아 아 아 아 아 아 아\\n (끝나는 소리)\\n (끝나는 소리)\\n 근데 시내는...\\n (끝나는 소리)\\n (끝나는 소리)\\n (끝나는 소리)\\n (끝나는 소리)\\n (끝나는 소리)\\n (끝나는 소리)\\n (끝나는 소리)\\n (끝나는 소리)\\n (끝나는 소리)\\n (끝나는 소리)\\n (끝나는 소리)\\n (끝나는 소리)\\n (끝나는 소리)\\n (끝나는 소리)\\n (끝나는 소리)\\n (끝나는 소리)\\n (끝나는 소리)\\n (끝나는 소리)\\n (끝나는 소리)\\n (끝나는 소리)\\n (끝나는 소리)\\n 수고하셨습니다.\\n 수고하셨습니다.\\n 수고하셨습니다.\\n 수고하셨습니다.\\n 수고하셨습니다.\\n 수고하셨습니다.\\n 수고하셨습니다.\\n 수고하셨습니다.\\n 수고하셨습니다.\\n 수고하셨습니다.\\n 수고하셨습니다.\\n 수고하셨습니다.\\n 수고합니다.\\n 수고하셨습니다.\\n 시작할게요.\\n 제가 사내PC를 가져오러 간 이유는\\n Argo CD를\\n Jenkins도 그렇고, Argo CD도 그렇고\\n 저희가 사내환경에서 배포를 할 때는\\n 사내PC로\\n 접속을 방압역을 뚫어서 하고 있어요.\\n 그래서'), Document(id='0173f626-a73d-4940-a360-90a126a27e3d', metadata={}, page_content='(인터뷰)\\n 혹시 그 여러분 젠킨스라는 투를 해본 적이 있다\\n 젠킨스 써본 적 있다\\n (인터뷰)\\n 혹시 써보실 때 어느 정도 쓰시니 까지 써보셨어요\\n 그냥 사용자 입장에서 파이프라인이 짜는 정도?\\n 짜고 젠킨스로 그러면 뭘 하셨어요 혹시?\\n (인터뷰)\\n 씨아이만 했어요 젠킨스로\\n 저희는 어제 컨테이너 빌드 를 했기 때문에\\n 오늘은 rgo cd로 별풀할 건데\\n kt 안에서 이제 vmv4 대상으로는\\n cd2로 젠킨스를 쓰고 있거든요\\n 그래서 그래서 이제 저희 개발맥을 써서\\n 간단하게 이제 토커업을 해서 이미지 가져와서 안에\\n 어떻게 부성되어 있고 어떻게 쓰는 지 정도만 확인을 하고\\n 그 다음에 저희가 교육용으로 구성해서 rgo cd로 넘어갈게요\\n 그러면 다들 토커업을 쓸 수 있는 개발맥을 가져와 보시고\\n 그걸 토커업에 이렇게 젠킨스 슬래쉬 젠킨스로 검색을 하면\\n 젠킨스라는 오픈소스를 만드는 곳에서\\n 배포하는 이미지가 있거든요 컨테이너 이미지\\n 이거를 각자 터미널 로컬로 토커풀 받으셔가지고\\n 레이티스트로 받으셔도 돼요\\n 카피해서 명령어 넣은 다음에\\n 이제 로컬PC에 다운로드 받아보겠습니다\\n 수업을 직접 들어가서 구성도 되고 명령어를 드릴게요\\n 공사는 4월 27일까지 예정되어 있으며 공사의 기간 중 다소의 소음과 진동이 발생될 수 있고\\n 비상 엘리베이서 오랜 제한의 불편사항이 출해될 수 있으니 양해해 주시기 바랍니다.\\n 다시 한 번 알려드립니다.\\n 현재 지경동 살칭에서 교육장 공축 공사가 진행되고 있습니다.\\n 공사는 4월 27일까지 예정되어 있으며 공사의 기간 중 다소의 소음과 진동이 발생될 수 있고\\n 비상 엘리베이서 오랜 제한의 불편사항이 출해물 수 있으니 양해해 주시기 바랍니다.\\n 감사합니다.\\n 다음 되셨나요?\\n 도커플 되셨으면 저는 이제 사내 환경에서 보여드릴 거고 여러분은 편하게 개발맥에서 하시면 되는데\\n 도커플 컨테이너를 실행하는 명령원은 잘 계시겠죠?\\n 도커플은 대본으로 돌릴 거고\\n 그다음에 컨테이너 이름 저하고\\n 진킨스 뭐 더비스 이런 거 하고'), Document(id='2e86040b-d00f-472c-88e6-abef537b8373', metadata={}, page_content='FQDN 정보인 이 URL로 방압역 443을 요청하시면\\n 우리 키터프에서 CI 할 때\\n 러너에서 서비스 구도마다의 블록스토리즈에 파일 업로드할 수 있는 경로를 뚫어주게 되는 거예요.\\n 우리는 이런 블록스토리즈 쓰는 CI를 안 했죠.\\n 그러면 우리 acr에다가 올렸는데 acr도 이 프라이벳 앤드 포인트가 있습니다.\\n 그래서 컨테이너 100포의 경우도 acr의 프라이벳 앤드 포인트에 FQDN에다가 수시를 하시면 되는데\\n 그거는 어디에 있느냐?\\n 또 동일하게 저희 구독 기준으로 말씀 드릴게요.\\n 이게 이제 여러분들이 팀에 가서 쓰셔야 할 acr이라고 한다면\\n 여기 네트워크 들어와서 프라이빗 액세스에 이 프라이빗 앤드 포인트를 기점으로 들어가는 거예요.\\n 근데 acr 같은 경우에는 FQDN 정보가 두 개거든요.\\n 하나는 이 리켓스를 받을 곳\\n 하나는 여기에 대한 실질적을 저장을 하고 있는 저장소에 대한 정보예요.\\n 그래서 우리가 acr에다가 푸시를 할 때는 러너에서 FQDN 두 개의 정보에다가\\n 이제 443 방압역 신청을 하셔야 된다.\\n 방압역 신청은 어디서 하시는지 아시나요?\\n 보시는 저희는 방압역 득컴이라는 거를 쓰고 있어요.\\n 마이오월드 케이펫 좀 먹어.\\n 여기 들어가서 그냥 KT4번 비밀밀밀밀밀밀밀밀\\n 1번을 넣으시고 정책 이런 걸 한번 보여드릴게요 그러면 방압 신청을 어떻게 하는지\\n 정책 신규 신청 들어가셔서 그냥 간단하게 참고만 이게 이제 방압 신청은 양식이거든요\\n 여기서 요청 구분에 우리는 이렇게 운영을 하고 있고 러너는 이제 운영의 개념이고요\\n 여러분들이 만약에 그 개발 구독을 받았다면 운영에서 개발 서버를 넣으시면 되고요\\n 여기는 내 숙소, IDKIT, 내부공부, 신까지, 담당자, 본인이 신청자 이름 넣으시면 됩니다\\n 단위 서비스 코드는 KT에서 IT 프로젝트를 관리하기 위한 고요한 값이거든요\\n 고요한 ID가 얘를 이제 직접 넣으시면 되고 대부분 앞에 알파펫 2개랑 숫자 한 4개에서 6개 정도로 이루어져 있어요'), Document(id='bdaf446a-0a97-4e44-82fc-b3a2d9ac0ec5', metadata={}, page_content='그다음에 컨테이너 이름 저하고\\n 진킨스 뭐 더비스 이런 거 하고\\n 포트를 열어줘야 돼요.\\n 8080에 서비스 포트가 8080이라서 그냥 편하게 열어주시고\\n 그다음에 컨테이너 이미지 있겠죠.\\n 오레이 키스 이런 거 이길죠.\\n 뭐 에러가 날 수도 있긴 한데 어쨌든 이런 식으로 명령어를 쓰셔서\\n 지금 혹시 다른 거 8080 포트로 돌리고 계시면\\n 옆에 있는 포트 포딩 해주는 포트를 그냥 커스테면서 바꿔주시면 되고\\n 그렇게 해서 한 번 프로세스 띄워볼게요.\\n 도커플 어느 명령어 실행할게요.\\n 도커플이 오래 걸려요?\\n 아 그래요?\\n 혹시 컨테이너 프로세스 띄우셨나요?\\n 만약에 제 명령어가 잘 안 먹으면 성공하신 분이 그냥 슬레게 공교를 주셔도 돼요.\\n 저는 약간 가이드 시겨드로만 드리는 거라서\\n 혹시 이진홍 님 승인됐다고 하는데\\n 멤버 추가 해볼게요.\\n 기품지력시팀 맞으세요?\\n 네 드디어 얻어드렸습니다.\\n 감사합니다.\\n 그래도 진호님은 오늘 배포할 이미지가 아직 없을 거잖아요.\\n acr에다가 그거 이제 옆에 동지분 거 같이 오늘 배포 실습만 하는 거라서\\n 호주 배꼽하고 ci는 다 흐름만 이해해줄 것 같아요.\\n 처음에 이제 그래서 컨테이너 프로세스 띄우고 로커러스트 8080으로 웹접습 하면\\n 이렇게 처음에 어드민 세덕을 하는 화면이 나와야 되거든요.\\n 저희 나오셨네요.\\n 저희는 저 화면이 안되고.\\n 언급하시려고 하고 있었어요.\\n 아 그래? 잠시만요.\\n 아니 이게 이미지가 약간 다른데.\\n 그러면 그 벌랍진 킨스로 이제 어드민 패스워드가 필요한데\\n 이게 저랑 이미지가 약간 달라질 거 같고 그 경우에는 그냥 터미널 하나 더 여셔서\\n 선생님께 공유해 드릴게요.\\n 터미널 하나 더 열어서\\n 김소원님께서 공유해 주셨네요.\\n 터미널 하나 더 열어서.\\n 아니 아까 데몬 뒤로 띄었으니까 그냥 바로 그 터미널에서 이 아래 명료 복사 무척 하시면 되거든요.\\n 그럼 이게 초기 세덕 비밀번호가 이제 프로세스에 남아 있는 거예요.'), Document(id='16ac4bd3-a37f-497d-bfd1-6610f6d1ddd3', metadata={}, page_content='OS에 있는 ETC에서 따라서 여기에다가\\n 아래에 이렇게 넣으시면 돼요\\n 그냥 IP 덕분에\\n 이 위에 꼭 무시해 주시고 제가 쓰는 거로\\n 아래 거 넣고 그다음에 이 URL에 넣고\\n 그냥 복사 붙여넣기 하셔서\\n 버전 사진 다음에 웹 브라우저에 다시 들어가셔서\\n 검색 검색이 아니라\\n 엔소드 누르면 이제 들어가지는 것까지 확인하셔야 돼요\\n 그래서 배터리 충전 좀 해보시고 rgocd 적극하시면 로그인까지 해보시고 로그인 입니다.\\n 이거 하고 11시 반에 밥 먹으러 가시면 돼요. 아시겠죠.\\n. [두 번째 질문]\\n [두 번째 질문]\\n [두 번째 질문]\\n [두 번째 질문]\\n [두 번째 질문]\\n [두 번째 질문]\\n [두 번째 질문]\\n [두 번째 질문]\\n [두 번째 질문]\\n [두 번째 질문]\\n [두 번째 질문]\\n [두 번째 질문]\\n [두 번째 질문]\\n [두 번째 질문]\\n [두 번째 질문]\\n [두 번째 질문]\\n [두 번째 질문]\\n 여기 이제 우리 오버니제이션메인으로 들어오셔가지고\\n 배포 스토리를 새로 하나 만들거에요. 근데\\n 어 이제\\n 저번에 보니까 이제 소스코도 안에\\n 포어네티스 형상들이 있는 경우가 있었어요.\\n 이분은 없으신?\\n 혹시 배포 뭘로 하셨어요? 그때? 각자 배포 해보셨을 때?\\n 포어네티스 배포 할 때? 헬렌 배포 이런거 할 때?\\n 아 그래요? 그럼 거기에 배포 형상들이 있어요.\\n 각자 하나씩? 그러면 그거 활용하시면 될 것 같고\\n 동일하게 해야되는데 우리는\\n 기돕스로, HTTPS로 배포를 해볼게요. 그러면\\n 지금 어제 이렇게 올려주신 랩코스토리는\\n 소스코드 랩코스토리라고 하는거에요. 그러니까 어플리케이션 개발을 위한 소스코드들만\\n 저장을 해둘거고\\n 우리는 이제 기돕스 랩포를 하나 만들어야 되죠. 여기\\n ARGO CD 전용으로 기돕스 랩포를 하나 만들거에요. 그래서\\n 그 두개의 형상을 분리하는게 좋겠죠. 아무래도 개발용이랑 운영용이 달라져야 되니까\\n 그래서\\n 여기도 그냥 어제처럼 간단하게 레이밍 롤을 제가 드릴게요.\\n 만약에 이 분명 위에 계시니까 이거 쓸게요.'), Document(id='c012a7eb-a5c1-4e61-8e42-c553918c5509', metadata={}, page_content=\"근데 KT 안에서는 파이선도 배포하실 때 우선 패키징을 해야 되고 파이선을 패키징하는 툴로는\\n 'Poetry'라는 오픈 소스나 'Wheel' 이런 게 있어요. 이런 거를 써서 파이선을 점 자르나 점 와르처럼 만든다고 생각하시면 됩니다.\\n 그래서 이 실행 파일을 이런 파일로 생기거든요. 이런 압축 파일로 생기는 거예요.\\n '2.5'하는 점 자르처럼. 이런 최종 파일을 뵙는다거나 아니면 NPM, 모두 NPM 같은 경우는\\n 빌드 하면 디스트 폴더 생기죠. 프론텐드 하시는 분들 맞나요 제가? 이걸 압축 파일로 생기지 않나요? 점 집이었나 이런 압축 파일이 생길 거거든요.\\n 이런 식으로 이제 배포를 하게 되는데 V&B 배포를 경우에는 어쨌든 장황했지만 언어별로 패키징한 정적 파일이라고 해요.\\n 정적 파일들이 어제 CIA를 했다면 저희가 V&B 배포의 경우에는 블록 스토리지에다가 저장을 한다고 했잖아요.\\n 'Poetry'는 이제 ACI에다가 저장을 하고. 그럼 얘는 기본적으로 지금 블록 스토리지에 저장이 되어 있을 거란 말이죠.\\n V&B를 가져다가 실행해라는 명령이 많습니다. 따라서 이 명령이라는 거는 어떤 블록 스토리지에서 어떤 파일을 다운로드 받아서 실행해.\\n 나는 이 세 가지 명령어 셋을 jenkins가 target vm에다 내리는 방식이에요 ssh로\\n 그래서 이 크게 세 가지만 기억하시면 될 것 같아요\\n jenkins가 vm에다가 접근하기 위해서는 ip랑 id password가 필요하고\\n 그러면 이렇게 jenkins가 접근해서 내리 명령어 셋은\\n target vm이 어느 블록 스토리즈에서 어떤 파일을 다운로드 받아서 실행해 나는 명령어구나\\n 이렇게 부분하시면 됩니다. 말이 어렵지만\\n 소스코드 그 파이프라인은 굉장히 간단하니까 바로 이해하실 수 있을 거예요\\n 그래서 이렇게 이해를 한 다음에 쓰레기를 공유해 드렸습니다\\n 그 방금 이제 hello world 찍은 잡 있죠 이 안에 부성으로 들어갈게요\\n 방금 그 파이프라인은 다시 짜야 되기 때문에\"), Document(id='e6563747-ef4c-4e1d-b4e1-b49b20c6d82e', metadata={}, page_content='이것도 이제 사실 블루고리인과 롤링 업데이트 약간 좀 롤링 업데이트에 가까운 방식이지만\\n 저는 오히려 이게 조금 더 조심스럽게 배포하기에는 좋은 방식이라 생각하고 다만 단점으로는 만약에 비율 설정을 잘못했을 때\\n 새로운 버전의 문제를 충분히 검증하지 못해서 전체 서비스에 문제가 될 수 있다. 그러니까 보수적인 비율들이 충분히 검증이 되었을 때 전체의 배포하는 방식이 좋겠다.\\n 이런 식으로 서비스 특히 AKS나 커버넘 키스에 올라가는 서비스\\n 기토브의 대상으로 100포를 할 때 각각의 100포 전략이 이제 장점이 있으기 때문에 열심히 고민을 해보셔서 고르신 다음에\\n 그러시면 좋을 것 같습니다. 이런 이론적인 문제가 있어요. 이론적인 것들이 있어요.\\n 그 다음에 이제 기독스인데 여러분들이 아까 얘기한 ROCD를 기토브에 연결해서 헬런 100포 하는 것 자체가 기독스에요.\\n 그러니까 정확히는 개발자가 저희가 어제 기톱 엔터파이지를 각각 레포스토리 만들어서 소스코드 업로드 했죠. 그걸로 저희 ACI 했죠.\\n 그 1년의 과정을 지금 여기까지 올라와 있는 거예요. 도커브보다는 ACI로 생각하시면 되고\\n 그래서 이 개발자가 소스코드 업로드해서 우리 기톱 액션으로 도커와 만들어서 ACI에다가 업로드를 했죠. 이 과정까지 끝났고\\n 오늘은 이제 ROCD로 AKS에다가 배포를 할 건데 이 기독스 레포스토리 그러니까 100포를 할 형상을 소스코드로 관리하겠다는 거거든요.\\n 아까 말씀 주신 게 헬륨으로 하셨다고 하셨는데 헬륨하셔도 되고 커스테마이지를 쓰셔도 되고 그냥 쌩자 디플루이먼트랑 서비스랑 그런 야볼 파일도 던지셔도 됩니다.\\n 그건 이따가 자유롭게 하실 거고요. 따라서 이 ROCD나 ROCD가 커버네티스 또는 AKS에다가 배포할 형상을 소스코드로 관리하겠다.\\n 그게 단일 원천의 소스 이런 용어가 있는데 어쨌든 이 킥에서 관리하는 포인트로 이 스테이터스를 맞춰주는 거죠.'), Document(id='f73e597d-4a27-422d-bdb3-43d548567e75', metadata={}, page_content='잡은 뭐 실질적으로 하는 테스트로 할게요.\\n 이 폴더 안에 있는 잡에는\\n 파이프라인으로 할 거예요 파이프라인을 써야 우리가 명령어를 써서 배플할 수 있기 때문에 파이프라인을 쓸 거고\\n 이름은 편하게 하시고 파이프라인만 골라주시면 됩니다 파이프라인\\n 자 이제 쭉 내려보시면 직접 스크립트를 짤 수 있는 파이프라인 라인이 있는데\\n 어 이때 직접 이 파이프라인 스크립트를 여기에 그냥 이렇게 쓰셔도 되고\\n scm이라고 해서 기시랑 연동을 할 수 있어요 젠킨 스파일이라는 파일을 기수로 관리할 수 있기 때문에\\n 이거를 어 만약에 나는 기텁이랑 연동하고 싶다 그러면 여기에 이 젠킨 스파일이\\n 저장된 레포스토리 url에 넣고 인증 넣으신 다음에 브랭치 설정 하시면 이 젠킨 스파일이라는 패스까지 정해 주시면\\n 알아서 이제 이 빌드 잡이 될 때 젠킨스가 기질러서 젠킨 스파일 정보를 불러온 다음에 실행하게 될 거예요\\n 우리는 그렇게까지 하지 않고 여기서 할 건데 여기 아주 쉬운 스니펫이 하나 있습니다\\n 여기 패러 월드 하면 짠 바로 그룹이 생기거든요 이게 기본적으로 언어가 그룹이 베이스이고\\n 야물로도 할 수 있다고 제가 어제는 잠깐 말씀드렸는데 야물들은 또 플러그인이 필요해요 그래서 야물을 쓰시고 싶으시면\\n 이제 플러그인을 설치를 하시면 됩니다 그래서 단단하게 어플라이 하고 저장하고 이제 다시 폴더 1 테스트 잡으러 나가보시면\\n 이제 아무것도 안 떠 있죠 지금 이 상태에서 왼쪽에 있는 지금 빌드 버튼을 눌러볼게요\\n 왼쪽 아래 빌드 히스토리라는 탭 아래 이렇게 초록색 체크 표시 뜨면서 언제 2월에 빌드 했고 뭐 이런 식으로 되어 있거든요\\n 여러분 아마 그 타임존 설정 안 돼 있으면 아마 한 9시간 정도 차이 날 건데 그건 그냥 무시해 주시고\\n 그래서 이 빌드 히스토리에 들어가 보시면 어떤 잡을 얘가 수행했는지 확인할 수 있어요 그래서 누가 이 잡을 트리거 했고 이 잡을 어 시작부터 끝까지\\n 어 수행하는데 얼마나 걸렸고 이 자배될..'), Document(id='6d39c985-6812-49cc-a2de-0be56f0d03c4', metadata={}, page_content='그럼 이게 초기 세덕 비밀번호가 이제 프로세스에 남아 있는 거예요.\\n 그래서 이걸로 확인하셔서 넣으시면 저랑 같은 화면이 나올 겁니다.\\n 플로그인 깔면 됩니다.\\n 그냥 추천해 주는 걸로 다 갈아주시면 되요.\\n 네.\\n 아니 안 보여요.\\n (댓글 읽는 중)\\n 괜찮았으시다면 안 됐겠지?\\n 괜찮았으시다면 안 됐겠지?\\n 괜찮았으시다면 안 됐겠지?\\n 괜찮았으시다면 안 됐겠지?\\n 괜찮았으시다면 안 됐겠지?\\n 괜찮았으시다면 안 됐겠지?\\n 괜찮았으시다면 안 됐겠지?\\n 괜찮았으시다면 안 됐겠지?\\n 괜찮았으시다면 안 됐겠지?\\n 괜찮았으시다면 안 됐겠지?\\n 괜찮았으시다면 안 됐겠지?\\n 괜찮았으시다면 안 됐겠지?\\n 괜찮았으시다면 안 됐겠지?\\n 괜찮았으시다면 안 됐겠지?\\n 괜찮았으시다면 안 됐겠지?\\n 괜찮았으시다면 안 됐겠지?\\n 괜찮았으시다면 안 됐겠지?\\n 괜찮았으시다면 안 됐겠지?\\n 괜찮았으시다면 안 됐겠지?\\n 괜찮았으시다면 안 됐겠지?\\n 괜찮았으시다면 안 됐겠지?\\n 괜찮았으시다면 안 됐겠지?\\n 괜찮았으시다면 안 됐겠지?\\n 괜찮았으시다면 안 됐겠지?\\n 괜찮았으시다면 안 됐겠지?\\n 괜찮았으시다면 안 됐겠지?\\n 괜찮았으시다면 안 됐겠지?\\n 괜찮았으시다면 안 됐겠지?\\n 괜찮았으시다면 안 됐겠지?\\n 괜찮았으시다면 안 됐겠지?\\n 괜찮았으시다면 안 됐겠지?\\n 괜찮았으시다면 안 됐겠지?\\n 괜찮았으시다면 안 됐겠지?\\n 괜찮았으시다면 안 됐겠지?\\n 괜찮았으시다면 안 됐겠지?\\n 괜찮았으시다면 안 됐겠지?\\n 괜찮았으시다면 안 됐겠지?\\n 괜찮았으시다면 안 됐겠지?\\n 괜찮았으시다면 안 됐겠지?\\n 괜찮았으시다면 안 됐겠지?\\n 괜찮았으시다면 안 됐겠지?\\n 괜찮았으시다면 안 됐겠지?\\n 괜찮았으시다면 안 됐겠지?\\n 괜찮았으시다면 안 됐겠지?\\n - 네. - 네.\\n - 여기 진짜 진짜 잘 찍는 거 같아요. - 네.\\n - 진짜 이쁘다. - 진짜 이쁘다.\\n - 네. - 되셨어요?\\n - 네, 네. 아, 됐어요. - 여기다가 이거 넣는 거죠.\\n - 이거 한 번 더? - 네.\\n - 오. - 됐어.')]}\n",
      "🧠 Answer: \n",
      "📄 Source Metadata: [Document(id='b7558aae-d869-48a8-92f6-ff99ea27d4d4', metadata={}, page_content='그런 경우에는 그냥 커버네티스를 쓴게 났습니다. 뭐 근데 이런 여러 가지 선택지가 장난점은 있지만 뭐가 좋고 나쁘다 라기보다는\\n 본인의 서비스에 맞춰서 이제 사용하시면 될 것 같아요. 그리고 이제 저희가 기본적으로 지금까지 오늘 오전에 좀 vm 배포도구로 간단하게 젠킷을 썼죠.\\n 젠킷으로 이제 대충 뭐가 교정되어 있는지까지 봤는데 이렇게 배포 자동화도구로 vm에다 배포하는 경우에 앤서블이라는 걸 많이 쓰고 있어요. 앤서블은\\n 플레이북이라는 단위로 약간 이런거죠. 앤서블 하나\\n 자다가 연동되어 있는 모든 VM에다가 한 번의 명령어를 때려서 안에 있는 OS 단 부터 해서\\n 그 안에 있는 서버의 구성이라든가 그런 거를 한 번에 취할 수 있는 중앙화된 SSH 기반의 트리거든요.\\n 이런 것도 있고, 오늘 다뤘던 진키스는 굉장히 유해가 잘 되어 있기 때문에\\n 타입 프라인을 잘 짜면 앤선분처럼 잘 유용하게 쓸 수 있고요.\\n 그 다음에 KT 앤섭프라이즈 환경에서는 저희가 기톱 액션으로 배포하지 않지만\\n 그냥 이제 퍼블릿 환경에서는 기톱 액션으로도 배포할 수 있습니다. 이런 게 있고, 이해하시면 되고\\n 컨테이너 같은 경우는 진키스로도 충분히 배폴할 수 있어요.\\n 그래서 여러분들이 입사하시기 전에 제가 예전에 KT 내부의 개발자 플랫폼이라고 해서\\n 데브스페이스라는 거 운영을 했는데 그때는 이제 여러분들의 애저에 배포하시겠지만\\n KT 클라우드라는 그룹사가 있죠. KT 클라우드는 데이터센터로 운영하고 있고\\n 거기에 산회 KT 전사를 위한 IPC라고 해서 인터넷 커블릿 클라우드라고 해서\\n 산회를 대상으로 하는 서비스가 있어요. 거기도 온크래미나 VM이 있고 커버렉티스가 있죠.\\n 거기에다가 배포하는 툴로 댕킹세로 썼습니다. 그때 이게 사실 굉장히 조확한 방법인데\\n 이때는 아르보 시즈를 도입하기 전이었고, 댕킹스로도 어쨌든 배포는 가능하다.\\n 근데 사실 효율적이지 않습니다. 그렇게 이해하시면 될 것 같고\\n 키터맥션으로도 배포 가능하고 오늘 아르보 시디 할 거죠.'), Document(id='349a512f-9e4f-40ee-b669-7d5a6f9a855c', metadata={}, page_content='근데 7번 학생도 해봤는데 7번 학생도 해봤는데 7번 학생도 해봤다고요?\\n 아 아 아 아 아 아 아 아 아 아 아 아 아 아 아 아 아 아 아 아 아 아 아 아 아 아 아 아 아 아 아 아 아 아 아 아 아 아 아 아 아 아아 아 아 아 아 아 아 아 아 아 아 아 아 아 아 아 아 아 아 아 아 아 아 아 아 아 아 아 아 아 아 아 아 아 아 아 아 아 아 아 아 아 아 아 아 아 아 아 아 아 아 아 아 아 아 아 아 아 아 아 아 아 아 아 아 아 아 아 아 아 아 아 아 아 아 아 아 아 아 아 아 아 아 아 아 아 아 아 아 아 아 아 아 아 아 아 아 아 아 아 아 아 아 아 아 아 아 아 아 아 아 아 아 아 아 아 아 아 아 아 아 아 아 아 아 아 아 아 아 아 아 아 아 아 아 아 아 아 아 아 아 아 아 아 아 아 아 아 아 아 아 아 아 아 아 아 아 아 아 아 아 아 아 아 아 아 아 아 아 아 아 아 아 아 아 아 아\\n (끝나는 소리)\\n (끝나는 소리)\\n 근데 시내는...\\n (끝나는 소리)\\n (끝나는 소리)\\n (끝나는 소리)\\n (끝나는 소리)\\n (끝나는 소리)\\n (끝나는 소리)\\n (끝나는 소리)\\n (끝나는 소리)\\n (끝나는 소리)\\n (끝나는 소리)\\n (끝나는 소리)\\n (끝나는 소리)\\n (끝나는 소리)\\n (끝나는 소리)\\n (끝나는 소리)\\n (끝나는 소리)\\n (끝나는 소리)\\n (끝나는 소리)\\n (끝나는 소리)\\n (끝나는 소리)\\n (끝나는 소리)\\n 수고하셨습니다.\\n 수고하셨습니다.\\n 수고하셨습니다.\\n 수고하셨습니다.\\n 수고하셨습니다.\\n 수고하셨습니다.\\n 수고하셨습니다.\\n 수고하셨습니다.\\n 수고하셨습니다.\\n 수고하셨습니다.\\n 수고하셨습니다.\\n 수고하셨습니다.\\n 수고합니다.\\n 수고하셨습니다.\\n 시작할게요.\\n 제가 사내PC를 가져오러 간 이유는\\n Argo CD를\\n Jenkins도 그렇고, Argo CD도 그렇고\\n 저희가 사내환경에서 배포를 할 때는\\n 사내PC로\\n 접속을 방압역을 뚫어서 하고 있어요.\\n 그래서'), Document(id='0173f626-a73d-4940-a360-90a126a27e3d', metadata={}, page_content='(인터뷰)\\n 혹시 그 여러분 젠킨스라는 투를 해본 적이 있다\\n 젠킨스 써본 적 있다\\n (인터뷰)\\n 혹시 써보실 때 어느 정도 쓰시니 까지 써보셨어요\\n 그냥 사용자 입장에서 파이프라인이 짜는 정도?\\n 짜고 젠킨스로 그러면 뭘 하셨어요 혹시?\\n (인터뷰)\\n 씨아이만 했어요 젠킨스로\\n 저희는 어제 컨테이너 빌드 를 했기 때문에\\n 오늘은 rgo cd로 별풀할 건데\\n kt 안에서 이제 vmv4 대상으로는\\n cd2로 젠킨스를 쓰고 있거든요\\n 그래서 그래서 이제 저희 개발맥을 써서\\n 간단하게 이제 토커업을 해서 이미지 가져와서 안에\\n 어떻게 부성되어 있고 어떻게 쓰는 지 정도만 확인을 하고\\n 그 다음에 저희가 교육용으로 구성해서 rgo cd로 넘어갈게요\\n 그러면 다들 토커업을 쓸 수 있는 개발맥을 가져와 보시고\\n 그걸 토커업에 이렇게 젠킨스 슬래쉬 젠킨스로 검색을 하면\\n 젠킨스라는 오픈소스를 만드는 곳에서\\n 배포하는 이미지가 있거든요 컨테이너 이미지\\n 이거를 각자 터미널 로컬로 토커풀 받으셔가지고\\n 레이티스트로 받으셔도 돼요\\n 카피해서 명령어 넣은 다음에\\n 이제 로컬PC에 다운로드 받아보겠습니다\\n 수업을 직접 들어가서 구성도 되고 명령어를 드릴게요\\n 공사는 4월 27일까지 예정되어 있으며 공사의 기간 중 다소의 소음과 진동이 발생될 수 있고\\n 비상 엘리베이서 오랜 제한의 불편사항이 출해될 수 있으니 양해해 주시기 바랍니다.\\n 다시 한 번 알려드립니다.\\n 현재 지경동 살칭에서 교육장 공축 공사가 진행되고 있습니다.\\n 공사는 4월 27일까지 예정되어 있으며 공사의 기간 중 다소의 소음과 진동이 발생될 수 있고\\n 비상 엘리베이서 오랜 제한의 불편사항이 출해물 수 있으니 양해해 주시기 바랍니다.\\n 감사합니다.\\n 다음 되셨나요?\\n 도커플 되셨으면 저는 이제 사내 환경에서 보여드릴 거고 여러분은 편하게 개발맥에서 하시면 되는데\\n 도커플 컨테이너를 실행하는 명령원은 잘 계시겠죠?\\n 도커플은 대본으로 돌릴 거고\\n 그다음에 컨테이너 이름 저하고\\n 진킨스 뭐 더비스 이런 거 하고'), Document(id='2e86040b-d00f-472c-88e6-abef537b8373', metadata={}, page_content='FQDN 정보인 이 URL로 방압역 443을 요청하시면\\n 우리 키터프에서 CI 할 때\\n 러너에서 서비스 구도마다의 블록스토리즈에 파일 업로드할 수 있는 경로를 뚫어주게 되는 거예요.\\n 우리는 이런 블록스토리즈 쓰는 CI를 안 했죠.\\n 그러면 우리 acr에다가 올렸는데 acr도 이 프라이벳 앤드 포인트가 있습니다.\\n 그래서 컨테이너 100포의 경우도 acr의 프라이벳 앤드 포인트에 FQDN에다가 수시를 하시면 되는데\\n 그거는 어디에 있느냐?\\n 또 동일하게 저희 구독 기준으로 말씀 드릴게요.\\n 이게 이제 여러분들이 팀에 가서 쓰셔야 할 acr이라고 한다면\\n 여기 네트워크 들어와서 프라이빗 액세스에 이 프라이빗 앤드 포인트를 기점으로 들어가는 거예요.\\n 근데 acr 같은 경우에는 FQDN 정보가 두 개거든요.\\n 하나는 이 리켓스를 받을 곳\\n 하나는 여기에 대한 실질적을 저장을 하고 있는 저장소에 대한 정보예요.\\n 그래서 우리가 acr에다가 푸시를 할 때는 러너에서 FQDN 두 개의 정보에다가\\n 이제 443 방압역 신청을 하셔야 된다.\\n 방압역 신청은 어디서 하시는지 아시나요?\\n 보시는 저희는 방압역 득컴이라는 거를 쓰고 있어요.\\n 마이오월드 케이펫 좀 먹어.\\n 여기 들어가서 그냥 KT4번 비밀밀밀밀밀밀밀밀\\n 1번을 넣으시고 정책 이런 걸 한번 보여드릴게요 그러면 방압 신청을 어떻게 하는지\\n 정책 신규 신청 들어가셔서 그냥 간단하게 참고만 이게 이제 방압 신청은 양식이거든요\\n 여기서 요청 구분에 우리는 이렇게 운영을 하고 있고 러너는 이제 운영의 개념이고요\\n 여러분들이 만약에 그 개발 구독을 받았다면 운영에서 개발 서버를 넣으시면 되고요\\n 여기는 내 숙소, IDKIT, 내부공부, 신까지, 담당자, 본인이 신청자 이름 넣으시면 됩니다\\n 단위 서비스 코드는 KT에서 IT 프로젝트를 관리하기 위한 고요한 값이거든요\\n 고요한 ID가 얘를 이제 직접 넣으시면 되고 대부분 앞에 알파펫 2개랑 숫자 한 4개에서 6개 정도로 이루어져 있어요'), Document(id='bdaf446a-0a97-4e44-82fc-b3a2d9ac0ec5', metadata={}, page_content='그다음에 컨테이너 이름 저하고\\n 진킨스 뭐 더비스 이런 거 하고\\n 포트를 열어줘야 돼요.\\n 8080에 서비스 포트가 8080이라서 그냥 편하게 열어주시고\\n 그다음에 컨테이너 이미지 있겠죠.\\n 오레이 키스 이런 거 이길죠.\\n 뭐 에러가 날 수도 있긴 한데 어쨌든 이런 식으로 명령어를 쓰셔서\\n 지금 혹시 다른 거 8080 포트로 돌리고 계시면\\n 옆에 있는 포트 포딩 해주는 포트를 그냥 커스테면서 바꿔주시면 되고\\n 그렇게 해서 한 번 프로세스 띄워볼게요.\\n 도커플 어느 명령어 실행할게요.\\n 도커플이 오래 걸려요?\\n 아 그래요?\\n 혹시 컨테이너 프로세스 띄우셨나요?\\n 만약에 제 명령어가 잘 안 먹으면 성공하신 분이 그냥 슬레게 공교를 주셔도 돼요.\\n 저는 약간 가이드 시겨드로만 드리는 거라서\\n 혹시 이진홍 님 승인됐다고 하는데\\n 멤버 추가 해볼게요.\\n 기품지력시팀 맞으세요?\\n 네 드디어 얻어드렸습니다.\\n 감사합니다.\\n 그래도 진호님은 오늘 배포할 이미지가 아직 없을 거잖아요.\\n acr에다가 그거 이제 옆에 동지분 거 같이 오늘 배포 실습만 하는 거라서\\n 호주 배꼽하고 ci는 다 흐름만 이해해줄 것 같아요.\\n 처음에 이제 그래서 컨테이너 프로세스 띄우고 로커러스트 8080으로 웹접습 하면\\n 이렇게 처음에 어드민 세덕을 하는 화면이 나와야 되거든요.\\n 저희 나오셨네요.\\n 저희는 저 화면이 안되고.\\n 언급하시려고 하고 있었어요.\\n 아 그래? 잠시만요.\\n 아니 이게 이미지가 약간 다른데.\\n 그러면 그 벌랍진 킨스로 이제 어드민 패스워드가 필요한데\\n 이게 저랑 이미지가 약간 달라질 거 같고 그 경우에는 그냥 터미널 하나 더 여셔서\\n 선생님께 공유해 드릴게요.\\n 터미널 하나 더 열어서\\n 김소원님께서 공유해 주셨네요.\\n 터미널 하나 더 열어서.\\n 아니 아까 데몬 뒤로 띄었으니까 그냥 바로 그 터미널에서 이 아래 명료 복사 무척 하시면 되거든요.\\n 그럼 이게 초기 세덕 비밀번호가 이제 프로세스에 남아 있는 거예요.'), Document(id='16ac4bd3-a37f-497d-bfd1-6610f6d1ddd3', metadata={}, page_content='OS에 있는 ETC에서 따라서 여기에다가\\n 아래에 이렇게 넣으시면 돼요\\n 그냥 IP 덕분에\\n 이 위에 꼭 무시해 주시고 제가 쓰는 거로\\n 아래 거 넣고 그다음에 이 URL에 넣고\\n 그냥 복사 붙여넣기 하셔서\\n 버전 사진 다음에 웹 브라우저에 다시 들어가셔서\\n 검색 검색이 아니라\\n 엔소드 누르면 이제 들어가지는 것까지 확인하셔야 돼요\\n 그래서 배터리 충전 좀 해보시고 rgocd 적극하시면 로그인까지 해보시고 로그인 입니다.\\n 이거 하고 11시 반에 밥 먹으러 가시면 돼요. 아시겠죠.\\n. [두 번째 질문]\\n [두 번째 질문]\\n [두 번째 질문]\\n [두 번째 질문]\\n [두 번째 질문]\\n [두 번째 질문]\\n [두 번째 질문]\\n [두 번째 질문]\\n [두 번째 질문]\\n [두 번째 질문]\\n [두 번째 질문]\\n [두 번째 질문]\\n [두 번째 질문]\\n [두 번째 질문]\\n [두 번째 질문]\\n [두 번째 질문]\\n [두 번째 질문]\\n 여기 이제 우리 오버니제이션메인으로 들어오셔가지고\\n 배포 스토리를 새로 하나 만들거에요. 근데\\n 어 이제\\n 저번에 보니까 이제 소스코도 안에\\n 포어네티스 형상들이 있는 경우가 있었어요.\\n 이분은 없으신?\\n 혹시 배포 뭘로 하셨어요? 그때? 각자 배포 해보셨을 때?\\n 포어네티스 배포 할 때? 헬렌 배포 이런거 할 때?\\n 아 그래요? 그럼 거기에 배포 형상들이 있어요.\\n 각자 하나씩? 그러면 그거 활용하시면 될 것 같고\\n 동일하게 해야되는데 우리는\\n 기돕스로, HTTPS로 배포를 해볼게요. 그러면\\n 지금 어제 이렇게 올려주신 랩코스토리는\\n 소스코드 랩코스토리라고 하는거에요. 그러니까 어플리케이션 개발을 위한 소스코드들만\\n 저장을 해둘거고\\n 우리는 이제 기돕스 랩포를 하나 만들어야 되죠. 여기\\n ARGO CD 전용으로 기돕스 랩포를 하나 만들거에요. 그래서\\n 그 두개의 형상을 분리하는게 좋겠죠. 아무래도 개발용이랑 운영용이 달라져야 되니까\\n 그래서\\n 여기도 그냥 어제처럼 간단하게 레이밍 롤을 제가 드릴게요.\\n 만약에 이 분명 위에 계시니까 이거 쓸게요.'), Document(id='c012a7eb-a5c1-4e61-8e42-c553918c5509', metadata={}, page_content=\"근데 KT 안에서는 파이선도 배포하실 때 우선 패키징을 해야 되고 파이선을 패키징하는 툴로는\\n 'Poetry'라는 오픈 소스나 'Wheel' 이런 게 있어요. 이런 거를 써서 파이선을 점 자르나 점 와르처럼 만든다고 생각하시면 됩니다.\\n 그래서 이 실행 파일을 이런 파일로 생기거든요. 이런 압축 파일로 생기는 거예요.\\n '2.5'하는 점 자르처럼. 이런 최종 파일을 뵙는다거나 아니면 NPM, 모두 NPM 같은 경우는\\n 빌드 하면 디스트 폴더 생기죠. 프론텐드 하시는 분들 맞나요 제가? 이걸 압축 파일로 생기지 않나요? 점 집이었나 이런 압축 파일이 생길 거거든요.\\n 이런 식으로 이제 배포를 하게 되는데 V&B 배포를 경우에는 어쨌든 장황했지만 언어별로 패키징한 정적 파일이라고 해요.\\n 정적 파일들이 어제 CIA를 했다면 저희가 V&B 배포의 경우에는 블록 스토리지에다가 저장을 한다고 했잖아요.\\n 'Poetry'는 이제 ACI에다가 저장을 하고. 그럼 얘는 기본적으로 지금 블록 스토리지에 저장이 되어 있을 거란 말이죠.\\n V&B를 가져다가 실행해라는 명령이 많습니다. 따라서 이 명령이라는 거는 어떤 블록 스토리지에서 어떤 파일을 다운로드 받아서 실행해.\\n 나는 이 세 가지 명령어 셋을 jenkins가 target vm에다 내리는 방식이에요 ssh로\\n 그래서 이 크게 세 가지만 기억하시면 될 것 같아요\\n jenkins가 vm에다가 접근하기 위해서는 ip랑 id password가 필요하고\\n 그러면 이렇게 jenkins가 접근해서 내리 명령어 셋은\\n target vm이 어느 블록 스토리즈에서 어떤 파일을 다운로드 받아서 실행해 나는 명령어구나\\n 이렇게 부분하시면 됩니다. 말이 어렵지만\\n 소스코드 그 파이프라인은 굉장히 간단하니까 바로 이해하실 수 있을 거예요\\n 그래서 이렇게 이해를 한 다음에 쓰레기를 공유해 드렸습니다\\n 그 방금 이제 hello world 찍은 잡 있죠 이 안에 부성으로 들어갈게요\\n 방금 그 파이프라인은 다시 짜야 되기 때문에\"), Document(id='e6563747-ef4c-4e1d-b4e1-b49b20c6d82e', metadata={}, page_content='이것도 이제 사실 블루고리인과 롤링 업데이트 약간 좀 롤링 업데이트에 가까운 방식이지만\\n 저는 오히려 이게 조금 더 조심스럽게 배포하기에는 좋은 방식이라 생각하고 다만 단점으로는 만약에 비율 설정을 잘못했을 때\\n 새로운 버전의 문제를 충분히 검증하지 못해서 전체 서비스에 문제가 될 수 있다. 그러니까 보수적인 비율들이 충분히 검증이 되었을 때 전체의 배포하는 방식이 좋겠다.\\n 이런 식으로 서비스 특히 AKS나 커버넘 키스에 올라가는 서비스\\n 기토브의 대상으로 100포를 할 때 각각의 100포 전략이 이제 장점이 있으기 때문에 열심히 고민을 해보셔서 고르신 다음에\\n 그러시면 좋을 것 같습니다. 이런 이론적인 문제가 있어요. 이론적인 것들이 있어요.\\n 그 다음에 이제 기독스인데 여러분들이 아까 얘기한 ROCD를 기토브에 연결해서 헬런 100포 하는 것 자체가 기독스에요.\\n 그러니까 정확히는 개발자가 저희가 어제 기톱 엔터파이지를 각각 레포스토리 만들어서 소스코드 업로드 했죠. 그걸로 저희 ACI 했죠.\\n 그 1년의 과정을 지금 여기까지 올라와 있는 거예요. 도커브보다는 ACI로 생각하시면 되고\\n 그래서 이 개발자가 소스코드 업로드해서 우리 기톱 액션으로 도커와 만들어서 ACI에다가 업로드를 했죠. 이 과정까지 끝났고\\n 오늘은 이제 ROCD로 AKS에다가 배포를 할 건데 이 기독스 레포스토리 그러니까 100포를 할 형상을 소스코드로 관리하겠다는 거거든요.\\n 아까 말씀 주신 게 헬륨으로 하셨다고 하셨는데 헬륨하셔도 되고 커스테마이지를 쓰셔도 되고 그냥 쌩자 디플루이먼트랑 서비스랑 그런 야볼 파일도 던지셔도 됩니다.\\n 그건 이따가 자유롭게 하실 거고요. 따라서 이 ROCD나 ROCD가 커버네티스 또는 AKS에다가 배포할 형상을 소스코드로 관리하겠다.\\n 그게 단일 원천의 소스 이런 용어가 있는데 어쨌든 이 킥에서 관리하는 포인트로 이 스테이터스를 맞춰주는 거죠.'), Document(id='f73e597d-4a27-422d-bdb3-43d548567e75', metadata={}, page_content='잡은 뭐 실질적으로 하는 테스트로 할게요.\\n 이 폴더 안에 있는 잡에는\\n 파이프라인으로 할 거예요 파이프라인을 써야 우리가 명령어를 써서 배플할 수 있기 때문에 파이프라인을 쓸 거고\\n 이름은 편하게 하시고 파이프라인만 골라주시면 됩니다 파이프라인\\n 자 이제 쭉 내려보시면 직접 스크립트를 짤 수 있는 파이프라인 라인이 있는데\\n 어 이때 직접 이 파이프라인 스크립트를 여기에 그냥 이렇게 쓰셔도 되고\\n scm이라고 해서 기시랑 연동을 할 수 있어요 젠킨 스파일이라는 파일을 기수로 관리할 수 있기 때문에\\n 이거를 어 만약에 나는 기텁이랑 연동하고 싶다 그러면 여기에 이 젠킨 스파일이\\n 저장된 레포스토리 url에 넣고 인증 넣으신 다음에 브랭치 설정 하시면 이 젠킨 스파일이라는 패스까지 정해 주시면\\n 알아서 이제 이 빌드 잡이 될 때 젠킨스가 기질러서 젠킨 스파일 정보를 불러온 다음에 실행하게 될 거예요\\n 우리는 그렇게까지 하지 않고 여기서 할 건데 여기 아주 쉬운 스니펫이 하나 있습니다\\n 여기 패러 월드 하면 짠 바로 그룹이 생기거든요 이게 기본적으로 언어가 그룹이 베이스이고\\n 야물로도 할 수 있다고 제가 어제는 잠깐 말씀드렸는데 야물들은 또 플러그인이 필요해요 그래서 야물을 쓰시고 싶으시면\\n 이제 플러그인을 설치를 하시면 됩니다 그래서 단단하게 어플라이 하고 저장하고 이제 다시 폴더 1 테스트 잡으러 나가보시면\\n 이제 아무것도 안 떠 있죠 지금 이 상태에서 왼쪽에 있는 지금 빌드 버튼을 눌러볼게요\\n 왼쪽 아래 빌드 히스토리라는 탭 아래 이렇게 초록색 체크 표시 뜨면서 언제 2월에 빌드 했고 뭐 이런 식으로 되어 있거든요\\n 여러분 아마 그 타임존 설정 안 돼 있으면 아마 한 9시간 정도 차이 날 건데 그건 그냥 무시해 주시고\\n 그래서 이 빌드 히스토리에 들어가 보시면 어떤 잡을 얘가 수행했는지 확인할 수 있어요 그래서 누가 이 잡을 트리거 했고 이 잡을 어 시작부터 끝까지\\n 어 수행하는데 얼마나 걸렸고 이 자배될..'), Document(id='6d39c985-6812-49cc-a2de-0be56f0d03c4', metadata={}, page_content='그럼 이게 초기 세덕 비밀번호가 이제 프로세스에 남아 있는 거예요.\\n 그래서 이걸로 확인하셔서 넣으시면 저랑 같은 화면이 나올 겁니다.\\n 플로그인 깔면 됩니다.\\n 그냥 추천해 주는 걸로 다 갈아주시면 되요.\\n 네.\\n 아니 안 보여요.\\n (댓글 읽는 중)\\n 괜찮았으시다면 안 됐겠지?\\n 괜찮았으시다면 안 됐겠지?\\n 괜찮았으시다면 안 됐겠지?\\n 괜찮았으시다면 안 됐겠지?\\n 괜찮았으시다면 안 됐겠지?\\n 괜찮았으시다면 안 됐겠지?\\n 괜찮았으시다면 안 됐겠지?\\n 괜찮았으시다면 안 됐겠지?\\n 괜찮았으시다면 안 됐겠지?\\n 괜찮았으시다면 안 됐겠지?\\n 괜찮았으시다면 안 됐겠지?\\n 괜찮았으시다면 안 됐겠지?\\n 괜찮았으시다면 안 됐겠지?\\n 괜찮았으시다면 안 됐겠지?\\n 괜찮았으시다면 안 됐겠지?\\n 괜찮았으시다면 안 됐겠지?\\n 괜찮았으시다면 안 됐겠지?\\n 괜찮았으시다면 안 됐겠지?\\n 괜찮았으시다면 안 됐겠지?\\n 괜찮았으시다면 안 됐겠지?\\n 괜찮았으시다면 안 됐겠지?\\n 괜찮았으시다면 안 됐겠지?\\n 괜찮았으시다면 안 됐겠지?\\n 괜찮았으시다면 안 됐겠지?\\n 괜찮았으시다면 안 됐겠지?\\n 괜찮았으시다면 안 됐겠지?\\n 괜찮았으시다면 안 됐겠지?\\n 괜찮았으시다면 안 됐겠지?\\n 괜찮았으시다면 안 됐겠지?\\n 괜찮았으시다면 안 됐겠지?\\n 괜찮았으시다면 안 됐겠지?\\n 괜찮았으시다면 안 됐겠지?\\n 괜찮았으시다면 안 됐겠지?\\n 괜찮았으시다면 안 됐겠지?\\n 괜찮았으시다면 안 됐겠지?\\n 괜찮았으시다면 안 됐겠지?\\n 괜찮았으시다면 안 됐겠지?\\n 괜찮았으시다면 안 됐겠지?\\n 괜찮았으시다면 안 됐겠지?\\n 괜찮았으시다면 안 됐겠지?\\n 괜찮았으시다면 안 됐겠지?\\n 괜찮았으시다면 안 됐겠지?\\n 괜찮았으시다면 안 됐겠지?\\n 괜찮았으시다면 안 됐겠지?\\n - 네. - 네.\\n - 여기 진짜 진짜 잘 찍는 거 같아요. - 네.\\n - 진짜 이쁘다. - 진짜 이쁘다.\\n - 네. - 되셨어요?\\n - 네, 네. 아, 됐어요. - 여기다가 이거 넣는 거죠.\\n - 이거 한 번 더? - 네.\\n - 오. - 됐어.')]\n"
     ]
    }
   ],
   "source": [
    "# STEP 7. 유저 질문 테스트\n",
    "query = \"젠킨스는 오픈소스 CI/CD 자동화 서버이다. 이거 맞아?\"\n",
    "result = qa(query)\n",
    "\n",
    "print(\"💬 Question:\", query)\n",
    "print(result)\n",
    "print(\"🧠 Answer:\", result['result'])\n",
    "print(\"📄 Source Metadata:\", result['source_documents'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "45d72094",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
