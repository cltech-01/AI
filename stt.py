import os
import json
import torch
import pydub
import whisper
import tempfile
import multiprocessing
import numpy as np
from pathlib import Path
from config import settings 
from common import measure_time
from pydub import AudioSegment

@measure_time
def transcribe_audio(audio_path: str, model_name: str) -> dict:
    """
    Whisper ëª¨ë¸ì„ ì‚¬ìš©í•˜ì—¬ ì˜¤ë””ì˜¤ íŒŒì¼ì„ í…ìŠ¤íŠ¸ë¡œ ë³€í™˜í•©ë‹ˆë‹¤.
    
    Args:
        audio_path: ì˜¤ë””ì˜¤ íŒŒì¼ ê²½ë¡œ
        model_name: ì‚¬ìš©í•  Whisper ëª¨ë¸ í¬ê¸° ("tiny", "base", "small", "medium", "large")
        
    Returns:
        ë³€í™˜ëœ í…ìŠ¤íŠ¸ì™€ ì„¸ê·¸ë¨¼íŠ¸ ì •ë³´ë¥¼ í¬í•¨í•œ ë”•ì…”ë„ˆë¦¬
    """
    print(f"ğŸ”Š Whisper {model_name} ëª¨ë¸ë¡œ ìŒì„± ì¸ì‹ ì‹œì‘...")
    
    # Whisper ëª¨ë¸ ë¡œë“œ
    model = whisper.load_model(model_name).to(get_device())
    
    # ì˜¤ë””ì˜¤ íŒŒì¼ ë³€í™˜
    result = model.transcribe(
        audio_path,
        language="ko",  # í•œêµ­ì–´ë¡œ ì„¤ì •
        fp16=False,     # GPUê°€ ì—†ëŠ” ê²½ìš° Falseë¡œ ì„¤ì •
        verbose=True,    # ì§„í–‰ ìƒí™© í‘œì‹œ
        condition_on_previous_text=False
    )
    
    # ê²°ê³¼ ì €ì¥ ê²½ë¡œ
    audio_filename = Path(audio_path).stem
    output_dir = Path(audio_path).parent
    text_path = output_dir / f"{audio_filename}.txt"
    
    # ì „ì²´ í…ìŠ¤íŠ¸ ì €ì¥
    with open(text_path, "w", encoding="utf-8") as f:
        f.write(result["text"])
    
    print(f"âœ… í…ìŠ¤íŠ¸ ì €ì¥ ì™„ë£Œ: {text_path}")
    
    return result


@measure_time
def process_audio(audio_path, model_name, username):
    """
    ì˜¤ë””ì˜¤ íŒŒì¼ì„ CPUë¡œ ì¶”ë¡ í•˜ê¸° (ë©€í‹°í”„ë¡œì„¸ì‹± ì—†ì´)
    """
    # 1. íŒŒì¼ ì¡´ì¬ í™•ì¸
    if not os.path.exists(audio_path):
        raise FileNotFoundError(f"ì˜¤ë””ì˜¤ íŒŒì¼ì„ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤: {audio_path}")
    
    # 2. ë¬´ì¡°ê±´ CPU ë””ë°”ì´ìŠ¤ ì‚¬ìš©
    device = torch.device("cpu")
    print(f"ğŸ–¥ï¸ ì‚¬ìš© ì¤‘ì¸ ë””ë°”ì´ìŠ¤: {device}")
    
    # 3. ë‹¨ì¼ í”„ë¡œì„¸ìŠ¤ë¡œ ì²˜ë¦¬
    print(f"ğŸ“Š ë‹¨ì¼ í”„ë¡œì„¸ìŠ¤ë¡œ ì²˜ë¦¬ ì‹œì‘")
    
    # ëª¨ë¸ ë¡œë“œ (CPUì—ì„œë§Œ)
    model = whisper.load_model(model_name).to(device)
    
    # ì¶”ë¡  ì‹¤í–‰
    with torch.inference_mode():
        result = model.transcribe(
            audio_path,
            language="ko",
            fp16=False,
            verbose=False,
            condition_on_previous_text=False
        )
    
    # 4. ê²°ê³¼ ì €ì¥
    text_path, json_path = save_transcription_results(audio_path, result, username)
    
    return result, text_path


def save_transcription_results(audio_path, result, username=None):
    """ê²°ê³¼ë¥¼ Text/{username}ì— ì €ì¥"""
    stem = Path(audio_path).stem
    base_dir = Path(f"./Data/Text/{username}")
    base_dir.mkdir(parents=True, exist_ok=True)
    
    # í…ìŠ¤íŠ¸ ì €ì¥
    text_path = base_dir / f"{stem}_transcript.txt"
    with open(text_path, "w", encoding="utf-8") as f:
        f.write(result["text"])
    
    # ì„¸ê·¸ë¨¼íŠ¸ ì €ì¥
    json_path = base_dir / f"{stem}.json"
    segments_data = [
        {"start_time": s["start"], "end_time": s["end"], "text": s["text"]}
        for s in result["segments"]
    ]
    with open(json_path, "w", encoding="utf-8") as f:
        json.dump(segments_data, f, ensure_ascii=False, indent=2)

    print(f"âœ… í…ìŠ¤íŠ¸ ì €ì¥ ì™„ë£Œ: {text_path}")
    print(f"âœ… JSON ì„¸ê·¸ë¨¼íŠ¸ ì €ì¥ ì™„ë£Œ: {json_path}")
    return [str(text_path), str(json_path)]

if __name__ == "__main__":
    # ì˜¤ë””ì˜¤ íŒŒì¼ ì²˜ë¦¬ ì‹œì‘
    process_audio(
        audio_path="./reference/example.wav",
        model_name="small",
        parallel_threshold_mb=50,
        chunk_duration=120,
        username="jhkim"
    )