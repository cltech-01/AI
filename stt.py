import os
import json
import torch
import pydub
import whisper
import tempfile
import multiprocessing
import numpy as np
from pathlib import Path
from config import settings 
from common import measure_time
from pydub import AudioSegment




# MPS(Metal Performance Shaders) í™œì„±í™” í™•ì¸ ë° ì„¤ì •
def get_device():
    if torch.backends.mps.is_available():
        return torch.device("mps")  # Mì‹œë¦¬ì¦ˆ GPU ì‚¬ìš©
    else:
        return torch.device("cpu")

@measure_time
def transcribe_audio_optimized(audio_path, model_name):
    """MPS ë°±ì—”ë“œë¥¼ í™œìš©í•œ ìµœì í™”ëœ Whisper ë³€í™˜"""
    device = get_device()
    print(f"ğŸ–¥ï¸ ì‚¬ìš© ì¤‘ì¸ ë””ë°”ì´ìŠ¤: {device}")
    
    try:
        # MPS ë°±ì—”ë“œ ì‚¬ìš© ì‹œë„
        model = whisper.load_model(model_name).to(device)
    except:
        # ì˜¤ë¥˜ ë°œìƒ ì‹œ CPUë¡œ í´ë°±
        print(f"âš ï¸ MPS ë””ë°”ì´ìŠ¤ì—ì„œ ì˜¤ë¥˜ ë°œìƒ, CPUë¡œ ëŒ€ì²´í•©ë‹ˆë‹¤")
        device = torch.device("cpu")
        model = whisper.load_model(model_name).to(device)
    
    # ëª¨ë¸ ì¶”ë¡  ìµœì í™” ì„¤ì •
    with torch.inference_mode():
        result = model.transcribe(
            audio_path,
            language="ko",
            fp16=False,
            verbose=True,
            condition_on_previous_text=False
        )
    
    return result

# í° ì˜¤ë””ì˜¤ íŒŒì¼ì„ ì—¬ëŸ¬ ì²­í¬ë¡œ ë‚˜ëˆ„ì–´ ë³‘ë ¬ ì²˜ë¦¬
def process_audio_in_parallel(audio_path, chunk_size, model_name):
    """ì˜¤ë””ì˜¤ë¥¼ ì—¬ëŸ¬ ì²­í¬ë¡œ ë‚˜ëˆ„ì–´ ë³‘ë ¬ ì²˜ë¦¬"""
    # 1. ì˜¤ë””ì˜¤ë¥¼ chunk_sizeì´ˆ ë‹¨ìœ„ë¡œ ë‚˜ëˆ„ê¸°
    # 2. ê° ì²­í¬ë¥¼ ë³„ë„ì˜ í”„ë¡œì„¸ìŠ¤ì—ì„œ ì²˜ë¦¬
    # 3. ê²°ê³¼ ë³‘í•©
    
    # ë©€í‹°í”„ë¡œì„¸ì‹± í’€ ìƒì„± (ì½”ì–´ ìˆ˜ì— ë§ê²Œ)
    num_processes = multiprocessing.cpu_count() - 1  # í•œ ì½”ì–´ëŠ” ì—¬ìœ ë¡­ê²Œ
    
    # ì´ ë¶€ë¶„ì€ ì‹¤ì œ êµ¬í˜„ì´ í•„ìš”í•¨ - ì˜¤ë””ì˜¤ ë¶„í•  ë° ë³‘ë ¬ ì²˜ë¦¬
    
    return "ë³‘ë ¬ ì²˜ë¦¬ ê²°ê³¼"

# ì—¬ëŸ¬ ì˜¤ë””ì˜¤ íŒŒì¼ ë™ì‹œ ì²˜ë¦¬
def process_multiple_files(audio_files, model_name):
    """ì—¬ëŸ¬ ì˜¤ë””ì˜¤ íŒŒì¼ì„ ë™ì‹œì— ì²˜ë¦¬"""
    with multiprocessing.Pool(processes=multiprocessing.cpu_count() - 1) as pool:
        results = pool.starmap(
            transcribe_audio_optimized,
            [(file, model_name) for file in audio_files]
        )
    return results

@measure_time
def transcribe_audio(audio_path: str, model_name: str) -> dict:
    """
    Whisper ëª¨ë¸ì„ ì‚¬ìš©í•˜ì—¬ ì˜¤ë””ì˜¤ íŒŒì¼ì„ í…ìŠ¤íŠ¸ë¡œ ë³€í™˜í•©ë‹ˆë‹¤.
    
    Args:
        audio_path: ì˜¤ë””ì˜¤ íŒŒì¼ ê²½ë¡œ
        model_name: ì‚¬ìš©í•  Whisper ëª¨ë¸ í¬ê¸° ("tiny", "base", "small", "medium", "large")
        
    Returns:
        ë³€í™˜ëœ í…ìŠ¤íŠ¸ì™€ ì„¸ê·¸ë¨¼íŠ¸ ì •ë³´ë¥¼ í¬í•¨í•œ ë”•ì…”ë„ˆë¦¬
    """
    print(f"ğŸ”Š Whisper {model_name} ëª¨ë¸ë¡œ ìŒì„± ì¸ì‹ ì‹œì‘...")
    
    # Whisper ëª¨ë¸ ë¡œë“œ
    model = whisper.load_model(model_name).to(get_device())
    
    # ì˜¤ë””ì˜¤ íŒŒì¼ ë³€í™˜
    result = model.transcribe(
        audio_path,
        language="ko",  # í•œêµ­ì–´ë¡œ ì„¤ì •
        fp16=False,     # GPUê°€ ì—†ëŠ” ê²½ìš° Falseë¡œ ì„¤ì •
        verbose=True,    # ì§„í–‰ ìƒí™© í‘œì‹œ
        condition_on_previous_text=False
    )
    
    # ê²°ê³¼ ì €ì¥ ê²½ë¡œ
    audio_filename = Path(audio_path).stem
    output_dir = Path(audio_path).parent
    text_path = output_dir / f"{audio_filename}.txt"
    
    # ì „ì²´ í…ìŠ¤íŠ¸ ì €ì¥
    with open(text_path, "w", encoding="utf-8") as f:
        f.write(result["text"])
    
    print(f"âœ… í…ìŠ¤íŠ¸ ì €ì¥ ì™„ë£Œ: {text_path}")
    
    return result

@measure_time
def transcribe_with_segments(audio_path: str, model_name: str) -> dict:
    """
    ì˜¤ë””ì˜¤ íŒŒì¼ì„ ë³€í™˜í•˜ê³  ì„¸ê·¸ë¨¼íŠ¸ ì •ë³´ë„ ì €ì¥í•©ë‹ˆë‹¤.
    
    Args:
        audio_path: ì˜¤ë””ì˜¤ íŒŒì¼ ê²½ë¡œ
        model_name: ì‚¬ìš©í•  Whisper ëª¨ë¸ í¬ê¸°
        
    Returns:
        ë³€í™˜ ê²°ê³¼ ë”•ì…”ë„ˆë¦¬
    """
    # ì˜¤ë””ì˜¤ íŒŒì¼ ë³€í™˜
    result = transcribe_audio(audio_path, model_name)
    
    # ì„¸ê·¸ë¨¼íŠ¸ ì •ë³´ ì €ì¥
    audio_filename = Path(audio_path).stem
    output_dir = Path(audio_path).parent
    segments_path = output_dir / f"{audio_filename}_segments.txt"
    
    with open(segments_path, "w", encoding="utf-8") as f:
        for segment in result["segments"]:
            start = segment["start"]
            end = segment["end"]
            text = segment["text"]
            f.write(f"[{start:.2f} â†’ {end:.2f}] {text}\n")
    
    print(f"âœ… ì„¸ê·¸ë¨¼íŠ¸ ì •ë³´ ì €ì¥ ì™„ë£Œ: {segments_path}")
    print(f"ğŸ“Š ì„¸ê·¸ë¨¼íŠ¸ ìˆ˜: {len(result['segments'])} ê°œ")
    
    return result

@measure_time
def process_audio(audio_path, model_name, parallel_threshold_mb=50, chunk_duration=120, username=None):
    """
    ì˜¤ë””ì˜¤ íŒŒì¼ì„ MPSë¡œ ì¶”ë¡ í•˜ê¸°
    
    Args:
        audio_path: ì²˜ë¦¬í•  ì˜¤ë””ì˜¤ íŒŒì¼ ê²½ë¡œ
        model_name: Whisper ëª¨ë¸ í¬ê¸° ("tiny", "base", "small", "medium", "large")
        parallel_threshold_mb: ë³‘ë ¬ ì²˜ë¦¬ ì‹œì‘í•  íŒŒì¼ í¬ê¸° ì„ê³„ê°’(MB)
        chunk_duration: ë³‘ë ¬ ì²˜ë¦¬ì‹œ ë¶„í• í•  ì²­í¬ ê¸¸ì´(ì´ˆ)
        username: ê²°ê³¼ë¥¼ ì €ì¥í•  ì‚¬ìš©ì ë””ë ‰í† ë¦¬ ì´ë¦„
        
    Returns:
        ë³€í™˜ ê²°ê³¼ ë”•ì…”ë„ˆë¦¬
    """
    # 1. íŒŒì¼ ì¡´ì¬ í™•ì¸
    if not os.path.exists(audio_path):
        raise FileNotFoundError(f"ì˜¤ë””ì˜¤ íŒŒì¼ì„ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤: {audio_path}")
    
    # 2. MPS ë””ë°”ì´ìŠ¤ í™•ì¸
    device = get_device()
    print(f"ğŸ–¥ï¸ ì‚¬ìš© ì¤‘ì¸ ë””ë°”ì´ìŠ¤: {device}")
    
    # 3. ì˜¤ë””ì˜¤ íŒŒì¼ì´ í° ê²½ìš° ë³‘ë ¬ ì²˜ë¦¬, ì‘ì€ ê²½ìš° ë‹¨ì¼ ì²˜ë¦¬
    audio_info = Path(audio_path).stat()
    file_size_mb = audio_info.st_size / (1024 * 1024)
    
    # íŒŒì¼ í¬ê¸°ê°€ ì„ê³„ê°’ ì´ìƒì´ë©´ ë³‘ë ¬ ì²˜ë¦¬
    if file_size_mb > parallel_threshold_mb:
        print(f"ğŸ“Š íŒŒì¼ í¬ê¸°: {file_size_mb:.2f}MB - ë³‘ë ¬ ì²˜ë¦¬ ì‹œì‘")
        result = split_and_process_parallel(audio_path, chunk_duration, model_name)
    else:
        print(f"ğŸ“Š íŒŒì¼ í¬ê¸°: {file_size_mb:.2f}MB - ë‹¨ì¼ ì²˜ë¦¬ ì‹œì‘")
        result = transcribe_audio_optimized(audio_path, model_name)
    
    # 4. ê²°ê³¼ ì €ì¥
    text_path, json_path = save_transcription_results(audio_path, result, username)
    
    return result, text_path

@measure_time
def split_and_process_parallel(audio_path, chunk_duration, model_name):
    """ì˜¤ë””ì˜¤ë¥¼ ì²­í¬ë¡œ ë‚˜ëˆ„ì–´ ë³‘ë ¬ ì²˜ë¦¬"""
    # 1. ì˜¤ë””ì˜¤ ë¡œë“œ
    print(f"ğŸ”Š ì˜¤ë””ì˜¤ íŒŒì¼ ë¡œë“œ ì¤‘: {audio_path}")
    audio = AudioSegment.from_wav(audio_path)
    
    # 2. ì²­í¬ë¡œ ë¶„í• 
    total_duration = len(audio) / 1000  # ì´ˆ ë‹¨ìœ„
    num_chunks = int(np.ceil(total_duration / chunk_duration))
    
    print(f"â±ï¸ ì´ ì˜¤ë””ì˜¤ ê¸¸ì´: {total_duration:.2f}ì´ˆ, {num_chunks}ê°œ ì²­í¬ë¡œ ë¶„í• ")
    
    # 3. ì„ì‹œ íŒŒì¼ ìƒì„± ë° ì²­í¬ ì €ì¥
    temp_dir = tempfile.mkdtemp()
    chunk_files = []
    
    for i in range(num_chunks):
        start_time = i * chunk_duration * 1000  # ms ë‹¨ìœ„
        end_time = min((i + 1) * chunk_duration * 1000, len(audio))
        
        chunk = audio[start_time:end_time]
        chunk_path = os.path.join(temp_dir, f"chunk_{i:03d}.wav")
        chunk.export(chunk_path, format="wav")
        chunk_files.append(chunk_path)
    
    # 4. ë³‘ë ¬ ì²˜ë¦¬
    num_processes = min(multiprocessing.cpu_count() - 1, len(chunk_files))
    print(f"ğŸ§µ ë³‘ë ¬ ì²˜ë¦¬ ì‹œì‘: {num_processes}ê°œ í”„ë¡œì„¸ìŠ¤ë¡œ {len(chunk_files)}ê°œ ì²­í¬ ì²˜ë¦¬")
    
    with multiprocessing.Pool(processes=num_processes) as pool:
        chunk_results = pool.starmap(
            process_chunk,
            [(chunk_file, model_name) for chunk_file in chunk_files]
        )
    
    # 5. ê²°ê³¼ í•©ì¹˜ê¸°
    merged_text = " ".join([result["text"] for result in chunk_results])
    
    # ì„¸ê·¸ë¨¼íŠ¸ë„ í•©ì¹˜ê¸° (ì‹œê°„ ì˜¤í”„ì…‹ ì¡°ì • í•„ìš”)
    merged_segments = []
    time_offset = 0
    
    for i, result in enumerate(chunk_results):
        for segment in result["segments"]:
            segment["start"] += time_offset
            segment["end"] += time_offset
            merged_segments.append(segment)
        
        # ë‹¤ìŒ ì²­í¬ì˜ ì‹œê°„ ì˜¤í”„ì…‹ ì—…ë°ì´íŠ¸
        if i < len(chunk_results) - 1:
            time_offset += chunk_duration
    
    # 6. ì„ì‹œ íŒŒì¼ ì •ë¦¬
    for file in chunk_files:
        os.remove(file)
    os.rmdir(temp_dir)
    
    # 7. ìµœì¢… ê²°ê³¼ ë°˜í™˜
    return {
        "text": merged_text,
        "segments": merged_segments
    }

def process_chunk(chunk_path, model_name):
    """ê°œë³„ ì²­í¬ë¥¼ ì²˜ë¦¬ (ë³‘ë ¬ í”„ë¡œì„¸ìŠ¤ì—ì„œ ì‹¤í–‰)"""
    # ì¤‘ìš”: MPSê°€ ì•„ë‹Œ CPU ì‚¬ìš©
    device = torch.device("cpu")
    model = whisper.load_model(model_name).to(device)
    
    with torch.inference_mode():
        result = model.transcribe(
            chunk_path,
            language="ko",
            fp16=False,
            verbose=False,
            condition_on_previous_text=False  
        )
    
    return result

def save_transcription_results(audio_path, result, username=None):
    """ê²°ê³¼ë¥¼ íŒŒì¼ë¡œ ì €ì¥"""
    path = Path(audio_path)
    base_dir = path.parent
    stem = path.stem
    
    # ê¸°ë³¸ í…ìŠ¤íŠ¸ ì €ì¥
    text_path = base_dir / f"{stem}_transcript.txt"
    with open(text_path, "w", encoding="utf-8") as f:
        f.write(result["text"])
    
    # JSON íŒŒì¼ë„ ê°™ì€ ë””ë ‰í† ë¦¬ì— ì €ì¥
    json_path = base_dir / f"{stem}.json"
    
    # ì„¸ê·¸ë¨¼íŠ¸ ë°ì´í„° JSON í˜•ì‹ìœ¼ë¡œ ì¤€ë¹„
    segments_data = []
    for segment in result["segments"]:
        segments_data.append({
            "start_time": segment["start"],
            "end_time": segment["end"],
            "text": segment["text"]
        })
    
    # JSON íŒŒì¼ ì €ì¥
    with open(json_path, "w", encoding="utf-8") as f:
        json.dump(segments_data, f, ensure_ascii=False, indent=2)
    
    print(f"âœ… í…ìŠ¤íŠ¸ ì €ì¥ ì™„ë£Œ: {text_path}")
    print(f"âœ… JSON ì„¸ê·¸ë¨¼íŠ¸ ì €ì¥ ì™„ë£Œ: {json_path}")
    print(f"ğŸ”¤ í…ìŠ¤íŠ¸ ê¸¸ì´: {len(result['text'])} ê¸€ì")
    print(f"ğŸ“Š ì„¸ê·¸ë¨¼íŠ¸ ìˆ˜: {len(result['segments'])} ê°œ")
    return [str(text_path),str(json_path)]

if __name__ == "__main__":
    # ì˜¤ë””ì˜¤ íŒŒì¼ ì²˜ë¦¬ ì‹œì‘
    process_audio(
        audio_path="./Data/Sound/jhkim/01.wav",
        model_name="small",
        parallel_threshold_mb=50,
        chunk_duration=120,
        username="jhkim"
    )